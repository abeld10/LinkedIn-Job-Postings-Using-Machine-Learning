{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertModel, \n",
    "    AutoConfig, \n",
    "    AutoTokenizer, \n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    "    )\n",
    "import torch.nn as nn\n",
    "import datasets\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1171.],\n",
       "       [  36.],\n",
       "       [1227.],\n",
       "       [1260.],\n",
       "       [  36.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load dataset\n",
    "feats_fp = open(\"BERT_X.csv\", \"r\")\n",
    "labels_fp = open(\"BERT_y.csv\", \"r\")\n",
    "feats = csv.reader(feats_fp)\n",
    "labels = csv.reader(labels_fp)\n",
    "\n",
    "# skip header\n",
    "next(feats)\n",
    "next(labels)\n",
    "\n",
    "unscaled_data = {'text': [], 'label': []}\n",
    "nn_data = []\n",
    "for row in feats:\n",
    "    unscaled_data['text'].append(row[0].strip().replace(\"\\n\", \" \"))\n",
    "    nn_feats = [float(col.strip().replace(\"\\n\", \"\")) for col in row[1:]]\n",
    "    \n",
    "    \n",
    "    nn_data.append(nn_feats[-2]) # company employee count only\n",
    "for i,row in enumerate(labels):\n",
    "    unscaled_data['label'].append(float(row[0].strip().replace(\"\\n\", \"\")))\n",
    "nn_data = np.array(nn_data).reshape(-1,1).astype(np.float32)\n",
    "nn_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.58786439, -0.94649659,  2.86819603, ..., -1.04441134,\n",
       "        -0.86218111, -0.69471581]),\n",
       " array([-0.30006492, -0.31242558, -0.29945505, ..., -0.31238204,\n",
       "        -0.31282854, -0.31278497], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_labels = scaler.fit_transform(np.array(unscaled_data['label']).reshape(-1,1)).flatten()\n",
    "scaled_data = unscaled_data\n",
    "scaled_data['label'] = scaled_labels\n",
    "\n",
    "employee_count_scaler = StandardScaler()\n",
    "scaled_employee_count =employee_count_scaler.fit_transform(nn_data[:,0].reshape(-1,1)).flatten()\n",
    "nn_data[:,0] = scaled_employee_count\n",
    "scaled_labels, nn_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(scaled_data['text']) == len(scaled_data['label']) == nn_data.shape[0]\n",
    "dataset = datasets.Dataset.from_dict(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview  HearingLife is a national hearing care company and part of the Demant Group, a global leader in hearing healthcare built on a heritage of care, health, and innovation since 1904. HearingLife operates more than 600 hearing care centers across 42 states. We follow a scientific, results-oriented approach to hearing healthcare that is provided by highly skilled and caring professionals. Our vision is to help more people hear better through life-changing hearing health delivered by the best personalized care. This Team Member must uphold the HearingLife Core Values:   We create trust  We are team players  We apply a can-do attitude  We create innovative solutions   Responsibilities  You will help more people hear better by providing clinical expertise to diagnose and treat hearing loss while ensuring a positive patient experience. The Hearing Care Provider acts in accordance with required industry and state professional licensing standards and local practice scope and is responsible for growing and maintaining the practice. This includes achieving all financial goals as well as offering best in class modern hearing healthcare assessment and treatment. The Hearing Care Provider is responsible for creating an exceptional patient journey through patient care and appropriate treatment options.   Provide quality care and aftercare of dispensing services such as hearing tests, hearing aid fittings, educate and train hearing aid users of best practices.  Perform checks on hearing aids and other amplification devices including but not limited to troubleshooting, conducting repairs to hearing aids, earmolds and cleaning of hearing aids.  Achieves growth with a strong mindset on sales and key business metrics while focusing on providing quality patient care.  Continuously develop a relationship with local community leaders by representing HearingLife as an advocate to making a life changing difference.  Ensure clinic inventory meets a sustainable level to drive business including accurate recordkeeping of inventory.  Support Telehealth initiatives (Remote Care) to expand patient care and product portfolio including but not limited conducting hearing tests, coach and educate patients on hearing aid devices.   Qualifications   Maintain an active Hearing Aid Dispensing License in accordance with state requirements.  A minimum of two years of professional experience; previous experience in selling hearing aids in an Audiology/dispensing practice. In lieu of two years of experience, demonstrated previous experience or training or equivalent combination of education and experience.  Maintain continuing education requirements based on state requirements.  Ability to operate audiometric equipment and to interpret the results.  Exceptional critical thinking skills to analyze patient‚Äôs situation.  Excellent interpersonal skills to engage and motivate patients and third parties.  Skill in handling sensitive matters and patients with tact, courtesy, and discretion.  Demonstrated ability to manage multiple tasks efficiently, including determining priorities, organizing work, and working independently in a fast-paced environment.  Ability to communicate test results and interpret and propose treatment in a manner easily understood by patients.   üå¥ Unwind with Paid Time Off: We value work-life balance. Enjoy company-paid holidays, floating holidays, and more!  üíº Flexible Work Dynamics: Experience the future of work with numerous hybrid and remote opportunities tailored for the modern professional.  üåü Comprehensive Health Benefits: Choose from a diverse range of health insurance plans covering medical, dental, vision, and HSA. Your well-being is our priority.  üí∞ Invest in Your Future: With our competitive 401(k) Program, your future looks bright.  üéÅ Exclusive Discounts & Programs: Get special discounts on our products, including hearing aids, for both family and possibly friends. Plus, take advantage of our Employee and Family Purchase Hearing Aid program.  üöÄ Boundless Growth Opportunities   DMIT Program: Dive deep into management insights.  Apprentice Program: Learn from the best in the field.  Amplify Leadership Program: Get one-on-one guidance and real-world exposure to grow and excel as a Leader.   ‚ù§Ô∏è A Thriving, Positive Environment: We live our C ore V alues : We C reate T rust, W e are T eam P layers, W e A pply a C an- D o A ttitude and We C reate I nnovative S olutions.  üìö Empower Your Ambitions: Avail up to $5250 annually with our Education Expense Reimbursement. Keep learning, keep growing!  ü§ù Refer & Earn: Know someone perfect for the team? Our Team Member Referral Program rewards you with up to $3500 per hire, depending on the role.  üõ°Ô∏è Protection for the Unexpected: Enjoy peace of mind with our basic life and AD&D insurance, as well as short-term disability insurance.  Come be part of a team where every day brings new challenges, learning, and the opportunity to make a difference. Join us!  We are an Equal Opportunity / Affirmative Action employer, all qualified applicants will receive consideration for employment  without regard to race, color, religion, sexual orientation, sex, national origin, disability, or protected veteran status.  #HearingLife_US\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[63000.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset[0]['text'])\n",
    "scaler.inverse_transform(np.array([dataset[0]['label']]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d67ac8eac494153a0ec9c7f00134336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',          \n",
    "#     num_train_epochs=3,              \n",
    "#     per_device_train_batch_size=16,  \n",
    "#     learning_rate=5e-5,               \n",
    "#     warmup_steps=500,                \n",
    "#     weight_decay=0.01,              \n",
    "#     logging_dir='./logs',\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 26990\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels, nn_data):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "        self.nn_data = nn_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx]),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx]),\n",
    "            'labels': torch.tensor(self.labels[idx]),\n",
    "            'nn_data': torch.tensor(self.nn_data[idx])\n",
    "        }\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)  # Assuming all data entries have labels\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenized_dataset['input_ids']\n",
    "attention_mask = tokenized_dataset['attention_mask']\n",
    "labels = dataset['label']  # Assuming your labels are in the original dataset\n",
    "nn_data = nn_data\n",
    "\n",
    "reg_dataset = RegressionDataset(input_ids, attention_mask, labels, nn_data)\n",
    "dataloader = torch.utils.data.DataLoader(reg_dataset, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3001])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_dataset[0]['nn_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "config.problem_type = 'regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add linear layer\n",
    "output_size = 1  \n",
    "\n",
    "\n",
    "# Combine BERT and the linear layer\n",
    "class BertWithLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertWithLinear, self).__init__()\n",
    "        self.bert = bert_model.to('cuda')\n",
    "    \n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size ,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_size)\n",
    "        ).to('cuda')\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, nn_data = None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)\n",
    "        # Use pooled output for classification/regression\n",
    "        pooled_output = output.pooler_output\n",
    "        #nn_data = torch.cat((pooled_output, nn_data), 1)\n",
    "        nn_output = self.nn(pooled_output)\n",
    "        return nn_output\n",
    "\n",
    "    \n",
    "\n",
    "model = BertWithLinear()\n",
    "model.bert.load_state_dict(torch.load('bert.pth'))\n",
    "model.nn.load_state_dict(torch.load('nn.pth'))\n",
    "model = model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze/unfreeze BERT pretrained weights\n",
    "for param in model.bert.embeddings.parameters():\n",
    "    param.requires_grad = True\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x2baa5790e9e8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split \n",
    "\n",
    "train_size = int(0.8 * len(reg_dataset))  # 80% of the dataset for training\n",
    "val_size = len(reg_dataset) - train_size \n",
    "\n",
    "# Create the train and validation datasets\n",
    "train_dataset, val_dataset = random_split(reg_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)  # No need to shuffle validation\n",
    "loss_fn = nn.MSELoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 10\n",
      "batch 0 complete. Loss:  0.5700047612190247\n",
      "batch 1 complete. Loss:  0.7581078410148621\n",
      "batch 2 complete. Loss:  0.6639477014541626\n",
      "batch 3 complete. Loss:  1.5543856620788574\n",
      "batch 4 complete. Loss:  1.2973146438598633\n",
      "batch 5 complete. Loss:  0.8860270380973816\n",
      "batch 6 complete. Loss:  0.5424118638038635\n",
      "batch 7 complete. Loss:  0.8580881357192993\n",
      "batch 8 complete. Loss:  0.8982554078102112\n",
      "batch 9 complete. Loss:  0.7982231378555298\n",
      "batch 10 complete. Loss:  0.46301859617233276\n",
      "batch 11 complete. Loss:  0.9971774816513062\n",
      "batch 12 complete. Loss:  0.9486339092254639\n",
      "batch 13 complete. Loss:  1.1373666524887085\n",
      "batch 14 complete. Loss:  1.074381947517395\n",
      "batch 15 complete. Loss:  1.1108343601226807\n",
      "batch 16 complete. Loss:  0.9202673435211182\n",
      "batch 17 complete. Loss:  0.48033520579338074\n",
      "batch 18 complete. Loss:  0.5611052513122559\n",
      "batch 19 complete. Loss:  0.8585028052330017\n",
      "batch 20 complete. Loss:  1.056932806968689\n",
      "batch 21 complete. Loss:  1.0107691287994385\n",
      "batch 22 complete. Loss:  0.7473170757293701\n",
      "batch 23 complete. Loss:  0.6260859966278076\n",
      "batch 24 complete. Loss:  0.9418565034866333\n",
      "batch 25 complete. Loss:  0.3691798448562622\n",
      "batch 26 complete. Loss:  0.41487830877304077\n",
      "batch 27 complete. Loss:  0.7216020822525024\n",
      "batch 28 complete. Loss:  0.9558732509613037\n",
      "batch 29 complete. Loss:  0.5174424052238464\n",
      "batch 30 complete. Loss:  1.160982608795166\n",
      "batch 31 complete. Loss:  0.6609532833099365\n",
      "batch 32 complete. Loss:  0.37754443287849426\n",
      "batch 33 complete. Loss:  0.5999186635017395\n",
      "batch 34 complete. Loss:  0.4915248155593872\n",
      "batch 35 complete. Loss:  0.39493176341056824\n",
      "batch 36 complete. Loss:  0.18740491569042206\n",
      "batch 37 complete. Loss:  0.38341885805130005\n",
      "batch 38 complete. Loss:  0.38858693838119507\n",
      "batch 39 complete. Loss:  0.6624127626419067\n",
      "batch 40 complete. Loss:  0.34536057710647583\n",
      "batch 41 complete. Loss:  0.9593683481216431\n",
      "batch 42 complete. Loss:  0.4185263514518738\n",
      "batch 43 complete. Loss:  0.3446581959724426\n",
      "batch 44 complete. Loss:  0.20118489861488342\n",
      "batch 45 complete. Loss:  0.5031648278236389\n",
      "batch 46 complete. Loss:  0.10539817810058594\n",
      "batch 47 complete. Loss:  0.2639714479446411\n",
      "batch 48 complete. Loss:  0.5105302333831787\n",
      "batch 49 complete. Loss:  0.30489811301231384\n",
      "batch 50 complete. Loss:  0.5090517401695251\n",
      "batch 51 complete. Loss:  0.17048177123069763\n",
      "batch 52 complete. Loss:  0.7471754550933838\n",
      "batch 53 complete. Loss:  0.12213465571403503\n",
      "batch 54 complete. Loss:  0.24396908283233643\n",
      "batch 55 complete. Loss:  0.5755245089530945\n",
      "batch 56 complete. Loss:  0.6740515232086182\n",
      "batch 57 complete. Loss:  0.18818816542625427\n",
      "batch 58 complete. Loss:  0.1577417254447937\n",
      "batch 59 complete. Loss:  0.21869072318077087\n",
      "batch 60 complete. Loss:  0.630113959312439\n",
      "batch 61 complete. Loss:  0.20192843675613403\n",
      "batch 62 complete. Loss:  0.18937693536281586\n",
      "batch 63 complete. Loss:  0.3121349513530731\n",
      "batch 64 complete. Loss:  0.24533021450042725\n",
      "batch 65 complete. Loss:  0.3484093248844147\n",
      "batch 66 complete. Loss:  0.2289414405822754\n",
      "batch 67 complete. Loss:  0.21263538300991058\n",
      "batch 68 complete. Loss:  0.3108874559402466\n",
      "batch 69 complete. Loss:  0.36449292302131653\n",
      "batch 70 complete. Loss:  0.1652756929397583\n",
      "batch 71 complete. Loss:  0.19257768988609314\n",
      "batch 72 complete. Loss:  0.25098690390586853\n",
      "batch 73 complete. Loss:  0.36795106530189514\n",
      "batch 74 complete. Loss:  0.2550286650657654\n",
      "batch 75 complete. Loss:  0.14175909757614136\n",
      "batch 76 complete. Loss:  0.30553969740867615\n",
      "batch 77 complete. Loss:  0.13989169895648956\n",
      "batch 78 complete. Loss:  0.3997052013874054\n",
      "batch 79 complete. Loss:  0.14353421330451965\n",
      "batch 80 complete. Loss:  0.11855451762676239\n",
      "batch 81 complete. Loss:  0.4138963222503662\n",
      "batch 82 complete. Loss:  0.35397768020629883\n",
      "batch 83 complete. Loss:  0.1807754784822464\n",
      "batch 84 complete. Loss:  0.09526523947715759\n",
      "batch 85 complete. Loss:  0.5135048627853394\n",
      "batch 86 complete. Loss:  0.2976672947406769\n",
      "batch 87 complete. Loss:  0.09952367842197418\n",
      "batch 88 complete. Loss:  0.22331930696964264\n",
      "batch 89 complete. Loss:  0.2249913513660431\n",
      "batch 90 complete. Loss:  0.45216405391693115\n",
      "batch 91 complete. Loss:  0.09825850278139114\n",
      "batch 92 complete. Loss:  0.1690642535686493\n",
      "batch 93 complete. Loss:  0.2553175985813141\n",
      "batch 94 complete. Loss:  0.20002935826778412\n",
      "batch 95 complete. Loss:  0.09009076654911041\n",
      "batch 96 complete. Loss:  0.13409247994422913\n",
      "batch 97 complete. Loss:  0.045923955738544464\n",
      "batch 98 complete. Loss:  0.11372964829206467\n",
      "batch 99 complete. Loss:  0.12430183589458466\n",
      "batch 100 complete. Loss:  0.07774601131677628\n",
      "batch 101 complete. Loss:  0.1679593026638031\n",
      "batch 102 complete. Loss:  0.18230555951595306\n",
      "batch 103 complete. Loss:  0.050046585500240326\n",
      "batch 104 complete. Loss:  0.08227378129959106\n",
      "batch 105 complete. Loss:  0.19115129113197327\n",
      "batch 106 complete. Loss:  0.09601445496082306\n",
      "batch 107 complete. Loss:  0.09791811555624008\n",
      "batch 108 complete. Loss:  0.10597416013479233\n",
      "batch 109 complete. Loss:  0.16157540678977966\n",
      "batch 110 complete. Loss:  0.060110270977020264\n",
      "batch 111 complete. Loss:  0.08571933209896088\n",
      "batch 112 complete. Loss:  0.11246349662542343\n",
      "batch 113 complete. Loss:  0.20213301479816437\n",
      "batch 114 complete. Loss:  0.09064024686813354\n",
      "batch 115 complete. Loss:  0.0844469666481018\n",
      "batch 116 complete. Loss:  0.0825207307934761\n",
      "batch 117 complete. Loss:  0.05174444615840912\n",
      "batch 118 complete. Loss:  0.15091979503631592\n",
      "batch 119 complete. Loss:  0.14534051716327667\n",
      "batch 120 complete. Loss:  0.11171131581068039\n",
      "batch 121 complete. Loss:  0.055342547595500946\n",
      "batch 122 complete. Loss:  0.15874618291854858\n",
      "batch 123 complete. Loss:  0.22639703750610352\n",
      "batch 124 complete. Loss:  0.1190093606710434\n",
      "batch 125 complete. Loss:  0.19677948951721191\n",
      "batch 126 complete. Loss:  0.030128005892038345\n",
      "batch 127 complete. Loss:  0.0463150218129158\n",
      "batch 128 complete. Loss:  0.07874701172113419\n",
      "batch 129 complete. Loss:  0.11884592473506927\n",
      "batch 130 complete. Loss:  0.053092580288648605\n",
      "batch 131 complete. Loss:  0.07153362035751343\n",
      "batch 132 complete. Loss:  0.09871425479650497\n",
      "batch 133 complete. Loss:  0.07796187698841095\n",
      "batch 134 complete. Loss:  0.036389123648405075\n",
      "batch 135 complete. Loss:  0.051765456795692444\n",
      "batch 136 complete. Loss:  0.1250680685043335\n",
      "batch 137 complete. Loss:  0.06895342469215393\n",
      "batch 138 complete. Loss:  0.03412432223558426\n",
      "batch 139 complete. Loss:  0.046784769743680954\n",
      "batch 140 complete. Loss:  0.08162115514278412\n",
      "batch 141 complete. Loss:  0.033987969160079956\n",
      "batch 142 complete. Loss:  0.07903394103050232\n",
      "batch 143 complete. Loss:  0.04836937040090561\n",
      "batch 144 complete. Loss:  0.11952240765094757\n",
      "batch 145 complete. Loss:  0.09453237801790237\n",
      "batch 146 complete. Loss:  0.13705769181251526\n",
      "batch 147 complete. Loss:  0.07677671313285828\n",
      "batch 148 complete. Loss:  0.0718914344906807\n",
      "batch 149 complete. Loss:  0.09847280383110046\n",
      "batch 150 complete. Loss:  0.2658502161502838\n",
      "batch 151 complete. Loss:  0.17440713942050934\n",
      "batch 152 complete. Loss:  0.03254499286413193\n",
      "batch 153 complete. Loss:  0.03309088945388794\n",
      "batch 154 complete. Loss:  0.15920744836330414\n",
      "batch 155 complete. Loss:  0.03554954007267952\n",
      "batch 156 complete. Loss:  0.06951180100440979\n",
      "batch 157 complete. Loss:  0.0799056813120842\n",
      "batch 158 complete. Loss:  0.041289810091257095\n",
      "batch 159 complete. Loss:  0.29618826508522034\n",
      "batch 160 complete. Loss:  0.12032859027385712\n",
      "batch 161 complete. Loss:  0.0645788162946701\n",
      "batch 162 complete. Loss:  0.1121370792388916\n",
      "batch 163 complete. Loss:  0.06894007325172424\n",
      "batch 164 complete. Loss:  0.043519772589206696\n",
      "batch 165 complete. Loss:  0.18398356437683105\n",
      "batch 166 complete. Loss:  0.06651129573583603\n",
      "batch 167 complete. Loss:  0.06407718360424042\n",
      "batch 168 complete. Loss:  0.08134935051202774\n",
      "batch 169 complete. Loss:  0.0618395134806633\n",
      "batch 170 complete. Loss:  0.0927368700504303\n",
      "batch 171 complete. Loss:  0.04134231433272362\n",
      "batch 172 complete. Loss:  0.05906709283590317\n",
      "batch 173 complete. Loss:  0.04346625506877899\n",
      "batch 174 complete. Loss:  0.03862340748310089\n",
      "batch 175 complete. Loss:  0.02654089778661728\n",
      "batch 176 complete. Loss:  0.11742355674505234\n",
      "batch 177 complete. Loss:  0.04028265178203583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 178 complete. Loss:  0.0431470163166523\n",
      "batch 179 complete. Loss:  0.02094198204576969\n",
      "batch 180 complete. Loss:  0.02937246859073639\n",
      "batch 181 complete. Loss:  0.1320217400789261\n",
      "batch 182 complete. Loss:  0.043571509420871735\n",
      "batch 183 complete. Loss:  0.04479469358921051\n",
      "batch 184 complete. Loss:  0.04925588145852089\n",
      "batch 185 complete. Loss:  0.04533040151000023\n",
      "batch 186 complete. Loss:  0.04253024607896805\n",
      "batch 187 complete. Loss:  0.11066080629825592\n",
      "batch 188 complete. Loss:  0.033809177577495575\n",
      "batch 189 complete. Loss:  0.05635162070393562\n",
      "batch 190 complete. Loss:  0.17995309829711914\n",
      "batch 191 complete. Loss:  0.2043861597776413\n",
      "batch 192 complete. Loss:  0.024013882502913475\n",
      "batch 193 complete. Loss:  0.08927648514509201\n",
      "batch 194 complete. Loss:  0.19663158059120178\n",
      "batch 195 complete. Loss:  0.0978340208530426\n",
      "batch 196 complete. Loss:  0.06178425997495651\n",
      "batch 197 complete. Loss:  0.1168874055147171\n",
      "batch 198 complete. Loss:  0.027888400480151176\n",
      "batch 199 complete. Loss:  0.15966030955314636\n",
      "batch 200 complete. Loss:  0.04047757759690285\n",
      "batch 201 complete. Loss:  0.025144904851913452\n",
      "batch 202 complete. Loss:  0.09491857886314392\n",
      "batch 203 complete. Loss:  0.0974477082490921\n",
      "batch 204 complete. Loss:  0.036091726273298264\n",
      "batch 205 complete. Loss:  0.040395718067884445\n",
      "batch 206 complete. Loss:  0.14362698793411255\n",
      "batch 207 complete. Loss:  0.09637533128261566\n",
      "batch 208 complete. Loss:  0.07178696244955063\n",
      "batch 209 complete. Loss:  0.04131363332271576\n",
      "batch 210 complete. Loss:  0.03010038658976555\n",
      "batch 211 complete. Loss:  0.06537434458732605\n",
      "batch 212 complete. Loss:  0.08885154128074646\n",
      "batch 213 complete. Loss:  0.0492175817489624\n",
      "batch 214 complete. Loss:  0.055517129600048065\n",
      "batch 215 complete. Loss:  0.0761033445596695\n",
      "batch 216 complete. Loss:  0.09606578201055527\n",
      "batch 217 complete. Loss:  0.04684659093618393\n",
      "batch 218 complete. Loss:  0.04535933583974838\n",
      "batch 219 complete. Loss:  0.15010136365890503\n",
      "batch 220 complete. Loss:  0.28803133964538574\n",
      "batch 221 complete. Loss:  0.01759530045092106\n",
      "batch 222 complete. Loss:  0.031643375754356384\n",
      "batch 223 complete. Loss:  0.08288107067346573\n",
      "batch 224 complete. Loss:  0.05670119449496269\n",
      "batch 225 complete. Loss:  0.058770984411239624\n",
      "batch 226 complete. Loss:  0.0412948876619339\n",
      "batch 227 complete. Loss:  0.1058443933725357\n",
      "batch 228 complete. Loss:  0.11062690615653992\n",
      "batch 229 complete. Loss:  0.02369418740272522\n",
      "batch 230 complete. Loss:  0.03615356236696243\n",
      "batch 231 complete. Loss:  0.04031132906675339\n",
      "batch 232 complete. Loss:  0.09200134873390198\n",
      "batch 233 complete. Loss:  0.13023607432842255\n",
      "batch 234 complete. Loss:  0.03947554528713226\n",
      "batch 235 complete. Loss:  0.06761506199836731\n",
      "batch 236 complete. Loss:  0.10753639042377472\n",
      "batch 237 complete. Loss:  0.028060825541615486\n",
      "batch 238 complete. Loss:  0.06072311848402023\n",
      "batch 239 complete. Loss:  0.03547321632504463\n",
      "batch 240 complete. Loss:  0.04499351233243942\n",
      "batch 241 complete. Loss:  0.04005499556660652\n",
      "batch 242 complete. Loss:  0.1189969927072525\n",
      "batch 243 complete. Loss:  0.022778086364269257\n",
      "batch 244 complete. Loss:  0.042268168181180954\n",
      "batch 245 complete. Loss:  0.17798101902008057\n",
      "batch 246 complete. Loss:  0.022807179018855095\n",
      "batch 247 complete. Loss:  0.020484210923314095\n",
      "batch 248 complete. Loss:  0.024055451154708862\n",
      "batch 249 complete. Loss:  0.1612982451915741\n",
      "batch 250 complete. Loss:  0.06867410242557526\n",
      "batch 251 complete. Loss:  0.055810362100601196\n",
      "batch 252 complete. Loss:  0.09341038018465042\n",
      "batch 253 complete. Loss:  0.04710525646805763\n",
      "batch 254 complete. Loss:  0.06142246350646019\n",
      "batch 255 complete. Loss:  0.018221139907836914\n",
      "batch 256 complete. Loss:  0.04808315634727478\n",
      "batch 257 complete. Loss:  0.11521303653717041\n",
      "batch 258 complete. Loss:  0.16012175381183624\n",
      "batch 259 complete. Loss:  0.09695816040039062\n",
      "batch 260 complete. Loss:  0.03567208722233772\n",
      "batch 261 complete. Loss:  0.022846311330795288\n",
      "batch 262 complete. Loss:  0.04588521271944046\n",
      "batch 263 complete. Loss:  0.06039038300514221\n",
      "batch 264 complete. Loss:  0.06727077066898346\n",
      "batch 265 complete. Loss:  0.0576409213244915\n",
      "batch 266 complete. Loss:  0.060884639620780945\n",
      "batch 267 complete. Loss:  0.06084030121564865\n",
      "batch 268 complete. Loss:  0.09331200271844864\n",
      "batch 269 complete. Loss:  0.06518608331680298\n",
      "batch 270 complete. Loss:  0.2925938367843628\n",
      "batch 271 complete. Loss:  0.06375952810049057\n",
      "batch 272 complete. Loss:  0.07644176483154297\n",
      "batch 273 complete. Loss:  0.03292141854763031\n",
      "batch 274 complete. Loss:  0.06437220424413681\n",
      "batch 275 complete. Loss:  0.02435041218996048\n",
      "batch 276 complete. Loss:  0.060384392738342285\n",
      "batch 277 complete. Loss:  0.02834942378103733\n",
      "batch 278 complete. Loss:  0.17406107485294342\n",
      "batch 279 complete. Loss:  0.03671040013432503\n",
      "batch 280 complete. Loss:  0.0614774189889431\n",
      "batch 281 complete. Loss:  0.045974940061569214\n",
      "batch 282 complete. Loss:  0.1027362048625946\n",
      "batch 283 complete. Loss:  0.0951487272977829\n",
      "batch 284 complete. Loss:  0.08355893939733505\n",
      "batch 285 complete. Loss:  0.04951958358287811\n",
      "batch 286 complete. Loss:  0.052934132516384125\n",
      "batch 287 complete. Loss:  0.04145289957523346\n",
      "batch 288 complete. Loss:  0.05154789984226227\n",
      "batch 289 complete. Loss:  0.09053781628608704\n",
      "batch 290 complete. Loss:  0.03245609998703003\n",
      "batch 291 complete. Loss:  0.12982547283172607\n",
      "batch 292 complete. Loss:  0.06906331330537796\n",
      "batch 293 complete. Loss:  0.035383742302656174\n",
      "batch 294 complete. Loss:  0.10514829307794571\n",
      "batch 295 complete. Loss:  0.053481727838516235\n",
      "batch 296 complete. Loss:  0.030236313119530678\n",
      "batch 297 complete. Loss:  0.02762344479560852\n",
      "batch 298 complete. Loss:  0.10086597502231598\n",
      "batch 299 complete. Loss:  0.030205892398953438\n",
      "batch 300 complete. Loss:  0.08767139911651611\n",
      "batch 301 complete. Loss:  0.07825462520122528\n",
      "batch 302 complete. Loss:  0.022937089204788208\n",
      "batch 303 complete. Loss:  0.04486895352602005\n",
      "batch 304 complete. Loss:  0.08890825510025024\n",
      "batch 305 complete. Loss:  0.11329840123653412\n",
      "batch 306 complete. Loss:  0.02738833986222744\n",
      "batch 307 complete. Loss:  0.10293623805046082\n",
      "batch 308 complete. Loss:  0.04992607608437538\n",
      "batch 309 complete. Loss:  0.011356573551893234\n",
      "batch 310 complete. Loss:  0.024811800569295883\n",
      "batch 311 complete. Loss:  0.039461635053157806\n",
      "batch 312 complete. Loss:  0.18773715198040009\n",
      "batch 313 complete. Loss:  0.07420118898153305\n",
      "batch 314 complete. Loss:  0.05121202766895294\n",
      "batch 315 complete. Loss:  0.05631866678595543\n",
      "batch 316 complete. Loss:  0.08140105754137039\n",
      "batch 317 complete. Loss:  0.03339502960443497\n",
      "batch 318 complete. Loss:  0.02391282096505165\n",
      "batch 319 complete. Loss:  0.03733189404010773\n",
      "batch 320 complete. Loss:  0.021420905366539955\n",
      "batch 321 complete. Loss:  0.081216961145401\n",
      "batch 322 complete. Loss:  0.13080227375030518\n",
      "batch 323 complete. Loss:  0.014533581212162971\n",
      "batch 324 complete. Loss:  0.019946565851569176\n",
      "batch 325 complete. Loss:  0.048744991421699524\n",
      "batch 326 complete. Loss:  0.03439153730869293\n",
      "batch 327 complete. Loss:  0.05614043027162552\n",
      "batch 328 complete. Loss:  0.05549274757504463\n",
      "batch 329 complete. Loss:  0.040876567363739014\n",
      "batch 330 complete. Loss:  0.04467286542057991\n",
      "batch 331 complete. Loss:  0.03693818300962448\n",
      "batch 332 complete. Loss:  0.03613197058439255\n",
      "batch 333 complete. Loss:  0.07704338431358337\n",
      "batch 334 complete. Loss:  0.12054657191038132\n",
      "batch 335 complete. Loss:  0.040334947407245636\n",
      "batch 336 complete. Loss:  0.03719563037157059\n",
      "batch 337 complete. Loss:  0.028257280588150024\n",
      "batch 338 complete. Loss:  0.054115571081638336\n",
      "batch 339 complete. Loss:  0.02934863790869713\n",
      "batch 340 complete. Loss:  0.03649221360683441\n",
      "batch 341 complete. Loss:  0.05331011116504669\n",
      "batch 342 complete. Loss:  0.028876863420009613\n",
      "batch 343 complete. Loss:  0.10350284725427628\n",
      "batch 344 complete. Loss:  0.0256427600979805\n",
      "batch 345 complete. Loss:  0.037619560956954956\n",
      "batch 346 complete. Loss:  0.06445334851741791\n",
      "batch 347 complete. Loss:  0.06429670751094818\n",
      "batch 348 complete. Loss:  0.03407198190689087\n",
      "batch 349 complete. Loss:  0.025268428027629852\n",
      "batch 350 complete. Loss:  0.05871766060590744\n",
      "batch 351 complete. Loss:  0.05558818578720093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 352 complete. Loss:  0.06490476429462433\n",
      "batch 353 complete. Loss:  0.05915256217122078\n",
      "batch 354 complete. Loss:  0.02690613642334938\n",
      "batch 355 complete. Loss:  0.03757155314087868\n",
      "batch 356 complete. Loss:  0.026608631014823914\n",
      "batch 357 complete. Loss:  0.055934179574251175\n",
      "batch 358 complete. Loss:  0.08040249347686768\n",
      "batch 359 complete. Loss:  0.01655518263578415\n",
      "batch 360 complete. Loss:  0.031405504792928696\n",
      "batch 361 complete. Loss:  0.030721575021743774\n",
      "batch 362 complete. Loss:  0.04137811064720154\n",
      "batch 363 complete. Loss:  0.047429896891117096\n",
      "batch 364 complete. Loss:  0.13159729540348053\n",
      "batch 365 complete. Loss:  0.02530796453356743\n",
      "batch 366 complete. Loss:  0.08534310758113861\n",
      "batch 367 complete. Loss:  0.017725884914398193\n",
      "batch 368 complete. Loss:  0.07496778666973114\n",
      "batch 369 complete. Loss:  0.0271858349442482\n",
      "batch 370 complete. Loss:  0.058033011853694916\n",
      "batch 371 complete. Loss:  0.056070003658533096\n",
      "batch 372 complete. Loss:  0.044279031455516815\n",
      "batch 373 complete. Loss:  0.028158897534012794\n",
      "batch 374 complete. Loss:  0.08030416071414948\n",
      "batch 375 complete. Loss:  0.030819548293948174\n",
      "batch 376 complete. Loss:  0.021537911146879196\n",
      "batch 377 complete. Loss:  0.04978564381599426\n",
      "batch 378 complete. Loss:  0.03795982897281647\n",
      "batch 379 complete. Loss:  0.030778173357248306\n",
      "batch 380 complete. Loss:  0.07265534996986389\n",
      "batch 381 complete. Loss:  0.018514957278966904\n",
      "batch 382 complete. Loss:  0.07146096229553223\n",
      "batch 383 complete. Loss:  0.03271445259451866\n",
      "batch 384 complete. Loss:  0.07346144318580627\n",
      "batch 385 complete. Loss:  0.0650162324309349\n",
      "batch 386 complete. Loss:  0.023526007309556007\n",
      "batch 387 complete. Loss:  0.026521902531385422\n",
      "batch 388 complete. Loss:  0.030351940542459488\n",
      "batch 389 complete. Loss:  0.03375580906867981\n",
      "batch 390 complete. Loss:  0.04814043268561363\n",
      "batch 391 complete. Loss:  0.050324685871601105\n",
      "batch 392 complete. Loss:  0.021253516897559166\n",
      "batch 393 complete. Loss:  0.11713911592960358\n",
      "batch 394 complete. Loss:  0.06672057509422302\n",
      "batch 395 complete. Loss:  0.0365031436085701\n",
      "batch 396 complete. Loss:  0.011556308716535568\n",
      "batch 397 complete. Loss:  0.02185521274805069\n",
      "batch 398 complete. Loss:  0.028228767216205597\n",
      "batch 399 complete. Loss:  0.03379019722342491\n",
      "batch 400 complete. Loss:  0.05754677578806877\n",
      "batch 401 complete. Loss:  0.06264256685972214\n",
      "batch 402 complete. Loss:  0.019329892471432686\n",
      "batch 403 complete. Loss:  0.045455798506736755\n",
      "batch 404 complete. Loss:  0.10479484498500824\n",
      "batch 405 complete. Loss:  0.035505324602127075\n",
      "batch 406 complete. Loss:  0.04769250750541687\n",
      "batch 407 complete. Loss:  0.018545493483543396\n",
      "batch 408 complete. Loss:  0.043940819799900055\n",
      "batch 409 complete. Loss:  0.07039773464202881\n",
      "batch 410 complete. Loss:  0.02254321798682213\n",
      "batch 411 complete. Loss:  0.03726750612258911\n",
      "batch 412 complete. Loss:  0.04126144200563431\n",
      "batch 413 complete. Loss:  0.016573969274759293\n",
      "batch 414 complete. Loss:  0.045309875160455704\n",
      "batch 415 complete. Loss:  0.043228961527347565\n",
      "batch 416 complete. Loss:  0.2615242600440979\n",
      "batch 417 complete. Loss:  0.08172781765460968\n",
      "batch 418 complete. Loss:  0.03382304310798645\n",
      "batch 419 complete. Loss:  0.015464399009943008\n",
      "batch 420 complete. Loss:  0.06050839647650719\n",
      "batch 421 complete. Loss:  0.02210168167948723\n",
      "batch 422 complete. Loss:  0.026729457080364227\n",
      "batch 423 complete. Loss:  0.04869634658098221\n",
      "batch 424 complete. Loss:  0.04062626138329506\n",
      "batch 425 complete. Loss:  0.03887730464339256\n",
      "batch 426 complete. Loss:  0.0568835474550724\n",
      "batch 427 complete. Loss:  0.028041571378707886\n",
      "batch 428 complete. Loss:  0.055863820016384125\n",
      "batch 429 complete. Loss:  0.048257481306791306\n",
      "batch 430 complete. Loss:  0.02957545965909958\n",
      "batch 431 complete. Loss:  0.03608401119709015\n",
      "batch 432 complete. Loss:  0.13496354222297668\n",
      "batch 433 complete. Loss:  0.024521591141819954\n",
      "batch 434 complete. Loss:  0.14922523498535156\n",
      "batch 435 complete. Loss:  0.015431895852088928\n",
      "batch 436 complete. Loss:  0.0657738596200943\n",
      "batch 437 complete. Loss:  0.039643384516239166\n",
      "batch 438 complete. Loss:  0.044015318155288696\n",
      "batch 439 complete. Loss:  0.031192932277917862\n",
      "batch 440 complete. Loss:  0.04347433149814606\n",
      "batch 441 complete. Loss:  0.05311790853738785\n",
      "batch 442 complete. Loss:  0.08643932640552521\n",
      "batch 443 complete. Loss:  0.04341523349285126\n",
      "batch 444 complete. Loss:  0.05156544968485832\n",
      "batch 445 complete. Loss:  0.042806487530469894\n",
      "batch 446 complete. Loss:  0.021664857864379883\n",
      "batch 447 complete. Loss:  0.01635594852268696\n",
      "batch 448 complete. Loss:  0.06481494009494781\n",
      "batch 449 complete. Loss:  0.11596061289310455\n",
      "batch 450 complete. Loss:  0.045534130185842514\n",
      "batch 451 complete. Loss:  0.03992513567209244\n",
      "batch 452 complete. Loss:  0.01269887387752533\n",
      "batch 453 complete. Loss:  0.04211021214723587\n",
      "batch 454 complete. Loss:  0.016468167304992676\n",
      "batch 455 complete. Loss:  0.030958320945501328\n",
      "batch 456 complete. Loss:  0.055972591042518616\n",
      "batch 457 complete. Loss:  0.04066799581050873\n",
      "batch 458 complete. Loss:  0.019693639129400253\n",
      "batch 459 complete. Loss:  0.011782357469201088\n",
      "batch 460 complete. Loss:  0.04500211775302887\n",
      "batch 461 complete. Loss:  0.02710404247045517\n",
      "batch 462 complete. Loss:  0.0194002203643322\n",
      "batch 463 complete. Loss:  0.02831682190299034\n",
      "batch 464 complete. Loss:  0.030332960188388824\n",
      "batch 465 complete. Loss:  0.019234318286180496\n",
      "batch 466 complete. Loss:  0.029424205422401428\n",
      "batch 467 complete. Loss:  0.06278806179761887\n",
      "batch 468 complete. Loss:  0.1166701465845108\n",
      "batch 469 complete. Loss:  0.0246746763586998\n",
      "batch 470 complete. Loss:  0.044581420719623566\n",
      "batch 471 complete. Loss:  0.029816197231411934\n",
      "batch 472 complete. Loss:  0.019724685698747635\n",
      "batch 473 complete. Loss:  0.06796177476644516\n",
      "batch 474 complete. Loss:  0.022602777928113937\n",
      "batch 475 complete. Loss:  0.0345386266708374\n",
      "batch 476 complete. Loss:  0.03315189480781555\n",
      "batch 477 complete. Loss:  0.02139451913535595\n",
      "batch 478 complete. Loss:  0.039708591997623444\n",
      "batch 479 complete. Loss:  0.07462694495916367\n",
      "batch 480 complete. Loss:  0.03893488645553589\n",
      "batch 481 complete. Loss:  0.060631051659584045\n",
      "batch 482 complete. Loss:  0.03261970356106758\n",
      "batch 483 complete. Loss:  0.024132687598466873\n",
      "batch 484 complete. Loss:  0.04633355885744095\n",
      "batch 485 complete. Loss:  0.024405743926763535\n",
      "batch 486 complete. Loss:  0.027544673532247543\n",
      "batch 487 complete. Loss:  0.02494809404015541\n",
      "batch 488 complete. Loss:  0.0982498824596405\n",
      "batch 489 complete. Loss:  0.03875213861465454\n",
      "batch 490 complete. Loss:  0.023597102612257004\n",
      "batch 491 complete. Loss:  0.035285234451293945\n",
      "batch 492 complete. Loss:  0.06292831152677536\n",
      "batch 493 complete. Loss:  0.15980985760688782\n",
      "batch 494 complete. Loss:  0.01998540572822094\n",
      "batch 495 complete. Loss:  0.018141139298677444\n",
      "batch 496 complete. Loss:  0.0676303431391716\n",
      "batch 497 complete. Loss:  0.04538627341389656\n",
      "batch 498 complete. Loss:  0.05062456801533699\n",
      "batch 499 complete. Loss:  0.03799104318022728\n",
      "batch 500 complete. Loss:  0.05951128154993057\n",
      "batch 501 complete. Loss:  0.022716866806149483\n",
      "batch 502 complete. Loss:  0.026668500155210495\n",
      "batch 503 complete. Loss:  0.040841907262802124\n",
      "batch 504 complete. Loss:  0.05548694357275963\n",
      "batch 505 complete. Loss:  0.09906674921512604\n",
      "batch 506 complete. Loss:  0.0566929467022419\n",
      "batch 507 complete. Loss:  0.05127698928117752\n",
      "batch 508 complete. Loss:  0.027004534378647804\n",
      "batch 509 complete. Loss:  0.018047839403152466\n",
      "batch 510 complete. Loss:  0.02508111670613289\n",
      "batch 511 complete. Loss:  0.06150694936513901\n",
      "batch 512 complete. Loss:  0.044449836015701294\n",
      "batch 513 complete. Loss:  0.03568565845489502\n",
      "batch 514 complete. Loss:  0.01828593946993351\n",
      "batch 515 complete. Loss:  0.01925845444202423\n",
      "batch 516 complete. Loss:  0.031465597450733185\n",
      "batch 517 complete. Loss:  0.030274035409092903\n",
      "batch 518 complete. Loss:  0.06332741677761078\n",
      "batch 519 complete. Loss:  0.04550864174962044\n",
      "batch 520 complete. Loss:  0.014360395260155201\n",
      "batch 521 complete. Loss:  0.01914665475487709\n",
      "batch 522 complete. Loss:  0.021873028948903084\n",
      "batch 523 complete. Loss:  0.03772539272904396\n",
      "batch 524 complete. Loss:  0.05217947065830231\n",
      "batch 525 complete. Loss:  0.0610496811568737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 526 complete. Loss:  0.04321522265672684\n",
      "batch 527 complete. Loss:  0.02707824669778347\n",
      "batch 528 complete. Loss:  0.04004845768213272\n",
      "batch 529 complete. Loss:  0.05223090201616287\n",
      "batch 530 complete. Loss:  0.03672068566083908\n",
      "batch 531 complete. Loss:  0.033288635313510895\n",
      "batch 532 complete. Loss:  0.04720262438058853\n",
      "batch 533 complete. Loss:  0.05907497555017471\n",
      "batch 534 complete. Loss:  0.08742944896221161\n",
      "batch 535 complete. Loss:  0.01842883601784706\n",
      "batch 536 complete. Loss:  0.021942105144262314\n",
      "batch 537 complete. Loss:  0.07223717868328094\n",
      "batch 538 complete. Loss:  0.01667710766196251\n",
      "batch 539 complete. Loss:  0.020731467753648758\n",
      "batch 540 complete. Loss:  0.11347804218530655\n",
      "batch 541 complete. Loss:  0.04523283615708351\n",
      "batch 542 complete. Loss:  0.02690810151398182\n",
      "batch 543 complete. Loss:  0.04391837492585182\n",
      "batch 544 complete. Loss:  0.024698717519640923\n",
      "batch 545 complete. Loss:  0.02754676714539528\n",
      "batch 546 complete. Loss:  0.028470680117607117\n",
      "batch 547 complete. Loss:  0.030693918466567993\n",
      "batch 548 complete. Loss:  0.020009201020002365\n",
      "batch 549 complete. Loss:  0.05433065816760063\n",
      "batch 550 complete. Loss:  0.04880780726671219\n",
      "batch 551 complete. Loss:  0.01619662344455719\n",
      "batch 552 complete. Loss:  0.0249883271753788\n",
      "batch 553 complete. Loss:  0.14697657525539398\n",
      "batch 554 complete. Loss:  0.0264446884393692\n",
      "batch 555 complete. Loss:  0.020313002169132233\n",
      "batch 556 complete. Loss:  0.018344556912779808\n",
      "batch 557 complete. Loss:  0.04557659476995468\n",
      "batch 558 complete. Loss:  0.020983461290597916\n",
      "batch 559 complete. Loss:  0.04314662888646126\n",
      "batch 560 complete. Loss:  0.1113208532333374\n",
      "batch 561 complete. Loss:  0.08079442381858826\n",
      "batch 562 complete. Loss:  0.028530603274703026\n",
      "batch 563 complete. Loss:  0.044967394322156906\n",
      "batch 564 complete. Loss:  0.11682350933551788\n",
      "batch 565 complete. Loss:  0.03265899419784546\n",
      "batch 566 complete. Loss:  0.04712315648794174\n",
      "batch 567 complete. Loss:  0.053160637617111206\n",
      "batch 568 complete. Loss:  0.04546276107430458\n",
      "batch 569 complete. Loss:  0.058897122740745544\n",
      "batch 570 complete. Loss:  0.06045674532651901\n",
      "batch 571 complete. Loss:  0.039515651762485504\n",
      "batch 572 complete. Loss:  0.040474943816661835\n",
      "batch 573 complete. Loss:  0.045080818235874176\n",
      "batch 574 complete. Loss:  0.11289430409669876\n",
      "batch 575 complete. Loss:  0.016214143484830856\n",
      "batch 576 complete. Loss:  0.025517240166664124\n",
      "batch 577 complete. Loss:  0.07754047214984894\n",
      "batch 578 complete. Loss:  0.02866930142045021\n",
      "batch 579 complete. Loss:  0.04624374210834503\n",
      "batch 580 complete. Loss:  0.03568539023399353\n",
      "batch 581 complete. Loss:  0.08084388822317123\n",
      "batch 582 complete. Loss:  0.037598468363285065\n",
      "batch 583 complete. Loss:  0.05853446573019028\n",
      "batch 584 complete. Loss:  0.04161445051431656\n",
      "batch 585 complete. Loss:  0.07068503648042679\n",
      "batch 586 complete. Loss:  0.05172098055481911\n",
      "batch 587 complete. Loss:  0.03319280594587326\n",
      "batch 588 complete. Loss:  0.029060594737529755\n",
      "batch 589 complete. Loss:  0.10177120566368103\n",
      "batch 590 complete. Loss:  0.039500437676906586\n",
      "batch 591 complete. Loss:  0.10454581677913666\n",
      "batch 592 complete. Loss:  0.0434543639421463\n",
      "batch 593 complete. Loss:  0.030774181708693504\n",
      "batch 594 complete. Loss:  0.017345037311315536\n",
      "batch 595 complete. Loss:  0.05808866024017334\n",
      "batch 596 complete. Loss:  0.15960532426834106\n",
      "batch 597 complete. Loss:  0.045989252626895905\n",
      "batch 598 complete. Loss:  0.030395057052373886\n",
      "batch 599 complete. Loss:  0.0413859598338604\n",
      "batch 600 complete. Loss:  0.059541039168834686\n",
      "batch 601 complete. Loss:  0.023316869512200356\n",
      "batch 602 complete. Loss:  0.0113306175917387\n",
      "batch 603 complete. Loss:  0.11472246795892715\n",
      "batch 604 complete. Loss:  0.09666518121957779\n",
      "batch 605 complete. Loss:  0.01336499210447073\n",
      "batch 606 complete. Loss:  0.03286273777484894\n",
      "batch 607 complete. Loss:  0.07307545840740204\n",
      "batch 608 complete. Loss:  0.02639452926814556\n",
      "batch 609 complete. Loss:  0.039271704852581024\n",
      "batch 610 complete. Loss:  0.037921611219644547\n",
      "batch 611 complete. Loss:  0.033251963555812836\n",
      "batch 612 complete. Loss:  0.09881140291690826\n",
      "batch 613 complete. Loss:  0.031244002282619476\n",
      "batch 614 complete. Loss:  0.03490563482046127\n",
      "batch 615 complete. Loss:  0.020799467340111732\n",
      "batch 616 complete. Loss:  0.01866888627409935\n",
      "batch 617 complete. Loss:  0.02504049241542816\n",
      "batch 618 complete. Loss:  0.022660523653030396\n",
      "batch 619 complete. Loss:  0.04551490768790245\n",
      "batch 620 complete. Loss:  0.044032372534275055\n",
      "batch 621 complete. Loss:  0.018858348950743675\n",
      "batch 622 complete. Loss:  0.02313471958041191\n",
      "batch 623 complete. Loss:  0.012864051386713982\n",
      "batch 624 complete. Loss:  0.02949260175228119\n",
      "batch 625 complete. Loss:  0.022252744063735008\n",
      "batch 626 complete. Loss:  0.03636308014392853\n",
      "batch 627 complete. Loss:  0.0243983156979084\n",
      "batch 628 complete. Loss:  0.029134541749954224\n",
      "batch 629 complete. Loss:  0.018561042845249176\n",
      "batch 630 complete. Loss:  0.04738621413707733\n",
      "batch 631 complete. Loss:  0.03628179430961609\n",
      "batch 632 complete. Loss:  0.033207595348358154\n",
      "batch 633 complete. Loss:  0.10020323097705841\n",
      "batch 634 complete. Loss:  0.034125179052352905\n",
      "batch 635 complete. Loss:  0.025377020239830017\n",
      "batch 636 complete. Loss:  0.018270255997776985\n",
      "batch 637 complete. Loss:  0.02853129804134369\n",
      "batch 638 complete. Loss:  0.017950505018234253\n",
      "batch 639 complete. Loss:  0.03141766041517258\n",
      "batch 640 complete. Loss:  0.02735632285475731\n",
      "batch 641 complete. Loss:  0.03005620464682579\n",
      "batch 642 complete. Loss:  0.06398393213748932\n",
      "batch 643 complete. Loss:  0.08333422988653183\n",
      "batch 644 complete. Loss:  0.03010978177189827\n",
      "batch 645 complete. Loss:  0.031038790941238403\n",
      "batch 646 complete. Loss:  0.0327468067407608\n",
      "batch 647 complete. Loss:  0.02154669165611267\n",
      "batch 648 complete. Loss:  0.05137152969837189\n",
      "batch 649 complete. Loss:  0.022526519373059273\n",
      "batch 650 complete. Loss:  0.04970351606607437\n",
      "batch 651 complete. Loss:  0.018400117754936218\n",
      "batch 652 complete. Loss:  0.022221624851226807\n",
      "batch 653 complete. Loss:  0.03361961990594864\n",
      "batch 654 complete. Loss:  0.02456863597035408\n",
      "batch 655 complete. Loss:  0.019221166148781776\n",
      "batch 656 complete. Loss:  0.024064352735877037\n",
      "batch 657 complete. Loss:  0.030654722824692726\n",
      "batch 658 complete. Loss:  0.1761104166507721\n",
      "batch 659 complete. Loss:  0.09017367660999298\n",
      "batch 660 complete. Loss:  0.04939105361700058\n",
      "batch 661 complete. Loss:  0.10618958622217178\n",
      "batch 662 complete. Loss:  0.0173359252512455\n",
      "batch 663 complete. Loss:  0.021395407617092133\n",
      "batch 664 complete. Loss:  0.12710820138454437\n",
      "batch 665 complete. Loss:  0.018997710198163986\n",
      "batch 666 complete. Loss:  0.018619805574417114\n",
      "batch 667 complete. Loss:  0.07726480811834335\n",
      "batch 668 complete. Loss:  0.027870744466781616\n",
      "batch 669 complete. Loss:  0.02080398052930832\n",
      "batch 670 complete. Loss:  0.03433191403746605\n",
      "batch 671 complete. Loss:  0.013517060317099094\n",
      "batch 672 complete. Loss:  0.093109130859375\n",
      "batch 673 complete. Loss:  0.0200408473610878\n",
      "batch 674 complete. Loss:  0.031849682331085205\n",
      "Epoch 0 Validation Loss: 0.043224400289621226\n",
      "1 / 10\n",
      "batch 0 complete. Loss:  0.037553124129772186\n",
      "batch 1 complete. Loss:  0.03994248807430267\n",
      "batch 2 complete. Loss:  0.01694544032216072\n",
      "batch 3 complete. Loss:  0.026952136307954788\n",
      "batch 4 complete. Loss:  0.027134358882904053\n",
      "batch 5 complete. Loss:  0.03552890568971634\n",
      "batch 6 complete. Loss:  0.009324506856501102\n",
      "batch 7 complete. Loss:  0.016687937080860138\n",
      "batch 8 complete. Loss:  0.0559292696416378\n",
      "batch 9 complete. Loss:  0.04290832206606865\n",
      "batch 10 complete. Loss:  0.0212356336414814\n",
      "batch 11 complete. Loss:  0.06595677882432938\n",
      "batch 12 complete. Loss:  0.03385310620069504\n",
      "batch 13 complete. Loss:  0.049655526876449585\n",
      "batch 14 complete. Loss:  0.09570370614528656\n",
      "batch 15 complete. Loss:  0.014301164075732231\n",
      "batch 16 complete. Loss:  0.01783604547381401\n",
      "batch 17 complete. Loss:  0.019904211163520813\n",
      "batch 18 complete. Loss:  0.046643879264593124\n",
      "batch 19 complete. Loss:  0.02696240320801735\n",
      "batch 20 complete. Loss:  0.013258075341582298\n",
      "batch 21 complete. Loss:  0.022504102438688278\n",
      "batch 22 complete. Loss:  0.010819830000400543\n",
      "batch 23 complete. Loss:  0.089600570499897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 24 complete. Loss:  0.019598012790083885\n",
      "batch 25 complete. Loss:  0.020826928317546844\n",
      "batch 26 complete. Loss:  0.04102468490600586\n",
      "batch 27 complete. Loss:  0.038473695516586304\n",
      "batch 28 complete. Loss:  0.019475728273391724\n",
      "batch 29 complete. Loss:  0.022347252815961838\n",
      "batch 30 complete. Loss:  0.023358067497611046\n",
      "batch 31 complete. Loss:  0.018502432852983475\n",
      "batch 32 complete. Loss:  0.02247358113527298\n",
      "batch 33 complete. Loss:  0.03230348974466324\n",
      "batch 34 complete. Loss:  0.02879701368510723\n",
      "batch 35 complete. Loss:  0.05130987614393234\n",
      "batch 36 complete. Loss:  0.011680712923407555\n",
      "batch 37 complete. Loss:  0.019409822300076485\n",
      "batch 38 complete. Loss:  0.027404453605413437\n",
      "batch 39 complete. Loss:  0.019142955541610718\n",
      "batch 40 complete. Loss:  0.03062751144170761\n",
      "batch 41 complete. Loss:  0.05143783614039421\n",
      "batch 42 complete. Loss:  0.022565215826034546\n",
      "batch 43 complete. Loss:  0.023946085944771767\n",
      "batch 44 complete. Loss:  0.05313175916671753\n",
      "batch 45 complete. Loss:  0.025672227144241333\n",
      "batch 46 complete. Loss:  0.029033010825514793\n",
      "batch 47 complete. Loss:  0.015981189906597137\n",
      "batch 48 complete. Loss:  0.031883567571640015\n",
      "batch 49 complete. Loss:  0.02305879071354866\n",
      "batch 50 complete. Loss:  0.024244841188192368\n",
      "batch 51 complete. Loss:  0.014522879384458065\n",
      "batch 52 complete. Loss:  0.016938548535108566\n",
      "batch 53 complete. Loss:  0.028223082423210144\n",
      "batch 54 complete. Loss:  0.014887135475873947\n",
      "batch 55 complete. Loss:  0.03383197262883186\n",
      "batch 56 complete. Loss:  0.015230432152748108\n",
      "batch 57 complete. Loss:  0.021101240068674088\n",
      "batch 58 complete. Loss:  0.01731017418205738\n",
      "batch 59 complete. Loss:  0.029978670179843903\n",
      "batch 60 complete. Loss:  0.020537791773676872\n",
      "batch 61 complete. Loss:  0.012799141928553581\n",
      "batch 62 complete. Loss:  0.04862815886735916\n",
      "batch 63 complete. Loss:  0.018776407465338707\n",
      "batch 64 complete. Loss:  0.031240709125995636\n",
      "batch 65 complete. Loss:  0.024241503328084946\n",
      "batch 66 complete. Loss:  0.017123091965913773\n",
      "batch 67 complete. Loss:  0.09725024551153183\n",
      "batch 68 complete. Loss:  0.017356891185045242\n",
      "batch 69 complete. Loss:  0.04617898166179657\n",
      "batch 70 complete. Loss:  0.05972321331501007\n",
      "batch 71 complete. Loss:  0.030161680653691292\n",
      "batch 72 complete. Loss:  0.011625986546278\n",
      "batch 73 complete. Loss:  0.02678058296442032\n",
      "batch 74 complete. Loss:  0.023341437801718712\n",
      "batch 75 complete. Loss:  0.010030580684542656\n",
      "batch 76 complete. Loss:  0.01413765363395214\n",
      "batch 77 complete. Loss:  0.10446952283382416\n",
      "batch 78 complete. Loss:  0.020312611013650894\n",
      "batch 79 complete. Loss:  0.020038627088069916\n",
      "batch 80 complete. Loss:  0.009712356142699718\n",
      "batch 81 complete. Loss:  0.02365795150399208\n",
      "batch 82 complete. Loss:  0.03071466274559498\n",
      "batch 83 complete. Loss:  0.07031180709600449\n",
      "batch 84 complete. Loss:  0.027381137013435364\n",
      "batch 85 complete. Loss:  0.056994687765836716\n",
      "batch 86 complete. Loss:  0.028564680367708206\n",
      "batch 87 complete. Loss:  0.07039980590343475\n",
      "batch 88 complete. Loss:  0.02658403292298317\n",
      "batch 89 complete. Loss:  0.019054245203733444\n",
      "batch 90 complete. Loss:  0.07759088277816772\n",
      "batch 91 complete. Loss:  0.047491829842329025\n",
      "batch 92 complete. Loss:  0.028783544898033142\n",
      "batch 93 complete. Loss:  0.018976222723722458\n",
      "batch 94 complete. Loss:  0.012529218569397926\n",
      "batch 95 complete. Loss:  0.051047202199697495\n",
      "batch 96 complete. Loss:  0.016800757497549057\n",
      "batch 97 complete. Loss:  0.017241492867469788\n",
      "batch 98 complete. Loss:  0.01364610344171524\n",
      "batch 99 complete. Loss:  0.02803187258541584\n",
      "batch 100 complete. Loss:  0.06516838073730469\n",
      "batch 101 complete. Loss:  0.014627495780587196\n",
      "batch 102 complete. Loss:  0.01804526150226593\n",
      "batch 103 complete. Loss:  0.03405516594648361\n",
      "batch 104 complete. Loss:  0.018149545416235924\n",
      "batch 105 complete. Loss:  0.02087532728910446\n",
      "batch 106 complete. Loss:  0.044664472341537476\n",
      "batch 107 complete. Loss:  0.021338967606425285\n",
      "batch 108 complete. Loss:  0.03388853743672371\n",
      "batch 109 complete. Loss:  0.024977225810289383\n",
      "batch 110 complete. Loss:  0.057375647127628326\n",
      "batch 111 complete. Loss:  0.05455108731985092\n",
      "batch 112 complete. Loss:  0.02546071447432041\n",
      "batch 113 complete. Loss:  0.04546355456113815\n",
      "batch 114 complete. Loss:  0.06941826641559601\n",
      "batch 115 complete. Loss:  0.0424104705452919\n",
      "batch 116 complete. Loss:  0.026238851249217987\n",
      "batch 117 complete. Loss:  0.02399420365691185\n",
      "batch 118 complete. Loss:  0.03380962461233139\n",
      "batch 119 complete. Loss:  0.043652668595314026\n",
      "batch 120 complete. Loss:  0.06408128887414932\n",
      "batch 121 complete. Loss:  0.034172143787145615\n",
      "batch 122 complete. Loss:  0.015833787620067596\n",
      "batch 123 complete. Loss:  0.0388651005923748\n",
      "batch 124 complete. Loss:  0.04001733660697937\n",
      "batch 125 complete. Loss:  0.02371164597570896\n",
      "batch 126 complete. Loss:  0.02001022920012474\n",
      "batch 127 complete. Loss:  0.04351940006017685\n",
      "batch 128 complete. Loss:  0.03498302400112152\n",
      "batch 129 complete. Loss:  0.0185028538107872\n",
      "batch 130 complete. Loss:  0.03856885805726051\n",
      "batch 131 complete. Loss:  0.033213596791028976\n",
      "batch 132 complete. Loss:  0.033860787749290466\n",
      "batch 133 complete. Loss:  0.048970822244882584\n",
      "batch 134 complete. Loss:  0.027201281860470772\n",
      "batch 135 complete. Loss:  0.04101862013339996\n",
      "batch 136 complete. Loss:  0.026558278128504753\n",
      "batch 137 complete. Loss:  0.031968291848897934\n",
      "batch 138 complete. Loss:  0.03861802816390991\n",
      "batch 139 complete. Loss:  0.03373999893665314\n",
      "batch 140 complete. Loss:  0.03740371763706207\n",
      "batch 141 complete. Loss:  0.014337415806949139\n",
      "batch 142 complete. Loss:  0.026127152144908905\n",
      "batch 143 complete. Loss:  0.03970875218510628\n",
      "batch 144 complete. Loss:  0.03359650447964668\n",
      "batch 145 complete. Loss:  0.018556680530309677\n",
      "batch 146 complete. Loss:  0.011684868484735489\n",
      "batch 147 complete. Loss:  0.03788009285926819\n",
      "batch 148 complete. Loss:  0.02625923417508602\n",
      "batch 149 complete. Loss:  0.03358571231365204\n",
      "batch 150 complete. Loss:  0.026043422520160675\n",
      "batch 151 complete. Loss:  0.026162968948483467\n",
      "batch 152 complete. Loss:  0.03362162783741951\n",
      "batch 153 complete. Loss:  0.02790733054280281\n",
      "batch 154 complete. Loss:  0.13089069724082947\n",
      "batch 155 complete. Loss:  0.018620504066348076\n",
      "batch 156 complete. Loss:  0.07074815034866333\n",
      "batch 157 complete. Loss:  0.016941454261541367\n",
      "batch 158 complete. Loss:  0.1025206446647644\n",
      "batch 159 complete. Loss:  0.03209284320473671\n",
      "batch 160 complete. Loss:  0.03454361483454704\n",
      "batch 161 complete. Loss:  0.016894102096557617\n",
      "batch 162 complete. Loss:  0.0207362100481987\n",
      "batch 163 complete. Loss:  0.02117714285850525\n",
      "batch 164 complete. Loss:  0.04902258515357971\n",
      "batch 165 complete. Loss:  0.0287775881588459\n",
      "batch 166 complete. Loss:  0.011783103458583355\n",
      "batch 167 complete. Loss:  0.05752546712756157\n",
      "batch 168 complete. Loss:  0.027326785027980804\n",
      "batch 169 complete. Loss:  0.035233501344919205\n",
      "batch 170 complete. Loss:  0.04615861922502518\n",
      "batch 171 complete. Loss:  0.03279624879360199\n",
      "batch 172 complete. Loss:  0.018808435648679733\n",
      "batch 173 complete. Loss:  0.04612504318356514\n",
      "batch 174 complete. Loss:  0.03156397491693497\n",
      "batch 175 complete. Loss:  0.019225170835852623\n",
      "batch 176 complete. Loss:  0.013203920796513557\n",
      "batch 177 complete. Loss:  0.09583102911710739\n",
      "batch 178 complete. Loss:  0.0312395878136158\n",
      "batch 179 complete. Loss:  0.021313998848199844\n",
      "batch 180 complete. Loss:  0.014078320935368538\n",
      "batch 181 complete. Loss:  0.015158065594732761\n",
      "batch 182 complete. Loss:  0.03887869045138359\n",
      "batch 183 complete. Loss:  0.03393751382827759\n",
      "batch 184 complete. Loss:  0.02548360824584961\n",
      "batch 185 complete. Loss:  0.03303650766611099\n",
      "batch 186 complete. Loss:  0.013489414006471634\n",
      "batch 187 complete. Loss:  0.09162454307079315\n",
      "batch 188 complete. Loss:  0.053326308727264404\n",
      "batch 189 complete. Loss:  0.025466546416282654\n",
      "batch 190 complete. Loss:  0.03135567158460617\n",
      "batch 191 complete. Loss:  0.01954914629459381\n",
      "batch 192 complete. Loss:  0.03352740406990051\n",
      "batch 193 complete. Loss:  0.08649348467588425\n",
      "batch 194 complete. Loss:  0.01319408044219017\n",
      "batch 195 complete. Loss:  0.045912545174360275\n",
      "batch 196 complete. Loss:  0.01871037855744362\n",
      "batch 197 complete. Loss:  0.017257286235690117\n",
      "batch 198 complete. Loss:  0.08178116381168365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 199 complete. Loss:  0.016657419502735138\n",
      "batch 200 complete. Loss:  0.029285047203302383\n",
      "batch 201 complete. Loss:  0.017941368743777275\n",
      "batch 202 complete. Loss:  0.038301412016153336\n",
      "batch 203 complete. Loss:  0.045484039932489395\n",
      "batch 204 complete. Loss:  0.013604119420051575\n",
      "batch 205 complete. Loss:  0.022695422172546387\n",
      "batch 206 complete. Loss:  0.03130432963371277\n",
      "batch 207 complete. Loss:  0.08228114992380142\n",
      "batch 208 complete. Loss:  0.023671599105000496\n",
      "batch 209 complete. Loss:  0.021622467786073685\n",
      "batch 210 complete. Loss:  0.08095686882734299\n",
      "batch 211 complete. Loss:  0.027797576040029526\n",
      "batch 212 complete. Loss:  0.027599887922406197\n",
      "batch 213 complete. Loss:  0.019694093614816666\n",
      "batch 214 complete. Loss:  0.029862400144338608\n",
      "batch 215 complete. Loss:  0.019854016602039337\n",
      "batch 216 complete. Loss:  0.08048440515995026\n",
      "batch 217 complete. Loss:  0.029997827485203743\n",
      "batch 218 complete. Loss:  0.01789173111319542\n",
      "batch 219 complete. Loss:  0.028904132544994354\n",
      "batch 220 complete. Loss:  0.019546443596482277\n",
      "batch 221 complete. Loss:  0.0349094495177269\n",
      "batch 222 complete. Loss:  0.017790062353014946\n",
      "batch 223 complete. Loss:  0.07985465973615646\n",
      "batch 224 complete. Loss:  0.01767783984541893\n",
      "batch 225 complete. Loss:  0.03090718388557434\n",
      "batch 226 complete. Loss:  0.023867513984441757\n",
      "batch 227 complete. Loss:  0.015029331669211388\n",
      "batch 228 complete. Loss:  0.03309521824121475\n",
      "batch 229 complete. Loss:  0.02970615215599537\n",
      "batch 230 complete. Loss:  0.04341226816177368\n",
      "batch 231 complete. Loss:  0.059961434453725815\n",
      "batch 232 complete. Loss:  0.031737904995679855\n",
      "batch 233 complete. Loss:  0.07061640918254852\n",
      "batch 234 complete. Loss:  0.04143928363919258\n",
      "batch 235 complete. Loss:  0.05571262538433075\n",
      "batch 236 complete. Loss:  0.017960522323846817\n",
      "batch 237 complete. Loss:  0.011621920391917229\n",
      "batch 238 complete. Loss:  0.027649957686662674\n",
      "batch 239 complete. Loss:  0.02106735296547413\n",
      "batch 240 complete. Loss:  0.024536997079849243\n",
      "batch 241 complete. Loss:  0.023378167301416397\n",
      "batch 242 complete. Loss:  0.049280133098363876\n",
      "batch 243 complete. Loss:  0.04604903608560562\n",
      "batch 244 complete. Loss:  0.024031545966863632\n",
      "batch 245 complete. Loss:  0.01991291530430317\n",
      "batch 246 complete. Loss:  0.02024109661579132\n",
      "batch 247 complete. Loss:  0.01757878065109253\n",
      "batch 248 complete. Loss:  0.04393874853849411\n",
      "batch 249 complete. Loss:  0.03280923515558243\n",
      "batch 250 complete. Loss:  0.010869265533983707\n",
      "batch 251 complete. Loss:  0.03584888577461243\n",
      "batch 252 complete. Loss:  0.02763165719807148\n",
      "batch 253 complete. Loss:  0.037654776126146317\n",
      "batch 254 complete. Loss:  0.02668660879135132\n",
      "batch 255 complete. Loss:  0.01984148658812046\n",
      "batch 256 complete. Loss:  0.013440568000078201\n",
      "batch 257 complete. Loss:  0.045642685145139694\n",
      "batch 258 complete. Loss:  0.01995835453271866\n",
      "batch 259 complete. Loss:  0.019518814980983734\n",
      "batch 260 complete. Loss:  0.026623914018273354\n",
      "batch 261 complete. Loss:  0.016291186213493347\n",
      "batch 262 complete. Loss:  0.027458781376481056\n",
      "batch 263 complete. Loss:  0.033594727516174316\n",
      "batch 264 complete. Loss:  0.015463104471564293\n",
      "batch 265 complete. Loss:  0.021063558757305145\n",
      "batch 266 complete. Loss:  0.016013460233807564\n",
      "batch 267 complete. Loss:  0.011790363118052483\n",
      "batch 268 complete. Loss:  0.0194308552891016\n",
      "batch 269 complete. Loss:  0.013420621864497662\n",
      "batch 270 complete. Loss:  0.020115245133638382\n",
      "batch 271 complete. Loss:  0.029185950756072998\n",
      "batch 272 complete. Loss:  0.01635746844112873\n",
      "batch 273 complete. Loss:  0.010870295576751232\n",
      "batch 274 complete. Loss:  0.01846703141927719\n",
      "batch 275 complete. Loss:  0.014318845234811306\n",
      "batch 276 complete. Loss:  0.019797123968601227\n",
      "batch 277 complete. Loss:  0.021348752081394196\n",
      "batch 278 complete. Loss:  0.020535428076982498\n",
      "batch 279 complete. Loss:  0.04806893691420555\n",
      "batch 280 complete. Loss:  0.034239716827869415\n",
      "batch 281 complete. Loss:  0.025346949696540833\n",
      "batch 282 complete. Loss:  0.030860038474202156\n",
      "batch 283 complete. Loss:  0.028924614191055298\n",
      "batch 284 complete. Loss:  0.031015129759907722\n",
      "batch 285 complete. Loss:  0.01305270753800869\n",
      "batch 286 complete. Loss:  0.07708568871021271\n",
      "batch 287 complete. Loss:  0.026757825165987015\n",
      "batch 288 complete. Loss:  0.1012057214975357\n",
      "batch 289 complete. Loss:  0.02813439816236496\n",
      "batch 290 complete. Loss:  0.017545774579048157\n",
      "batch 291 complete. Loss:  0.03452283889055252\n",
      "batch 292 complete. Loss:  0.04480094835162163\n",
      "batch 293 complete. Loss:  0.017062855884432793\n",
      "batch 294 complete. Loss:  0.020382840186357498\n",
      "batch 295 complete. Loss:  0.020321860909461975\n",
      "batch 296 complete. Loss:  0.049093060195446014\n",
      "batch 297 complete. Loss:  0.03011157177388668\n",
      "batch 298 complete. Loss:  0.04960694909095764\n",
      "batch 299 complete. Loss:  0.04096487909555435\n",
      "batch 300 complete. Loss:  0.015362982638180256\n",
      "batch 301 complete. Loss:  0.015460127033293247\n",
      "batch 302 complete. Loss:  0.029739918187260628\n",
      "batch 303 complete. Loss:  0.01286176685243845\n",
      "batch 304 complete. Loss:  0.025894030928611755\n",
      "batch 305 complete. Loss:  0.015511022880673409\n",
      "batch 306 complete. Loss:  0.1131913810968399\n",
      "batch 307 complete. Loss:  0.019939754158258438\n",
      "batch 308 complete. Loss:  0.020119253545999527\n",
      "batch 309 complete. Loss:  0.021208778023719788\n",
      "batch 310 complete. Loss:  0.029310643672943115\n",
      "batch 311 complete. Loss:  0.10715150833129883\n",
      "batch 312 complete. Loss:  0.012964333407580853\n",
      "batch 313 complete. Loss:  0.015813302248716354\n",
      "batch 314 complete. Loss:  0.021482830867171288\n",
      "batch 315 complete. Loss:  0.02827807515859604\n",
      "batch 316 complete. Loss:  0.12032966315746307\n",
      "batch 317 complete. Loss:  0.018233176320791245\n",
      "batch 318 complete. Loss:  0.013761775568127632\n",
      "batch 319 complete. Loss:  0.08223830163478851\n",
      "batch 320 complete. Loss:  0.01896454021334648\n",
      "batch 321 complete. Loss:  0.0394676998257637\n",
      "batch 322 complete. Loss:  0.013004153966903687\n",
      "batch 323 complete. Loss:  0.06486714631319046\n",
      "batch 324 complete. Loss:  0.0276060551404953\n",
      "batch 325 complete. Loss:  0.027373094111680984\n",
      "batch 326 complete. Loss:  0.015154694207012653\n",
      "batch 327 complete. Loss:  0.069444939494133\n",
      "batch 328 complete. Loss:  0.029379038140177727\n",
      "batch 329 complete. Loss:  0.0516313835978508\n",
      "batch 330 complete. Loss:  0.03243889659643173\n",
      "batch 331 complete. Loss:  0.05924628674983978\n",
      "batch 332 complete. Loss:  0.02266448736190796\n",
      "batch 333 complete. Loss:  0.024421190842986107\n",
      "batch 334 complete. Loss:  0.02871369570493698\n",
      "batch 335 complete. Loss:  0.018789108842611313\n",
      "batch 336 complete. Loss:  0.01488595549017191\n",
      "batch 337 complete. Loss:  0.0573473684489727\n",
      "batch 338 complete. Loss:  0.028183501213788986\n",
      "batch 339 complete. Loss:  0.011529898270964622\n",
      "batch 340 complete. Loss:  0.017687475308775902\n",
      "batch 341 complete. Loss:  0.02165987715125084\n",
      "batch 342 complete. Loss:  0.07624725997447968\n",
      "batch 343 complete. Loss:  0.060782529413700104\n",
      "batch 344 complete. Loss:  0.05244600400328636\n",
      "batch 345 complete. Loss:  0.028723839670419693\n",
      "batch 346 complete. Loss:  0.02476375177502632\n",
      "batch 347 complete. Loss:  0.011815536767244339\n",
      "batch 348 complete. Loss:  0.0499802865087986\n",
      "batch 349 complete. Loss:  0.026905566453933716\n",
      "batch 350 complete. Loss:  0.016371287405490875\n",
      "batch 351 complete. Loss:  0.013764739967882633\n",
      "batch 352 complete. Loss:  0.027214113622903824\n",
      "batch 353 complete. Loss:  0.02402033656835556\n",
      "batch 354 complete. Loss:  0.01359619665890932\n",
      "batch 355 complete. Loss:  0.04725233465433121\n",
      "batch 356 complete. Loss:  0.012550005689263344\n",
      "batch 357 complete. Loss:  0.031235575675964355\n",
      "batch 358 complete. Loss:  0.034232236444950104\n",
      "batch 359 complete. Loss:  0.06251475214958191\n",
      "batch 360 complete. Loss:  0.018654808402061462\n",
      "batch 361 complete. Loss:  0.021087758243083954\n",
      "batch 362 complete. Loss:  0.022952158004045486\n",
      "batch 363 complete. Loss:  0.05235113203525543\n",
      "batch 364 complete. Loss:  0.02686421573162079\n",
      "batch 365 complete. Loss:  0.03329441323876381\n",
      "batch 366 complete. Loss:  0.030213650315999985\n",
      "batch 367 complete. Loss:  0.018488779664039612\n",
      "batch 368 complete. Loss:  0.03335970640182495\n",
      "batch 369 complete. Loss:  0.0572885200381279\n",
      "batch 370 complete. Loss:  0.017240479588508606\n",
      "batch 371 complete. Loss:  0.020073678344488144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 372 complete. Loss:  0.02584320493042469\n",
      "batch 373 complete. Loss:  0.02083432860672474\n",
      "batch 374 complete. Loss:  0.023821767419576645\n",
      "batch 375 complete. Loss:  0.023724399507045746\n",
      "batch 376 complete. Loss:  0.02810993418097496\n",
      "batch 377 complete. Loss:  0.04065664857625961\n",
      "batch 378 complete. Loss:  0.022674286738038063\n",
      "batch 379 complete. Loss:  0.024081170558929443\n",
      "batch 380 complete. Loss:  0.007340667303651571\n",
      "batch 381 complete. Loss:  0.027181390672922134\n",
      "batch 382 complete. Loss:  0.05740327760577202\n",
      "batch 383 complete. Loss:  0.01657922938466072\n",
      "batch 384 complete. Loss:  0.02038196474313736\n",
      "batch 385 complete. Loss:  0.03274551033973694\n",
      "batch 386 complete. Loss:  0.020519129931926727\n",
      "batch 387 complete. Loss:  0.021915381774306297\n",
      "batch 388 complete. Loss:  0.02440500818192959\n",
      "batch 389 complete. Loss:  0.020528718829154968\n",
      "batch 390 complete. Loss:  0.02444237470626831\n",
      "batch 391 complete. Loss:  0.015801211819052696\n",
      "batch 392 complete. Loss:  0.02250792644917965\n",
      "batch 393 complete. Loss:  0.040502071380615234\n",
      "batch 394 complete. Loss:  0.028480779379606247\n",
      "batch 395 complete. Loss:  0.029486335813999176\n",
      "batch 396 complete. Loss:  0.03925454616546631\n",
      "batch 397 complete. Loss:  0.04539700597524643\n",
      "batch 398 complete. Loss:  0.02459082193672657\n",
      "batch 399 complete. Loss:  0.03705834969878197\n",
      "batch 400 complete. Loss:  0.033051006495952606\n",
      "batch 401 complete. Loss:  0.019470974802970886\n",
      "batch 402 complete. Loss:  0.030635003000497818\n",
      "batch 403 complete. Loss:  0.026171240955591202\n",
      "batch 404 complete. Loss:  0.026142727583646774\n",
      "batch 405 complete. Loss:  0.04316610470414162\n",
      "batch 406 complete. Loss:  0.0856710821390152\n",
      "batch 407 complete. Loss:  0.029885869473218918\n",
      "batch 408 complete. Loss:  0.0948593020439148\n",
      "batch 409 complete. Loss:  0.01895689032971859\n",
      "batch 410 complete. Loss:  0.11901378631591797\n",
      "batch 411 complete. Loss:  0.025778677314519882\n",
      "batch 412 complete. Loss:  0.0269139613956213\n",
      "batch 413 complete. Loss:  0.018501529470086098\n",
      "batch 414 complete. Loss:  0.016310229897499084\n",
      "batch 415 complete. Loss:  0.029519962146878242\n",
      "batch 416 complete. Loss:  0.022626271471381187\n",
      "batch 417 complete. Loss:  0.023201050236821175\n",
      "batch 418 complete. Loss:  0.01427456084638834\n",
      "batch 419 complete. Loss:  0.022629596292972565\n",
      "batch 420 complete. Loss:  0.03947067633271217\n",
      "batch 421 complete. Loss:  0.03365260362625122\n",
      "batch 422 complete. Loss:  0.02429816871881485\n",
      "batch 423 complete. Loss:  0.021578889340162277\n",
      "batch 424 complete. Loss:  0.01561778225004673\n",
      "batch 425 complete. Loss:  0.01605074852705002\n",
      "batch 426 complete. Loss:  0.010840046219527721\n",
      "batch 427 complete. Loss:  0.01921922340989113\n",
      "batch 428 complete. Loss:  0.037222087383270264\n",
      "batch 429 complete. Loss:  0.037809621542692184\n",
      "batch 430 complete. Loss:  0.026693081483244896\n",
      "batch 431 complete. Loss:  0.03833460062742233\n",
      "batch 432 complete. Loss:  0.15584245324134827\n",
      "batch 433 complete. Loss:  0.022099360823631287\n",
      "batch 434 complete. Loss:  0.027202513068914413\n",
      "batch 435 complete. Loss:  0.04275236278772354\n",
      "batch 436 complete. Loss:  0.09162706881761551\n",
      "batch 437 complete. Loss:  0.05264279246330261\n",
      "batch 438 complete. Loss:  0.012113792821764946\n",
      "batch 439 complete. Loss:  0.027367670089006424\n",
      "batch 440 complete. Loss:  0.07822009921073914\n",
      "batch 441 complete. Loss:  0.03338891267776489\n",
      "batch 442 complete. Loss:  0.028937673196196556\n",
      "batch 443 complete. Loss:  0.021284187212586403\n",
      "batch 444 complete. Loss:  0.034389276057481766\n",
      "batch 445 complete. Loss:  0.030866965651512146\n",
      "batch 446 complete. Loss:  0.02707129716873169\n",
      "batch 447 complete. Loss:  0.022866670042276382\n",
      "batch 448 complete. Loss:  0.02504647895693779\n",
      "batch 449 complete. Loss:  0.04619824141263962\n",
      "batch 450 complete. Loss:  0.025860918685793877\n",
      "batch 451 complete. Loss:  0.021504374220967293\n",
      "batch 452 complete. Loss:  0.032932184636592865\n",
      "batch 453 complete. Loss:  0.018499605357646942\n",
      "batch 454 complete. Loss:  0.03679272159934044\n",
      "batch 455 complete. Loss:  0.019872764125466347\n",
      "batch 456 complete. Loss:  0.01908078044652939\n",
      "batch 457 complete. Loss:  0.01874646171927452\n",
      "batch 458 complete. Loss:  0.09665890038013458\n",
      "batch 459 complete. Loss:  0.01427189726382494\n",
      "batch 460 complete. Loss:  0.023876536637544632\n",
      "batch 461 complete. Loss:  0.026314551010727882\n",
      "batch 462 complete. Loss:  0.015380817465484142\n",
      "batch 463 complete. Loss:  0.02260369434952736\n",
      "batch 464 complete. Loss:  0.028773779049515724\n",
      "batch 465 complete. Loss:  0.0292576365172863\n",
      "batch 466 complete. Loss:  0.011248133145272732\n",
      "batch 467 complete. Loss:  0.02556055411696434\n",
      "batch 468 complete. Loss:  0.06526768207550049\n",
      "batch 469 complete. Loss:  0.032386183738708496\n",
      "batch 470 complete. Loss:  0.12609907984733582\n",
      "batch 471 complete. Loss:  0.1922234445810318\n",
      "batch 472 complete. Loss:  0.06539787352085114\n",
      "batch 473 complete. Loss:  0.02953995205461979\n",
      "batch 474 complete. Loss:  0.03437025845050812\n",
      "batch 475 complete. Loss:  0.026206091046333313\n",
      "batch 476 complete. Loss:  0.01702066883444786\n",
      "batch 477 complete. Loss:  0.019381657242774963\n",
      "batch 478 complete. Loss:  0.019385475665330887\n",
      "batch 479 complete. Loss:  0.01780930534005165\n",
      "batch 480 complete. Loss:  0.03438044339418411\n",
      "batch 481 complete. Loss:  0.02207469381392002\n",
      "batch 482 complete. Loss:  0.043074868619441986\n",
      "batch 483 complete. Loss:  0.019157862290740013\n",
      "batch 484 complete. Loss:  0.02894192561507225\n",
      "batch 485 complete. Loss:  0.08017825335264206\n",
      "batch 486 complete. Loss:  0.05987526476383209\n",
      "batch 487 complete. Loss:  0.054390355944633484\n",
      "batch 488 complete. Loss:  0.012968400493264198\n",
      "batch 489 complete. Loss:  0.09526555985212326\n",
      "batch 490 complete. Loss:  0.07432010769844055\n",
      "batch 491 complete. Loss:  0.02017986588180065\n",
      "batch 492 complete. Loss:  0.02166013792157173\n",
      "batch 493 complete. Loss:  0.01707925647497177\n",
      "batch 494 complete. Loss:  0.08091972768306732\n",
      "batch 495 complete. Loss:  0.025770287960767746\n",
      "batch 496 complete. Loss:  0.021939637139439583\n",
      "batch 497 complete. Loss:  0.01318613812327385\n",
      "batch 498 complete. Loss:  0.019600536674261093\n",
      "batch 499 complete. Loss:  0.02932552620768547\n",
      "batch 500 complete. Loss:  0.028860056772828102\n",
      "batch 501 complete. Loss:  0.012820862233638763\n",
      "batch 502 complete. Loss:  0.054774779826402664\n",
      "batch 503 complete. Loss:  0.01764175295829773\n",
      "batch 504 complete. Loss:  0.0283105056732893\n",
      "batch 505 complete. Loss:  0.034414637833833694\n",
      "batch 506 complete. Loss:  0.029387228190898895\n",
      "batch 507 complete. Loss:  0.02722989208996296\n",
      "batch 508 complete. Loss:  0.037405602633953094\n",
      "batch 509 complete. Loss:  0.017511587589979172\n",
      "batch 510 complete. Loss:  0.03457903116941452\n",
      "batch 511 complete. Loss:  0.05358722060918808\n",
      "batch 512 complete. Loss:  0.026467155665159225\n",
      "batch 513 complete. Loss:  0.0198297630995512\n",
      "batch 514 complete. Loss:  0.01257086917757988\n",
      "batch 515 complete. Loss:  0.02826102450489998\n",
      "batch 516 complete. Loss:  0.047287121415138245\n",
      "batch 517 complete. Loss:  0.03303280472755432\n",
      "batch 518 complete. Loss:  0.03617750108242035\n",
      "batch 519 complete. Loss:  0.03881104290485382\n",
      "batch 520 complete. Loss:  0.013234740123152733\n",
      "batch 521 complete. Loss:  0.02238946408033371\n",
      "batch 522 complete. Loss:  0.050972308963537216\n",
      "batch 523 complete. Loss:  0.028256092220544815\n",
      "batch 524 complete. Loss:  0.033732686191797256\n",
      "batch 525 complete. Loss:  0.02559288591146469\n",
      "batch 526 complete. Loss:  0.028344564139842987\n",
      "batch 527 complete. Loss:  0.034996796399354935\n",
      "batch 528 complete. Loss:  0.08138540387153625\n",
      "batch 529 complete. Loss:  0.03858943283557892\n",
      "batch 530 complete. Loss:  0.02162972092628479\n",
      "batch 531 complete. Loss:  0.02960943803191185\n",
      "batch 532 complete. Loss:  0.04162486270070076\n",
      "batch 533 complete. Loss:  0.026977945119142532\n",
      "batch 534 complete. Loss:  0.03746532276272774\n",
      "batch 535 complete. Loss:  0.05386650562286377\n",
      "batch 536 complete. Loss:  0.08173856884241104\n",
      "batch 537 complete. Loss:  0.05298953503370285\n",
      "batch 538 complete. Loss:  0.04227496683597565\n",
      "batch 539 complete. Loss:  0.025693736970424652\n",
      "batch 540 complete. Loss:  0.034217581152915955\n",
      "batch 541 complete. Loss:  0.04400216042995453\n",
      "batch 542 complete. Loss:  0.09350630640983582\n",
      "batch 543 complete. Loss:  0.03852662444114685\n",
      "batch 544 complete. Loss:  0.0219574011862278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 545 complete. Loss:  0.018047206103801727\n",
      "batch 546 complete. Loss:  0.04748299717903137\n",
      "batch 547 complete. Loss:  0.040777724236249924\n",
      "batch 548 complete. Loss:  0.058096449822187424\n",
      "batch 549 complete. Loss:  0.039636701345443726\n",
      "batch 550 complete. Loss:  0.03691893815994263\n",
      "batch 551 complete. Loss:  0.01934979483485222\n",
      "batch 552 complete. Loss:  0.035311825573444366\n",
      "batch 553 complete. Loss:  0.040126219391822815\n",
      "batch 554 complete. Loss:  0.02950146421790123\n",
      "batch 555 complete. Loss:  0.054673414677381516\n",
      "batch 556 complete. Loss:  0.030168289318680763\n",
      "batch 557 complete. Loss:  0.028236668556928635\n",
      "batch 558 complete. Loss:  0.12663529813289642\n",
      "batch 559 complete. Loss:  0.01635277085006237\n",
      "batch 560 complete. Loss:  0.011587198823690414\n",
      "batch 561 complete. Loss:  0.030130907893180847\n",
      "batch 562 complete. Loss:  0.025695545598864555\n",
      "batch 563 complete. Loss:  0.011599184945225716\n",
      "batch 564 complete. Loss:  0.10624122619628906\n",
      "batch 565 complete. Loss:  0.06160531938076019\n",
      "batch 566 complete. Loss:  0.0306687094271183\n",
      "batch 567 complete. Loss:  0.025978606194257736\n",
      "batch 568 complete. Loss:  0.03884381800889969\n",
      "batch 569 complete. Loss:  0.04804931581020355\n",
      "batch 570 complete. Loss:  0.01913771778345108\n",
      "batch 571 complete. Loss:  0.028878934681415558\n",
      "batch 572 complete. Loss:  0.024282393977046013\n",
      "batch 573 complete. Loss:  0.012592467479407787\n",
      "batch 574 complete. Loss:  0.019939903169870377\n",
      "batch 575 complete. Loss:  0.00734809460118413\n",
      "batch 576 complete. Loss:  0.024259062483906746\n",
      "batch 577 complete. Loss:  0.017258748412132263\n",
      "batch 578 complete. Loss:  0.010077862069010735\n",
      "batch 579 complete. Loss:  0.022613203153014183\n",
      "batch 580 complete. Loss:  0.01890687271952629\n",
      "batch 581 complete. Loss:  0.011236889287829399\n",
      "batch 582 complete. Loss:  0.030167412012815475\n",
      "batch 583 complete. Loss:  0.013362573459744453\n",
      "batch 584 complete. Loss:  0.01941634714603424\n",
      "batch 585 complete. Loss:  0.017273344099521637\n",
      "batch 586 complete. Loss:  0.029556378722190857\n",
      "batch 587 complete. Loss:  0.017603611573576927\n",
      "batch 588 complete. Loss:  0.019953764975070953\n",
      "batch 589 complete. Loss:  0.017392031848430634\n",
      "batch 590 complete. Loss:  0.021987704560160637\n",
      "batch 591 complete. Loss:  0.03546743094921112\n",
      "batch 592 complete. Loss:  0.0173563864082098\n",
      "batch 593 complete. Loss:  0.0146290622651577\n",
      "batch 594 complete. Loss:  0.014751600101590157\n",
      "batch 595 complete. Loss:  0.015470442362129688\n",
      "batch 596 complete. Loss:  0.08212034404277802\n",
      "batch 597 complete. Loss:  0.013559680432081223\n",
      "batch 598 complete. Loss:  0.019748467952013016\n",
      "batch 599 complete. Loss:  0.0456991009414196\n",
      "batch 600 complete. Loss:  0.08098319172859192\n",
      "batch 601 complete. Loss:  0.01233760267496109\n",
      "batch 602 complete. Loss:  0.022092927247285843\n",
      "batch 603 complete. Loss:  0.026858240365982056\n",
      "batch 604 complete. Loss:  0.02813892439007759\n",
      "batch 605 complete. Loss:  0.02098521962761879\n",
      "batch 606 complete. Loss:  0.02583763375878334\n",
      "batch 607 complete. Loss:  0.007919013500213623\n",
      "batch 608 complete. Loss:  0.014314103871583939\n",
      "batch 609 complete. Loss:  0.036281269043684006\n",
      "batch 610 complete. Loss:  0.0968695729970932\n",
      "batch 611 complete. Loss:  0.018932659178972244\n",
      "batch 612 complete. Loss:  0.02958216890692711\n",
      "batch 613 complete. Loss:  0.058865875005722046\n",
      "batch 614 complete. Loss:  0.019367313012480736\n",
      "batch 615 complete. Loss:  0.014086623676121235\n",
      "batch 616 complete. Loss:  0.05265922471880913\n",
      "batch 617 complete. Loss:  0.0523189976811409\n",
      "batch 618 complete. Loss:  0.02605207823216915\n",
      "batch 619 complete. Loss:  0.010183686390519142\n",
      "batch 620 complete. Loss:  0.02684304490685463\n",
      "batch 621 complete. Loss:  0.012904154136776924\n",
      "batch 622 complete. Loss:  0.049956098198890686\n",
      "batch 623 complete. Loss:  0.02971450611948967\n",
      "batch 624 complete. Loss:  0.018022891134023666\n",
      "batch 625 complete. Loss:  0.024555357173085213\n",
      "batch 626 complete. Loss:  0.019074171781539917\n",
      "batch 627 complete. Loss:  0.05662540718913078\n",
      "batch 628 complete. Loss:  0.03960076719522476\n",
      "batch 629 complete. Loss:  0.04080495983362198\n",
      "batch 630 complete. Loss:  0.024702513590455055\n",
      "batch 631 complete. Loss:  0.02882099337875843\n",
      "batch 632 complete. Loss:  0.017575081437826157\n",
      "batch 633 complete. Loss:  0.041418999433517456\n",
      "batch 634 complete. Loss:  0.053749263286590576\n",
      "batch 635 complete. Loss:  0.03874910622835159\n",
      "batch 636 complete. Loss:  0.02156381495296955\n",
      "batch 637 complete. Loss:  0.013224582187831402\n",
      "batch 638 complete. Loss:  0.04013388231396675\n",
      "batch 639 complete. Loss:  0.01861860603094101\n",
      "batch 640 complete. Loss:  0.031302083283662796\n",
      "batch 641 complete. Loss:  0.0762782096862793\n",
      "batch 642 complete. Loss:  0.025101041421294212\n",
      "batch 643 complete. Loss:  0.01462611649185419\n",
      "batch 644 complete. Loss:  0.034531235694885254\n",
      "batch 645 complete. Loss:  0.02371317893266678\n",
      "batch 646 complete. Loss:  0.08749635517597198\n",
      "batch 647 complete. Loss:  0.049758974462747574\n",
      "batch 648 complete. Loss:  0.01755427196621895\n",
      "batch 649 complete. Loss:  0.01765407621860504\n",
      "batch 650 complete. Loss:  0.019590407609939575\n",
      "batch 651 complete. Loss:  0.016170475631952286\n",
      "batch 652 complete. Loss:  0.01341077871620655\n",
      "batch 653 complete. Loss:  0.007599513977766037\n",
      "batch 654 complete. Loss:  0.02329324185848236\n",
      "batch 655 complete. Loss:  0.03804299980401993\n",
      "batch 656 complete. Loss:  0.021633919328451157\n",
      "batch 657 complete. Loss:  0.0317181758582592\n",
      "batch 658 complete. Loss:  0.03148697316646576\n",
      "batch 659 complete. Loss:  0.0850086659193039\n",
      "batch 660 complete. Loss:  0.030099883675575256\n",
      "batch 661 complete. Loss:  0.021678335964679718\n",
      "batch 662 complete. Loss:  0.05859210714697838\n",
      "batch 663 complete. Loss:  0.0625745952129364\n",
      "batch 664 complete. Loss:  0.017242692410945892\n",
      "batch 665 complete. Loss:  0.035515110939741135\n",
      "batch 666 complete. Loss:  0.01943986862897873\n",
      "batch 667 complete. Loss:  0.03831680491566658\n",
      "batch 668 complete. Loss:  0.07807829231023788\n",
      "batch 669 complete. Loss:  0.03686007857322693\n",
      "batch 670 complete. Loss:  0.025105109438300133\n",
      "batch 671 complete. Loss:  0.01611856184899807\n",
      "batch 672 complete. Loss:  0.01733016036450863\n",
      "batch 673 complete. Loss:  0.027079958468675613\n",
      "batch 674 complete. Loss:  0.013538142666220665\n",
      "Epoch 1 Validation Loss: 0.038426278271993415\n",
      "2 / 10\n",
      "batch 0 complete. Loss:  0.08735949546098709\n",
      "batch 1 complete. Loss:  0.011331993155181408\n",
      "batch 2 complete. Loss:  0.011370284482836723\n",
      "batch 3 complete. Loss:  0.017964502796530724\n",
      "batch 4 complete. Loss:  0.027014264836907387\n",
      "batch 5 complete. Loss:  0.04396054893732071\n",
      "batch 6 complete. Loss:  0.0182180292904377\n",
      "batch 7 complete. Loss:  0.01959691010415554\n",
      "batch 8 complete. Loss:  0.012195582501590252\n",
      "batch 9 complete. Loss:  0.014081789180636406\n",
      "batch 10 complete. Loss:  0.008464100770652294\n",
      "batch 11 complete. Loss:  0.015863005071878433\n",
      "batch 12 complete. Loss:  0.010573815554380417\n",
      "batch 13 complete. Loss:  0.01889702118933201\n",
      "batch 14 complete. Loss:  0.013289323076605797\n",
      "batch 15 complete. Loss:  0.0775739923119545\n",
      "batch 16 complete. Loss:  0.01896791160106659\n",
      "batch 17 complete. Loss:  0.03258886560797691\n",
      "batch 18 complete. Loss:  0.04812468960881233\n",
      "batch 19 complete. Loss:  0.016298767179250717\n",
      "batch 20 complete. Loss:  0.054212167859077454\n",
      "batch 21 complete. Loss:  0.023007109761238098\n",
      "batch 22 complete. Loss:  0.058297961950302124\n",
      "batch 23 complete. Loss:  0.013351722620427608\n",
      "batch 24 complete. Loss:  0.013213612139225006\n",
      "batch 25 complete. Loss:  0.02855425328016281\n",
      "batch 26 complete. Loss:  0.06722603738307953\n",
      "batch 27 complete. Loss:  0.013836052268743515\n",
      "batch 28 complete. Loss:  0.014466565102338791\n",
      "batch 29 complete. Loss:  0.026111673563718796\n",
      "batch 30 complete. Loss:  0.019602229818701744\n",
      "batch 31 complete. Loss:  0.022289544343948364\n",
      "batch 32 complete. Loss:  0.018238317221403122\n",
      "batch 33 complete. Loss:  0.011858342215418816\n",
      "batch 34 complete. Loss:  0.01782105676829815\n",
      "batch 35 complete. Loss:  0.017623357474803925\n",
      "batch 36 complete. Loss:  0.022204000502824783\n",
      "batch 37 complete. Loss:  0.021999750286340714\n",
      "batch 38 complete. Loss:  0.012381915003061295\n",
      "batch 39 complete. Loss:  0.010479800403118134\n",
      "batch 40 complete. Loss:  0.01558041200041771\n",
      "batch 41 complete. Loss:  0.02560417167842388\n",
      "batch 42 complete. Loss:  0.011869923211634159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 43 complete. Loss:  0.016942260786890984\n",
      "batch 44 complete. Loss:  0.014167106710374355\n",
      "batch 45 complete. Loss:  0.016823282465338707\n",
      "batch 46 complete. Loss:  0.01770017296075821\n",
      "batch 47 complete. Loss:  0.007614968344569206\n",
      "batch 48 complete. Loss:  0.02844129130244255\n",
      "batch 49 complete. Loss:  0.018023964017629623\n",
      "batch 50 complete. Loss:  0.021156497299671173\n",
      "batch 51 complete. Loss:  0.024658728390932083\n",
      "batch 52 complete. Loss:  0.01969069615006447\n",
      "batch 53 complete. Loss:  0.023137589916586876\n",
      "batch 54 complete. Loss:  0.022355780005455017\n",
      "batch 55 complete. Loss:  0.010132292285561562\n",
      "batch 56 complete. Loss:  0.015206814743578434\n",
      "batch 57 complete. Loss:  0.021946847438812256\n",
      "batch 58 complete. Loss:  0.026407916098833084\n",
      "batch 59 complete. Loss:  0.021416254341602325\n",
      "batch 60 complete. Loss:  0.015134597197175026\n",
      "batch 61 complete. Loss:  0.02594229206442833\n",
      "batch 62 complete. Loss:  0.014802150428295135\n",
      "batch 63 complete. Loss:  0.016912564635276794\n",
      "batch 64 complete. Loss:  0.022279031574726105\n",
      "batch 65 complete. Loss:  0.03309337794780731\n",
      "batch 66 complete. Loss:  0.015971090644598007\n",
      "batch 67 complete. Loss:  0.015230265446007252\n",
      "batch 68 complete. Loss:  0.014500182121992111\n",
      "batch 69 complete. Loss:  0.022896211594343185\n",
      "batch 70 complete. Loss:  0.023424843326210976\n",
      "batch 71 complete. Loss:  0.014412503689527512\n",
      "batch 72 complete. Loss:  0.013811090029776096\n",
      "batch 73 complete. Loss:  0.011947818100452423\n",
      "batch 74 complete. Loss:  0.01902252435684204\n",
      "batch 75 complete. Loss:  0.033101145178079605\n",
      "batch 76 complete. Loss:  0.019963864237070084\n",
      "batch 77 complete. Loss:  0.026100611314177513\n",
      "batch 78 complete. Loss:  0.012994755059480667\n",
      "batch 79 complete. Loss:  0.013112562708556652\n",
      "batch 80 complete. Loss:  0.01663510873913765\n",
      "batch 81 complete. Loss:  0.012324217706918716\n",
      "batch 82 complete. Loss:  0.026408769190311432\n",
      "batch 83 complete. Loss:  0.014827856793999672\n",
      "batch 84 complete. Loss:  0.01633245311677456\n",
      "batch 85 complete. Loss:  0.02124081924557686\n",
      "batch 86 complete. Loss:  0.019154086709022522\n",
      "batch 87 complete. Loss:  0.012716108001768589\n",
      "batch 88 complete. Loss:  0.012588129378855228\n",
      "batch 89 complete. Loss:  0.016644587740302086\n",
      "batch 90 complete. Loss:  0.020882710814476013\n",
      "batch 91 complete. Loss:  0.007830366492271423\n",
      "batch 92 complete. Loss:  0.019082827493548393\n",
      "batch 93 complete. Loss:  0.010810519568622112\n",
      "batch 94 complete. Loss:  0.029317473992705345\n",
      "batch 95 complete. Loss:  0.027353398501873016\n",
      "batch 96 complete. Loss:  0.08652202785015106\n",
      "batch 97 complete. Loss:  0.020961197093129158\n",
      "batch 98 complete. Loss:  0.020488424226641655\n",
      "batch 99 complete. Loss:  0.02471008710563183\n",
      "batch 100 complete. Loss:  0.026934698224067688\n",
      "batch 101 complete. Loss:  0.014898566529154778\n",
      "batch 102 complete. Loss:  0.007855920121073723\n",
      "batch 103 complete. Loss:  0.02295733615756035\n",
      "batch 104 complete. Loss:  0.00836876966059208\n",
      "batch 105 complete. Loss:  0.016466204077005386\n",
      "batch 106 complete. Loss:  0.014622143469750881\n",
      "batch 107 complete. Loss:  0.022895537316799164\n",
      "batch 108 complete. Loss:  0.018695373088121414\n",
      "batch 109 complete. Loss:  0.017539674416184425\n",
      "batch 110 complete. Loss:  0.01604902558028698\n",
      "batch 111 complete. Loss:  0.018569840118288994\n",
      "batch 112 complete. Loss:  0.037689823657274246\n",
      "batch 113 complete. Loss:  0.00830315425992012\n",
      "batch 114 complete. Loss:  0.07488224655389786\n",
      "batch 115 complete. Loss:  0.0257517471909523\n",
      "batch 116 complete. Loss:  0.01696094125509262\n",
      "batch 117 complete. Loss:  0.017442375421524048\n",
      "batch 118 complete. Loss:  0.02164837159216404\n",
      "batch 119 complete. Loss:  0.017494115978479385\n",
      "batch 120 complete. Loss:  0.014561988413333893\n",
      "batch 121 complete. Loss:  0.020151250064373016\n",
      "batch 122 complete. Loss:  0.010942663997411728\n",
      "batch 123 complete. Loss:  0.030840842053294182\n",
      "batch 124 complete. Loss:  0.029322868213057518\n",
      "batch 125 complete. Loss:  0.01367923989892006\n",
      "batch 126 complete. Loss:  0.012854572385549545\n",
      "batch 127 complete. Loss:  0.08514466881752014\n",
      "batch 128 complete. Loss:  0.03065471723675728\n",
      "batch 129 complete. Loss:  0.024061720818281174\n",
      "batch 130 complete. Loss:  0.023374827578663826\n",
      "batch 131 complete. Loss:  0.011201934888958931\n",
      "batch 132 complete. Loss:  0.018916895613074303\n",
      "batch 133 complete. Loss:  0.022071469575166702\n",
      "batch 134 complete. Loss:  0.018336959183216095\n",
      "batch 135 complete. Loss:  0.012255894020199776\n",
      "batch 136 complete. Loss:  0.017540816217660904\n",
      "batch 137 complete. Loss:  0.0164252407848835\n",
      "batch 138 complete. Loss:  0.02333463355898857\n",
      "batch 139 complete. Loss:  0.010596344247460365\n",
      "batch 140 complete. Loss:  0.04266829043626785\n",
      "batch 141 complete. Loss:  0.009318910539150238\n",
      "batch 142 complete. Loss:  0.015730341896414757\n",
      "batch 143 complete. Loss:  0.01089460402727127\n",
      "batch 144 complete. Loss:  0.017717678099870682\n",
      "batch 145 complete. Loss:  0.017925307154655457\n",
      "batch 146 complete. Loss:  0.009790463373064995\n",
      "batch 147 complete. Loss:  0.01697568967938423\n",
      "batch 148 complete. Loss:  0.027855250984430313\n",
      "batch 149 complete. Loss:  0.027327146381139755\n",
      "batch 150 complete. Loss:  0.015257300809025764\n",
      "batch 151 complete. Loss:  0.010583445429801941\n",
      "batch 152 complete. Loss:  0.019527848809957504\n",
      "batch 153 complete. Loss:  0.04277212917804718\n",
      "batch 154 complete. Loss:  0.018854718655347824\n",
      "batch 155 complete. Loss:  0.016378168016672134\n",
      "batch 156 complete. Loss:  0.03533385694026947\n",
      "batch 157 complete. Loss:  0.03418470174074173\n",
      "batch 158 complete. Loss:  0.013003870844841003\n",
      "batch 159 complete. Loss:  0.019750947132706642\n",
      "batch 160 complete. Loss:  0.019367758184671402\n",
      "batch 161 complete. Loss:  0.012742681428790092\n",
      "batch 162 complete. Loss:  0.015144187025725842\n",
      "batch 163 complete. Loss:  0.012743379920721054\n",
      "batch 164 complete. Loss:  0.022098323330283165\n",
      "batch 165 complete. Loss:  0.011584966443479061\n",
      "batch 166 complete. Loss:  0.021381132304668427\n",
      "batch 167 complete. Loss:  0.022241370752453804\n",
      "batch 168 complete. Loss:  0.013271672651171684\n",
      "batch 169 complete. Loss:  0.015712138265371323\n",
      "batch 170 complete. Loss:  0.0152829485014081\n",
      "batch 171 complete. Loss:  0.017274808138608932\n",
      "batch 172 complete. Loss:  0.03035094402730465\n",
      "batch 173 complete. Loss:  0.015086019411683083\n",
      "batch 174 complete. Loss:  0.023651178926229477\n",
      "batch 175 complete. Loss:  0.03058539889752865\n",
      "batch 176 complete. Loss:  0.02574111521244049\n",
      "batch 177 complete. Loss:  0.017394568771123886\n",
      "batch 178 complete. Loss:  0.026474781334400177\n",
      "batch 179 complete. Loss:  0.027708331122994423\n",
      "batch 180 complete. Loss:  0.014110922813415527\n",
      "batch 181 complete. Loss:  0.013975216075778008\n",
      "batch 182 complete. Loss:  0.01999250054359436\n",
      "batch 183 complete. Loss:  0.015989940613508224\n",
      "batch 184 complete. Loss:  0.026933547109365463\n",
      "batch 185 complete. Loss:  0.02302705869078636\n",
      "batch 186 complete. Loss:  0.0971313863992691\n",
      "batch 187 complete. Loss:  0.026190562173724174\n",
      "batch 188 complete. Loss:  0.019083429127931595\n",
      "batch 189 complete. Loss:  0.02840319089591503\n",
      "batch 190 complete. Loss:  0.023761341348290443\n",
      "batch 191 complete. Loss:  0.01598290726542473\n",
      "batch 192 complete. Loss:  0.022461123764514923\n",
      "batch 193 complete. Loss:  0.03382447361946106\n",
      "batch 194 complete. Loss:  0.023498568683862686\n",
      "batch 195 complete. Loss:  0.014888312667608261\n",
      "batch 196 complete. Loss:  0.022735949605703354\n",
      "batch 197 complete. Loss:  0.10061640292406082\n",
      "batch 198 complete. Loss:  0.044041551649570465\n",
      "batch 199 complete. Loss:  0.015331115573644638\n",
      "batch 200 complete. Loss:  0.022522952407598495\n",
      "batch 201 complete. Loss:  0.02580733224749565\n",
      "batch 202 complete. Loss:  0.028667889535427094\n",
      "batch 203 complete. Loss:  0.010564195923507214\n",
      "batch 204 complete. Loss:  0.019189869984984398\n",
      "batch 205 complete. Loss:  0.023631280288100243\n",
      "batch 206 complete. Loss:  0.02130991220474243\n",
      "batch 207 complete. Loss:  0.03316926956176758\n",
      "batch 208 complete. Loss:  0.011240051127970219\n",
      "batch 209 complete. Loss:  0.019041910767555237\n",
      "batch 210 complete. Loss:  0.015735559165477753\n",
      "batch 211 complete. Loss:  0.023154940456151962\n",
      "batch 212 complete. Loss:  0.015089861117303371\n",
      "batch 213 complete. Loss:  0.024923384189605713\n",
      "batch 214 complete. Loss:  0.02256263792514801\n",
      "batch 215 complete. Loss:  0.012822182849049568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 216 complete. Loss:  0.02144046127796173\n",
      "batch 217 complete. Loss:  0.011839254759252071\n",
      "batch 218 complete. Loss:  0.020326174795627594\n",
      "batch 219 complete. Loss:  0.02897736057639122\n",
      "batch 220 complete. Loss:  0.017894357442855835\n",
      "batch 221 complete. Loss:  0.024649931117892265\n",
      "batch 222 complete. Loss:  0.05282513052225113\n",
      "batch 223 complete. Loss:  0.092241071164608\n",
      "batch 224 complete. Loss:  0.015433081425726414\n",
      "batch 225 complete. Loss:  0.01850947178900242\n",
      "batch 226 complete. Loss:  0.021667830646038055\n",
      "batch 227 complete. Loss:  0.0816764160990715\n",
      "batch 228 complete. Loss:  0.01196311041712761\n",
      "batch 229 complete. Loss:  0.036263614892959595\n",
      "batch 230 complete. Loss:  0.018533412367105484\n",
      "batch 231 complete. Loss:  0.01428785640746355\n",
      "batch 232 complete. Loss:  0.023573333397507668\n",
      "batch 233 complete. Loss:  0.028844933956861496\n",
      "batch 234 complete. Loss:  0.02311450056731701\n",
      "batch 235 complete. Loss:  0.02304234728217125\n",
      "batch 236 complete. Loss:  0.01785244233906269\n",
      "batch 237 complete. Loss:  0.020265119150280952\n",
      "batch 238 complete. Loss:  0.020991016179323196\n",
      "batch 239 complete. Loss:  0.012827504426240921\n",
      "batch 240 complete. Loss:  0.020998600870370865\n",
      "batch 241 complete. Loss:  0.011375535279512405\n",
      "batch 242 complete. Loss:  0.012912511825561523\n",
      "batch 243 complete. Loss:  0.1379672884941101\n",
      "batch 244 complete. Loss:  0.007993584498763084\n",
      "batch 245 complete. Loss:  0.03206963837146759\n",
      "batch 246 complete. Loss:  0.02535940520465374\n",
      "batch 247 complete. Loss:  0.019291574135422707\n",
      "batch 248 complete. Loss:  0.057689834386110306\n",
      "batch 249 complete. Loss:  0.02534516528248787\n",
      "batch 250 complete. Loss:  0.11674836277961731\n",
      "batch 251 complete. Loss:  0.03473392501473427\n",
      "batch 252 complete. Loss:  0.023681730031967163\n",
      "batch 253 complete. Loss:  0.017148859798908234\n",
      "batch 254 complete. Loss:  0.010742178186774254\n",
      "batch 255 complete. Loss:  0.028263051062822342\n",
      "batch 256 complete. Loss:  0.015470200218260288\n",
      "batch 257 complete. Loss:  0.01811271160840988\n",
      "batch 258 complete. Loss:  0.010212965309619904\n",
      "batch 259 complete. Loss:  0.012002977542579174\n",
      "batch 260 complete. Loss:  0.0195116326212883\n",
      "batch 261 complete. Loss:  0.03352060168981552\n",
      "batch 262 complete. Loss:  0.11724802851676941\n",
      "batch 263 complete. Loss:  0.016054905951023102\n",
      "batch 264 complete. Loss:  0.014461932703852654\n",
      "batch 265 complete. Loss:  0.02554905042052269\n",
      "batch 266 complete. Loss:  0.02766864374279976\n",
      "batch 267 complete. Loss:  0.010404056869447231\n",
      "batch 268 complete. Loss:  0.026423130184412003\n",
      "batch 269 complete. Loss:  0.025728289037942886\n",
      "batch 270 complete. Loss:  0.034040797501802444\n",
      "batch 271 complete. Loss:  0.01681387796998024\n",
      "batch 272 complete. Loss:  0.03629179671406746\n",
      "batch 273 complete. Loss:  0.03479265794157982\n",
      "batch 274 complete. Loss:  0.018129700794816017\n",
      "batch 275 complete. Loss:  0.01182389073073864\n",
      "batch 276 complete. Loss:  0.02940133959054947\n",
      "batch 277 complete. Loss:  0.022862445563077927\n",
      "batch 278 complete. Loss:  0.044114433228969574\n",
      "batch 279 complete. Loss:  0.018889937549829483\n",
      "batch 280 complete. Loss:  0.013506855815649033\n",
      "batch 281 complete. Loss:  0.026611952111124992\n",
      "batch 282 complete. Loss:  0.024534188210964203\n",
      "batch 283 complete. Loss:  0.012722289189696312\n",
      "batch 284 complete. Loss:  0.010372871533036232\n",
      "batch 285 complete. Loss:  0.01997513137757778\n",
      "batch 286 complete. Loss:  0.07888288050889969\n",
      "batch 287 complete. Loss:  0.015872694551944733\n",
      "batch 288 complete. Loss:  0.012492996640503407\n",
      "batch 289 complete. Loss:  0.02247517928481102\n",
      "batch 290 complete. Loss:  0.06557312607765198\n",
      "batch 291 complete. Loss:  0.01752666011452675\n",
      "batch 292 complete. Loss:  0.041474148631095886\n",
      "batch 293 complete. Loss:  0.01950761303305626\n",
      "batch 294 complete. Loss:  0.011335616931319237\n",
      "batch 295 complete. Loss:  0.028149593621492386\n",
      "batch 296 complete. Loss:  0.012041250243782997\n",
      "batch 297 complete. Loss:  0.01020906399935484\n",
      "batch 298 complete. Loss:  0.03723715618252754\n",
      "batch 299 complete. Loss:  0.013617435470223427\n",
      "batch 300 complete. Loss:  0.02748931013047695\n",
      "batch 301 complete. Loss:  0.01327058020979166\n",
      "batch 302 complete. Loss:  0.03697054833173752\n",
      "batch 303 complete. Loss:  0.0190705806016922\n",
      "batch 304 complete. Loss:  0.011508435010910034\n",
      "batch 305 complete. Loss:  0.03364452347159386\n",
      "batch 306 complete. Loss:  0.019030777737498283\n",
      "batch 307 complete. Loss:  0.024088729172945023\n",
      "batch 308 complete. Loss:  0.020760387182235718\n",
      "batch 309 complete. Loss:  0.018720829859375954\n",
      "batch 310 complete. Loss:  0.016749583184719086\n",
      "batch 311 complete. Loss:  0.015344641171395779\n",
      "batch 312 complete. Loss:  0.028403718024492264\n",
      "batch 313 complete. Loss:  0.02163843810558319\n",
      "batch 314 complete. Loss:  0.021276434883475304\n",
      "batch 315 complete. Loss:  0.025303561240434647\n",
      "batch 316 complete. Loss:  0.023526741191744804\n",
      "batch 317 complete. Loss:  0.023740481585264206\n",
      "batch 318 complete. Loss:  0.01598680019378662\n",
      "batch 319 complete. Loss:  0.01527702622115612\n",
      "batch 320 complete. Loss:  0.02270592749118805\n",
      "batch 321 complete. Loss:  0.025471577420830727\n",
      "batch 322 complete. Loss:  0.03006444126367569\n",
      "batch 323 complete. Loss:  0.010704679414629936\n",
      "batch 324 complete. Loss:  0.08117668330669403\n",
      "batch 325 complete. Loss:  0.033163171261548996\n",
      "batch 326 complete. Loss:  0.013597642071545124\n",
      "batch 327 complete. Loss:  0.01924721524119377\n",
      "batch 328 complete. Loss:  0.018049297854304314\n",
      "batch 329 complete. Loss:  0.02587234601378441\n",
      "batch 330 complete. Loss:  0.014449688605964184\n",
      "batch 331 complete. Loss:  0.021995265036821365\n",
      "batch 332 complete. Loss:  0.01427990011870861\n",
      "batch 333 complete. Loss:  0.08275166153907776\n",
      "batch 334 complete. Loss:  0.015375006012618542\n",
      "batch 335 complete. Loss:  0.013372362591326237\n",
      "batch 336 complete. Loss:  0.019262615591287613\n",
      "batch 337 complete. Loss:  0.035758111625909805\n",
      "batch 338 complete. Loss:  0.010996266268193722\n",
      "batch 339 complete. Loss:  0.008147264830768108\n",
      "batch 340 complete. Loss:  0.020756838843226433\n",
      "batch 341 complete. Loss:  0.017647411674261093\n",
      "batch 342 complete. Loss:  0.09799101203680038\n",
      "batch 343 complete. Loss:  0.023928094655275345\n",
      "batch 344 complete. Loss:  0.03434257209300995\n",
      "batch 345 complete. Loss:  0.022396249696612358\n",
      "batch 346 complete. Loss:  0.011193253099918365\n",
      "batch 347 complete. Loss:  0.024183012545108795\n",
      "batch 348 complete. Loss:  0.016465507447719574\n",
      "batch 349 complete. Loss:  0.021147597581148148\n",
      "batch 350 complete. Loss:  0.042451512068510056\n",
      "batch 351 complete. Loss:  0.010515619069337845\n",
      "batch 352 complete. Loss:  0.01939711906015873\n",
      "batch 353 complete. Loss:  0.011829658411443233\n",
      "batch 354 complete. Loss:  0.013810012489557266\n",
      "batch 355 complete. Loss:  0.026474300771951675\n",
      "batch 356 complete. Loss:  0.013223331421613693\n",
      "batch 357 complete. Loss:  0.011980773881077766\n",
      "batch 358 complete. Loss:  0.012125477194786072\n",
      "batch 359 complete. Loss:  0.012843106873333454\n",
      "batch 360 complete. Loss:  0.023833099752664566\n",
      "batch 361 complete. Loss:  0.07991743087768555\n",
      "batch 362 complete. Loss:  0.052151937037706375\n",
      "batch 363 complete. Loss:  0.025273092091083527\n",
      "batch 364 complete. Loss:  0.10143588483333588\n",
      "batch 365 complete. Loss:  0.017793303355574608\n",
      "batch 366 complete. Loss:  0.03610942140221596\n",
      "batch 367 complete. Loss:  0.022216547280550003\n",
      "batch 368 complete. Loss:  0.025780456140637398\n",
      "batch 369 complete. Loss:  0.06885620951652527\n",
      "batch 370 complete. Loss:  0.02474159747362137\n",
      "batch 371 complete. Loss:  0.033885408192873\n",
      "batch 372 complete. Loss:  0.017574205994606018\n",
      "batch 373 complete. Loss:  0.022133316844701767\n",
      "batch 374 complete. Loss:  0.01590857282280922\n",
      "batch 375 complete. Loss:  0.02067083865404129\n",
      "batch 376 complete. Loss:  0.027803312987089157\n",
      "batch 377 complete. Loss:  0.015935372561216354\n",
      "batch 378 complete. Loss:  0.012919976375997066\n",
      "batch 379 complete. Loss:  0.021939558908343315\n",
      "batch 380 complete. Loss:  0.03575466573238373\n",
      "batch 381 complete. Loss:  0.062271032482385635\n",
      "batch 382 complete. Loss:  0.017123011872172356\n",
      "batch 383 complete. Loss:  0.06452500820159912\n",
      "batch 384 complete. Loss:  0.018920287489891052\n",
      "batch 385 complete. Loss:  0.07514571398496628\n",
      "batch 386 complete. Loss:  0.009984706528484821\n",
      "batch 387 complete. Loss:  0.026536619290709496\n",
      "batch 388 complete. Loss:  0.01679939031600952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 389 complete. Loss:  0.011471309699118137\n",
      "batch 390 complete. Loss:  0.033963460475206375\n",
      "batch 391 complete. Loss:  0.04267531633377075\n",
      "batch 392 complete. Loss:  0.03592502325773239\n",
      "batch 393 complete. Loss:  0.030236097052693367\n",
      "batch 394 complete. Loss:  0.05018514394760132\n",
      "batch 395 complete. Loss:  0.008755681104958057\n",
      "batch 396 complete. Loss:  0.01706615835428238\n",
      "batch 397 complete. Loss:  0.014740431681275368\n",
      "batch 398 complete. Loss:  0.02154339849948883\n",
      "batch 399 complete. Loss:  0.02919752523303032\n",
      "batch 400 complete. Loss:  0.035243988037109375\n",
      "batch 401 complete. Loss:  0.023573732003569603\n",
      "batch 402 complete. Loss:  0.013601049780845642\n",
      "batch 403 complete. Loss:  0.024186931550502777\n",
      "batch 404 complete. Loss:  0.02521764114499092\n",
      "batch 405 complete. Loss:  0.0328512005507946\n",
      "batch 406 complete. Loss:  0.01800210401415825\n",
      "batch 407 complete. Loss:  0.018050475046038628\n",
      "batch 408 complete. Loss:  0.0149502232670784\n",
      "batch 409 complete. Loss:  0.02839137613773346\n",
      "batch 410 complete. Loss:  0.03384264558553696\n",
      "batch 411 complete. Loss:  0.028812134638428688\n",
      "batch 412 complete. Loss:  0.019642826169729233\n",
      "batch 413 complete. Loss:  0.013143910095095634\n",
      "batch 414 complete. Loss:  0.02957654744386673\n",
      "batch 415 complete. Loss:  0.012500740587711334\n",
      "batch 416 complete. Loss:  0.03781188279390335\n",
      "batch 417 complete. Loss:  0.028185883536934853\n",
      "batch 418 complete. Loss:  0.01544129941612482\n",
      "batch 419 complete. Loss:  0.009166568517684937\n",
      "batch 420 complete. Loss:  0.01708676479756832\n",
      "batch 421 complete. Loss:  0.021823642775416374\n",
      "batch 422 complete. Loss:  0.021568913012742996\n",
      "batch 423 complete. Loss:  0.027347572147846222\n",
      "batch 424 complete. Loss:  0.029701221734285355\n",
      "batch 425 complete. Loss:  0.017827730625867844\n",
      "batch 426 complete. Loss:  0.030691511929035187\n",
      "batch 427 complete. Loss:  0.03313588351011276\n",
      "batch 428 complete. Loss:  0.020096927881240845\n",
      "batch 429 complete. Loss:  0.019302284345030785\n",
      "batch 430 complete. Loss:  0.0158630833029747\n",
      "batch 431 complete. Loss:  0.010277112945914268\n",
      "batch 432 complete. Loss:  0.029179921373724937\n",
      "batch 433 complete. Loss:  0.013979105278849602\n",
      "batch 434 complete. Loss:  0.026728592813014984\n",
      "batch 435 complete. Loss:  0.01966685615479946\n",
      "batch 436 complete. Loss:  0.01017072331160307\n",
      "batch 437 complete. Loss:  0.035075362771749496\n",
      "batch 438 complete. Loss:  0.0175459086894989\n",
      "batch 439 complete. Loss:  0.02183946780860424\n",
      "batch 440 complete. Loss:  0.02008591592311859\n",
      "batch 441 complete. Loss:  0.02182827889919281\n",
      "batch 442 complete. Loss:  0.03270985931158066\n",
      "batch 443 complete. Loss:  0.019206155091524124\n",
      "batch 444 complete. Loss:  0.052612315863370895\n",
      "batch 445 complete. Loss:  0.025628428906202316\n",
      "batch 446 complete. Loss:  0.024190353229641914\n",
      "batch 447 complete. Loss:  0.011494861915707588\n",
      "batch 448 complete. Loss:  0.011625228449702263\n",
      "batch 449 complete. Loss:  0.009255396202206612\n",
      "batch 450 complete. Loss:  0.02695557102560997\n",
      "batch 451 complete. Loss:  0.018294639885425568\n",
      "batch 452 complete. Loss:  0.013577951118350029\n",
      "batch 453 complete. Loss:  0.011845617555081844\n",
      "batch 454 complete. Loss:  0.014625857584178448\n",
      "batch 455 complete. Loss:  0.01627589389681816\n",
      "batch 456 complete. Loss:  0.019718747586011887\n",
      "batch 457 complete. Loss:  0.013457411900162697\n",
      "batch 458 complete. Loss:  0.020000644028186798\n",
      "batch 459 complete. Loss:  0.012646423652768135\n",
      "batch 460 complete. Loss:  0.05883891135454178\n",
      "batch 461 complete. Loss:  0.02020719274878502\n",
      "batch 462 complete. Loss:  0.07784689217805862\n",
      "batch 463 complete. Loss:  0.02219119295477867\n",
      "batch 464 complete. Loss:  0.026838097721338272\n",
      "batch 465 complete. Loss:  0.07699950039386749\n",
      "batch 466 complete. Loss:  0.014957784675061703\n",
      "batch 467 complete. Loss:  0.07135294377803802\n",
      "batch 468 complete. Loss:  0.009490299969911575\n",
      "batch 469 complete. Loss:  0.03663145750761032\n",
      "batch 470 complete. Loss:  0.01959148794412613\n",
      "batch 471 complete. Loss:  0.009624652564525604\n",
      "batch 472 complete. Loss:  0.01021416112780571\n",
      "batch 473 complete. Loss:  0.016196420416235924\n",
      "batch 474 complete. Loss:  0.021596504375338554\n",
      "batch 475 complete. Loss:  0.012087762355804443\n",
      "batch 476 complete. Loss:  0.005894577130675316\n",
      "batch 477 complete. Loss:  0.037629805505275726\n",
      "batch 478 complete. Loss:  0.02910466492176056\n",
      "batch 479 complete. Loss:  0.02311115711927414\n",
      "batch 480 complete. Loss:  0.015009935945272446\n",
      "batch 481 complete. Loss:  0.0359271839261055\n",
      "batch 482 complete. Loss:  0.023597806692123413\n",
      "batch 483 complete. Loss:  0.014684503898024559\n",
      "batch 484 complete. Loss:  0.020228568464517593\n",
      "batch 485 complete. Loss:  0.032296739518642426\n",
      "batch 486 complete. Loss:  0.01653481274843216\n",
      "batch 487 complete. Loss:  0.009397044777870178\n",
      "batch 488 complete. Loss:  0.014932881109416485\n",
      "batch 489 complete. Loss:  0.014384380541741848\n",
      "batch 490 complete. Loss:  0.008997490629553795\n",
      "batch 491 complete. Loss:  0.017690520733594894\n",
      "batch 492 complete. Loss:  0.060071032494306564\n",
      "batch 493 complete. Loss:  0.03302775323390961\n",
      "batch 494 complete. Loss:  0.026698477566242218\n",
      "batch 495 complete. Loss:  0.0540694035589695\n",
      "batch 496 complete. Loss:  0.08033278584480286\n",
      "batch 497 complete. Loss:  0.029964346438646317\n",
      "batch 498 complete. Loss:  0.021982569247484207\n",
      "batch 499 complete. Loss:  0.009199014864861965\n",
      "batch 500 complete. Loss:  0.012488506734371185\n",
      "batch 501 complete. Loss:  0.027142057195305824\n",
      "batch 502 complete. Loss:  0.02189440280199051\n",
      "batch 503 complete. Loss:  0.03117658570408821\n",
      "batch 504 complete. Loss:  0.019792545586824417\n",
      "batch 505 complete. Loss:  0.027074554935097694\n",
      "batch 506 complete. Loss:  0.021748557686805725\n",
      "batch 507 complete. Loss:  0.038765646517276764\n",
      "batch 508 complete. Loss:  0.025392621755599976\n",
      "batch 509 complete. Loss:  0.027705423533916473\n",
      "batch 510 complete. Loss:  0.008609546348452568\n",
      "batch 511 complete. Loss:  0.012912430800497532\n",
      "batch 512 complete. Loss:  0.06227770820260048\n",
      "batch 513 complete. Loss:  0.02284543216228485\n",
      "batch 514 complete. Loss:  0.010317189618945122\n",
      "batch 515 complete. Loss:  0.008278744295239449\n",
      "batch 516 complete. Loss:  0.020341504365205765\n",
      "batch 517 complete. Loss:  0.018897011876106262\n",
      "batch 518 complete. Loss:  0.013013537973165512\n",
      "batch 519 complete. Loss:  0.023965924978256226\n",
      "batch 520 complete. Loss:  0.010162502527236938\n",
      "batch 521 complete. Loss:  0.017115455120801926\n",
      "batch 522 complete. Loss:  0.0138259781524539\n",
      "batch 523 complete. Loss:  0.02699657529592514\n",
      "batch 524 complete. Loss:  0.03220892697572708\n",
      "batch 525 complete. Loss:  0.01860298588871956\n",
      "batch 526 complete. Loss:  0.030272837728261948\n",
      "batch 527 complete. Loss:  0.013135519810020924\n",
      "batch 528 complete. Loss:  0.027379699051380157\n",
      "batch 529 complete. Loss:  0.02196812629699707\n",
      "batch 530 complete. Loss:  0.021389007568359375\n",
      "batch 531 complete. Loss:  0.030491646379232407\n",
      "batch 532 complete. Loss:  0.01628107950091362\n",
      "batch 533 complete. Loss:  0.03281424567103386\n",
      "batch 534 complete. Loss:  0.016885582357645035\n",
      "batch 535 complete. Loss:  0.021019577980041504\n",
      "batch 536 complete. Loss:  0.022102978080511093\n",
      "batch 537 complete. Loss:  0.06772147119045258\n",
      "batch 538 complete. Loss:  0.01959880068898201\n",
      "batch 539 complete. Loss:  0.01119752787053585\n",
      "batch 540 complete. Loss:  0.03053940087556839\n",
      "batch 541 complete. Loss:  0.014334098435938358\n",
      "batch 542 complete. Loss:  0.01874426007270813\n",
      "batch 543 complete. Loss:  0.03194388374686241\n",
      "batch 544 complete. Loss:  0.01054577250033617\n",
      "batch 545 complete. Loss:  0.012102620676159859\n",
      "batch 546 complete. Loss:  0.012416415847837925\n",
      "batch 547 complete. Loss:  0.01437456626445055\n",
      "batch 548 complete. Loss:  0.02209986001253128\n",
      "batch 549 complete. Loss:  0.013984709978103638\n",
      "batch 550 complete. Loss:  0.019205017015337944\n",
      "batch 551 complete. Loss:  0.025498580187559128\n",
      "batch 552 complete. Loss:  0.03853824362158775\n",
      "batch 553 complete. Loss:  0.023858189582824707\n",
      "batch 554 complete. Loss:  0.01873784326016903\n",
      "batch 555 complete. Loss:  0.017880335450172424\n",
      "batch 556 complete. Loss:  0.011164565570652485\n",
      "batch 557 complete. Loss:  0.02098952978849411\n",
      "batch 558 complete. Loss:  0.02186136320233345\n",
      "batch 559 complete. Loss:  0.011866258457303047\n",
      "batch 560 complete. Loss:  0.021847106516361237\n",
      "batch 561 complete. Loss:  0.01771126687526703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 562 complete. Loss:  0.014613039791584015\n",
      "batch 563 complete. Loss:  0.05242014676332474\n",
      "batch 564 complete. Loss:  0.031628645956516266\n",
      "batch 565 complete. Loss:  0.02070210874080658\n",
      "batch 566 complete. Loss:  0.0147556746378541\n",
      "batch 567 complete. Loss:  0.020857280120253563\n",
      "batch 568 complete. Loss:  0.03588432818651199\n",
      "batch 569 complete. Loss:  0.011888779699802399\n",
      "batch 570 complete. Loss:  0.012901287525892258\n",
      "batch 571 complete. Loss:  0.012504443526268005\n",
      "batch 572 complete. Loss:  0.021137170493602753\n",
      "batch 573 complete. Loss:  0.017553411424160004\n",
      "batch 574 complete. Loss:  0.019014261662960052\n",
      "batch 575 complete. Loss:  0.023572102189064026\n",
      "batch 576 complete. Loss:  0.02308138646185398\n",
      "batch 577 complete. Loss:  0.04399605840444565\n",
      "batch 578 complete. Loss:  0.0235104002058506\n",
      "batch 579 complete. Loss:  0.033513765782117844\n",
      "batch 580 complete. Loss:  0.09511931985616684\n",
      "batch 581 complete. Loss:  0.01975051872432232\n",
      "batch 582 complete. Loss:  0.010578775778412819\n",
      "batch 583 complete. Loss:  0.01794486865401268\n",
      "batch 584 complete. Loss:  0.012745379470288754\n",
      "batch 585 complete. Loss:  0.020630542188882828\n",
      "batch 586 complete. Loss:  0.011970814317464828\n",
      "batch 587 complete. Loss:  0.012444816529750824\n",
      "batch 588 complete. Loss:  0.01145327277481556\n",
      "batch 589 complete. Loss:  0.01523735374212265\n",
      "batch 590 complete. Loss:  0.012775842100381851\n",
      "batch 591 complete. Loss:  0.012055967003107071\n",
      "batch 592 complete. Loss:  0.014330359175801277\n",
      "batch 593 complete. Loss:  0.014550544321537018\n",
      "batch 594 complete. Loss:  0.0203477144241333\n",
      "batch 595 complete. Loss:  0.018578149378299713\n",
      "batch 596 complete. Loss:  0.011285515502095222\n",
      "batch 597 complete. Loss:  0.013282060623168945\n",
      "batch 598 complete. Loss:  0.01760593056678772\n",
      "batch 599 complete. Loss:  0.011114001274108887\n",
      "batch 600 complete. Loss:  0.025332823395729065\n",
      "batch 601 complete. Loss:  0.02795669622719288\n",
      "batch 602 complete. Loss:  0.02019515074789524\n",
      "batch 603 complete. Loss:  0.023246586322784424\n",
      "batch 604 complete. Loss:  0.0233016237616539\n",
      "batch 605 complete. Loss:  0.020518556237220764\n",
      "batch 606 complete. Loss:  0.009541913866996765\n",
      "batch 607 complete. Loss:  0.015261240303516388\n",
      "batch 608 complete. Loss:  0.012257309630513191\n",
      "batch 609 complete. Loss:  0.012944887392222881\n",
      "batch 610 complete. Loss:  0.01538747176527977\n",
      "batch 611 complete. Loss:  0.015499250032007694\n",
      "batch 612 complete. Loss:  0.08944736421108246\n",
      "batch 613 complete. Loss:  0.013464599847793579\n",
      "batch 614 complete. Loss:  0.018992695957422256\n",
      "batch 615 complete. Loss:  0.016647247597575188\n",
      "batch 616 complete. Loss:  0.024232354015111923\n",
      "batch 617 complete. Loss:  0.028435256332159042\n",
      "batch 618 complete. Loss:  0.018538327887654305\n",
      "batch 619 complete. Loss:  0.021197661757469177\n",
      "batch 620 complete. Loss:  0.02352365106344223\n",
      "batch 621 complete. Loss:  0.0841602012515068\n",
      "batch 622 complete. Loss:  0.12551265954971313\n",
      "batch 623 complete. Loss:  0.010366292670369148\n",
      "batch 624 complete. Loss:  0.10533308982849121\n",
      "batch 625 complete. Loss:  0.015382537618279457\n",
      "batch 626 complete. Loss:  0.010101950727403164\n",
      "batch 627 complete. Loss:  0.015055309981107712\n",
      "batch 628 complete. Loss:  0.015829529613256454\n",
      "batch 629 complete. Loss:  0.0073089576326310635\n",
      "batch 630 complete. Loss:  0.02038760855793953\n",
      "batch 631 complete. Loss:  0.019641559571027756\n",
      "batch 632 complete. Loss:  0.05782172828912735\n",
      "batch 633 complete. Loss:  0.022075150161981583\n",
      "batch 634 complete. Loss:  0.025425702333450317\n",
      "batch 635 complete. Loss:  0.01273069716989994\n",
      "batch 636 complete. Loss:  0.014840803109109402\n",
      "batch 637 complete. Loss:  0.010580603033304214\n",
      "batch 638 complete. Loss:  0.011608525179326534\n",
      "batch 639 complete. Loss:  0.02935122698545456\n",
      "batch 640 complete. Loss:  0.012566832825541496\n",
      "batch 641 complete. Loss:  0.02730785310268402\n",
      "batch 642 complete. Loss:  0.013189793564379215\n",
      "batch 643 complete. Loss:  0.014448070898652077\n",
      "batch 644 complete. Loss:  0.012530408799648285\n",
      "batch 645 complete. Loss:  0.0269454475492239\n",
      "batch 646 complete. Loss:  0.023141149431467056\n",
      "batch 647 complete. Loss:  0.016610052436590195\n",
      "batch 648 complete. Loss:  0.010827213525772095\n",
      "batch 649 complete. Loss:  0.012286022305488586\n",
      "batch 650 complete. Loss:  0.016827914863824844\n",
      "batch 651 complete. Loss:  0.031640324741601944\n",
      "batch 652 complete. Loss:  0.01655551977455616\n",
      "batch 653 complete. Loss:  0.039736613631248474\n",
      "batch 654 complete. Loss:  0.0719524547457695\n",
      "batch 655 complete. Loss:  0.017452144995331764\n",
      "batch 656 complete. Loss:  0.018044130876660347\n",
      "batch 657 complete. Loss:  0.01549927331507206\n",
      "batch 658 complete. Loss:  0.06511295586824417\n",
      "batch 659 complete. Loss:  0.023606618866324425\n",
      "batch 660 complete. Loss:  0.012081409804522991\n",
      "batch 661 complete. Loss:  0.013756933622062206\n",
      "batch 662 complete. Loss:  0.01146860234439373\n",
      "batch 663 complete. Loss:  0.03460249677300453\n",
      "batch 664 complete. Loss:  0.019063450396060944\n",
      "batch 665 complete. Loss:  0.020133286714553833\n",
      "batch 666 complete. Loss:  0.023319600149989128\n",
      "batch 667 complete. Loss:  0.01469911728054285\n",
      "batch 668 complete. Loss:  0.02885960228741169\n",
      "batch 669 complete. Loss:  0.056289929896593094\n",
      "batch 670 complete. Loss:  0.01099321898072958\n",
      "batch 671 complete. Loss:  0.021247003227472305\n",
      "batch 672 complete. Loss:  0.059665217995643616\n",
      "batch 673 complete. Loss:  0.03883538022637367\n",
      "batch 674 complete. Loss:  0.01918640546500683\n",
      "Epoch 2 Validation Loss: 0.02898683602861046\n",
      "3 / 10\n",
      "batch 0 complete. Loss:  0.01940044015645981\n",
      "batch 1 complete. Loss:  0.010147538036108017\n",
      "batch 2 complete. Loss:  0.01114987675100565\n",
      "batch 3 complete. Loss:  0.013436747714877129\n",
      "batch 4 complete. Loss:  0.014735227450728416\n",
      "batch 5 complete. Loss:  0.020817458629608154\n",
      "batch 6 complete. Loss:  0.007110424339771271\n",
      "batch 7 complete. Loss:  0.02563774213194847\n",
      "batch 8 complete. Loss:  0.011094676330685616\n",
      "batch 9 complete. Loss:  0.021073488518595695\n",
      "batch 10 complete. Loss:  0.051912419497966766\n",
      "batch 11 complete. Loss:  0.015829194337129593\n",
      "batch 12 complete. Loss:  0.03826200217008591\n",
      "batch 13 complete. Loss:  0.015519731678068638\n",
      "batch 14 complete. Loss:  0.018030188977718353\n",
      "batch 15 complete. Loss:  0.01224144920706749\n",
      "batch 16 complete. Loss:  0.017998747527599335\n",
      "batch 17 complete. Loss:  0.03398779407143593\n",
      "batch 18 complete. Loss:  0.02111257240176201\n",
      "batch 19 complete. Loss:  0.00883228424936533\n",
      "batch 20 complete. Loss:  0.01796727627515793\n",
      "batch 21 complete. Loss:  0.010059709660708904\n",
      "batch 22 complete. Loss:  0.008907522074878216\n",
      "batch 23 complete. Loss:  0.025512276217341423\n",
      "batch 24 complete. Loss:  0.018159791827201843\n",
      "batch 25 complete. Loss:  0.025825902819633484\n",
      "batch 26 complete. Loss:  0.03665252774953842\n",
      "batch 27 complete. Loss:  0.012436160817742348\n",
      "batch 28 complete. Loss:  0.06099456921219826\n",
      "batch 29 complete. Loss:  0.021256528794765472\n",
      "batch 30 complete. Loss:  0.09843112528324127\n",
      "batch 31 complete. Loss:  0.01389656588435173\n",
      "batch 32 complete. Loss:  0.016829486936330795\n",
      "batch 33 complete. Loss:  0.040427595376968384\n",
      "batch 34 complete. Loss:  0.01752059906721115\n",
      "batch 35 complete. Loss:  0.026552438735961914\n",
      "batch 36 complete. Loss:  0.03428170457482338\n",
      "batch 37 complete. Loss:  0.013298962265253067\n",
      "batch 38 complete. Loss:  0.010125230066478252\n",
      "batch 39 complete. Loss:  0.036669451743364334\n",
      "batch 40 complete. Loss:  0.02867414429783821\n",
      "batch 41 complete. Loss:  0.03463023900985718\n",
      "batch 42 complete. Loss:  0.02798411436378956\n",
      "batch 43 complete. Loss:  0.014560221694409847\n",
      "batch 44 complete. Loss:  0.03289532661437988\n",
      "batch 45 complete. Loss:  0.0195161122828722\n",
      "batch 46 complete. Loss:  0.01864187978208065\n",
      "batch 47 complete. Loss:  0.035691291093826294\n",
      "batch 48 complete. Loss:  0.05139791965484619\n",
      "batch 49 complete. Loss:  0.014065800234675407\n",
      "batch 50 complete. Loss:  0.02060619741678238\n",
      "batch 51 complete. Loss:  0.030690019950270653\n",
      "batch 52 complete. Loss:  0.013608857989311218\n",
      "batch 53 complete. Loss:  0.04061609506607056\n",
      "batch 54 complete. Loss:  0.0693921446800232\n",
      "batch 55 complete. Loss:  0.013157553039491177\n",
      "batch 56 complete. Loss:  0.029853137210011482\n",
      "batch 57 complete. Loss:  0.013270655646920204\n",
      "batch 58 complete. Loss:  0.010598842054605484\n",
      "batch 59 complete. Loss:  0.014090057462453842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 60 complete. Loss:  0.04157329723238945\n",
      "batch 61 complete. Loss:  0.015482810325920582\n",
      "batch 62 complete. Loss:  0.018151776865124702\n",
      "batch 63 complete. Loss:  0.025266192853450775\n",
      "batch 64 complete. Loss:  0.016301041468977928\n",
      "batch 65 complete. Loss:  0.022087130695581436\n",
      "batch 66 complete. Loss:  0.026131972670555115\n",
      "batch 67 complete. Loss:  0.006483909208327532\n",
      "batch 68 complete. Loss:  0.013739025220274925\n",
      "batch 69 complete. Loss:  0.028576219454407692\n",
      "batch 70 complete. Loss:  0.011768635362386703\n",
      "batch 71 complete. Loss:  0.016481950879096985\n",
      "batch 72 complete. Loss:  0.018486058339476585\n",
      "batch 73 complete. Loss:  0.025826383382081985\n",
      "batch 74 complete. Loss:  0.01769445091485977\n",
      "batch 75 complete. Loss:  0.007706684060394764\n",
      "batch 76 complete. Loss:  0.015467380173504353\n",
      "batch 77 complete. Loss:  0.0332644060254097\n",
      "batch 78 complete. Loss:  0.010600458830595016\n",
      "batch 79 complete. Loss:  0.012345317751169205\n",
      "batch 80 complete. Loss:  0.04063792899250984\n",
      "batch 81 complete. Loss:  0.01980394870042801\n",
      "batch 82 complete. Loss:  0.012603161856532097\n",
      "batch 83 complete. Loss:  0.01746877282857895\n",
      "batch 84 complete. Loss:  0.013037042692303658\n",
      "batch 85 complete. Loss:  0.009411231614649296\n",
      "batch 86 complete. Loss:  0.01529847364872694\n",
      "batch 87 complete. Loss:  0.015246047638356686\n",
      "batch 88 complete. Loss:  0.02108103781938553\n",
      "batch 89 complete. Loss:  0.013560518622398376\n",
      "batch 90 complete. Loss:  0.010923286899924278\n",
      "batch 91 complete. Loss:  0.006822249852120876\n",
      "batch 92 complete. Loss:  0.07426407933235168\n",
      "batch 93 complete. Loss:  0.013414997607469559\n",
      "batch 94 complete. Loss:  0.023159794509410858\n",
      "batch 95 complete. Loss:  0.011403795331716537\n",
      "batch 96 complete. Loss:  0.05647458881139755\n",
      "batch 97 complete. Loss:  0.021036304533481598\n",
      "batch 98 complete. Loss:  0.026702186092734337\n",
      "batch 99 complete. Loss:  0.033531978726387024\n",
      "batch 100 complete. Loss:  0.010584671050310135\n",
      "batch 101 complete. Loss:  0.013339930213987827\n",
      "batch 102 complete. Loss:  0.018750213086605072\n",
      "batch 103 complete. Loss:  0.02386390045285225\n",
      "batch 104 complete. Loss:  0.007769746705889702\n",
      "batch 105 complete. Loss:  0.01745559647679329\n",
      "batch 106 complete. Loss:  0.016716375946998596\n",
      "batch 107 complete. Loss:  0.01923450641334057\n",
      "batch 108 complete. Loss:  0.008860422298312187\n",
      "batch 109 complete. Loss:  0.019636932760477066\n",
      "batch 110 complete. Loss:  0.015134420245885849\n",
      "batch 111 complete. Loss:  0.01398593932390213\n",
      "batch 112 complete. Loss:  0.01773959770798683\n",
      "batch 113 complete. Loss:  0.030613820999860764\n",
      "batch 114 complete. Loss:  0.019140005111694336\n",
      "batch 115 complete. Loss:  0.015178169123828411\n",
      "batch 116 complete. Loss:  0.01864630915224552\n",
      "batch 117 complete. Loss:  0.01087254099547863\n",
      "batch 118 complete. Loss:  0.012088717892765999\n",
      "batch 119 complete. Loss:  0.006959571037441492\n",
      "batch 120 complete. Loss:  0.08793811500072479\n",
      "batch 121 complete. Loss:  0.01880505681037903\n",
      "batch 122 complete. Loss:  0.017977183684706688\n",
      "batch 123 complete. Loss:  0.039781779050827026\n",
      "batch 124 complete. Loss:  0.02335362136363983\n",
      "batch 125 complete. Loss:  0.020864926278591156\n",
      "batch 126 complete. Loss:  0.012178262695670128\n",
      "batch 127 complete. Loss:  0.009709503501653671\n",
      "batch 128 complete. Loss:  0.018605299293994904\n",
      "batch 129 complete. Loss:  0.014173276722431183\n",
      "batch 130 complete. Loss:  0.015383278019726276\n",
      "batch 131 complete. Loss:  0.017984792590141296\n",
      "batch 132 complete. Loss:  0.07641111314296722\n",
      "batch 133 complete. Loss:  0.02102765440940857\n",
      "batch 134 complete. Loss:  0.010880507528781891\n",
      "batch 135 complete. Loss:  0.02755199931561947\n",
      "batch 136 complete. Loss:  0.011924413964152336\n",
      "batch 137 complete. Loss:  0.015171173959970474\n",
      "batch 138 complete. Loss:  0.012776085175573826\n",
      "batch 139 complete. Loss:  0.03546490520238876\n",
      "batch 140 complete. Loss:  0.019716337323188782\n",
      "batch 141 complete. Loss:  0.008458109572529793\n",
      "batch 142 complete. Loss:  0.014335408806800842\n",
      "batch 143 complete. Loss:  0.027109168469905853\n",
      "batch 144 complete. Loss:  0.031596120446920395\n",
      "batch 145 complete. Loss:  0.007236097939312458\n",
      "batch 146 complete. Loss:  0.06533645838499069\n",
      "batch 147 complete. Loss:  0.01782321184873581\n",
      "batch 148 complete. Loss:  0.025492586195468903\n",
      "batch 149 complete. Loss:  0.027267951518297195\n",
      "batch 150 complete. Loss:  0.01649361662566662\n",
      "batch 151 complete. Loss:  0.013245278969407082\n",
      "batch 152 complete. Loss:  0.020127661526203156\n",
      "batch 153 complete. Loss:  0.028275156393647194\n",
      "batch 154 complete. Loss:  0.026379261165857315\n",
      "batch 155 complete. Loss:  0.013624530285596848\n",
      "batch 156 complete. Loss:  0.01345869991928339\n",
      "batch 157 complete. Loss:  0.02142762765288353\n",
      "batch 158 complete. Loss:  0.029161835089325905\n",
      "batch 159 complete. Loss:  0.040908001363277435\n",
      "batch 160 complete. Loss:  0.03188129886984825\n",
      "batch 161 complete. Loss:  0.009353166446089745\n",
      "batch 162 complete. Loss:  0.01825006864964962\n",
      "batch 163 complete. Loss:  0.0195394828915596\n",
      "batch 164 complete. Loss:  0.01699085719883442\n",
      "batch 165 complete. Loss:  0.02244102768599987\n",
      "batch 166 complete. Loss:  0.015824146568775177\n",
      "batch 167 complete. Loss:  0.016069747507572174\n",
      "batch 168 complete. Loss:  0.022860007360577583\n",
      "batch 169 complete. Loss:  0.020520219579339027\n",
      "batch 170 complete. Loss:  0.02468913048505783\n",
      "batch 171 complete. Loss:  0.02574646845459938\n",
      "batch 172 complete. Loss:  0.025813966989517212\n",
      "batch 173 complete. Loss:  0.066849485039711\n",
      "batch 174 complete. Loss:  0.026798589155077934\n",
      "batch 175 complete. Loss:  0.02569112926721573\n",
      "batch 176 complete. Loss:  0.02614501118659973\n",
      "batch 177 complete. Loss:  0.01497659645974636\n",
      "batch 178 complete. Loss:  0.06497831642627716\n",
      "batch 179 complete. Loss:  0.012352061457931995\n",
      "batch 180 complete. Loss:  0.006710824556648731\n",
      "batch 181 complete. Loss:  0.01586097851395607\n",
      "batch 182 complete. Loss:  0.05713209509849548\n",
      "batch 183 complete. Loss:  0.00999058224260807\n",
      "batch 184 complete. Loss:  0.030262107029557228\n",
      "batch 185 complete. Loss:  0.0668567344546318\n",
      "batch 186 complete. Loss:  0.019633863121271133\n",
      "batch 187 complete. Loss:  0.028121035546064377\n",
      "batch 188 complete. Loss:  0.018557431176304817\n",
      "batch 189 complete. Loss:  0.012574953958392143\n",
      "batch 190 complete. Loss:  0.012179859913885593\n",
      "batch 191 complete. Loss:  0.010639520362019539\n",
      "batch 192 complete. Loss:  0.019761230796575546\n",
      "batch 193 complete. Loss:  0.018095888197422028\n",
      "batch 194 complete. Loss:  0.01697615534067154\n",
      "batch 195 complete. Loss:  0.017518222332000732\n",
      "batch 196 complete. Loss:  0.015080315992236137\n",
      "batch 197 complete. Loss:  0.020222052931785583\n",
      "batch 198 complete. Loss:  0.020222142338752747\n",
      "batch 199 complete. Loss:  0.011054137721657753\n",
      "batch 200 complete. Loss:  0.021473271772265434\n",
      "batch 201 complete. Loss:  0.00862653274089098\n",
      "batch 202 complete. Loss:  0.017376357689499855\n",
      "batch 203 complete. Loss:  0.0188752431422472\n",
      "batch 204 complete. Loss:  0.03839660808444023\n",
      "batch 205 complete. Loss:  0.015344033017754555\n",
      "batch 206 complete. Loss:  0.023676730692386627\n",
      "batch 207 complete. Loss:  0.013308090157806873\n",
      "batch 208 complete. Loss:  0.017830632627010345\n",
      "batch 209 complete. Loss:  0.04716362804174423\n",
      "batch 210 complete. Loss:  0.05368918180465698\n",
      "batch 211 complete. Loss:  0.016524523496627808\n",
      "batch 212 complete. Loss:  0.018655724823474884\n",
      "batch 213 complete. Loss:  0.02829248271882534\n",
      "batch 214 complete. Loss:  0.021379290148615837\n",
      "batch 215 complete. Loss:  0.051426053047180176\n",
      "batch 216 complete. Loss:  0.034950342029333115\n",
      "batch 217 complete. Loss:  0.028519993647933006\n",
      "batch 218 complete. Loss:  0.009329544380307198\n",
      "batch 219 complete. Loss:  0.017774082720279694\n",
      "batch 220 complete. Loss:  0.017542995512485504\n",
      "batch 221 complete. Loss:  0.017478201538324356\n",
      "batch 222 complete. Loss:  0.010571918450295925\n",
      "batch 223 complete. Loss:  0.016654808074235916\n",
      "batch 224 complete. Loss:  0.02035396173596382\n",
      "batch 225 complete. Loss:  0.0695362538099289\n",
      "batch 226 complete. Loss:  0.016027092933654785\n",
      "batch 227 complete. Loss:  0.0176418237388134\n",
      "batch 228 complete. Loss:  0.01640169695019722\n",
      "batch 229 complete. Loss:  0.035883743315935135\n",
      "batch 230 complete. Loss:  0.02524644136428833\n",
      "batch 231 complete. Loss:  0.009343493729829788\n",
      "batch 232 complete. Loss:  0.018549613654613495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 233 complete. Loss:  0.021990373730659485\n",
      "batch 234 complete. Loss:  0.03136830031871796\n",
      "batch 235 complete. Loss:  0.010923025198280811\n",
      "batch 236 complete. Loss:  0.020919034257531166\n",
      "batch 237 complete. Loss:  0.022106483578681946\n",
      "batch 238 complete. Loss:  0.02999497950077057\n",
      "batch 239 complete. Loss:  0.030307654291391373\n",
      "batch 240 complete. Loss:  0.029299236834049225\n",
      "batch 241 complete. Loss:  0.07377152889966965\n",
      "batch 242 complete. Loss:  0.017044594511389732\n",
      "batch 243 complete. Loss:  0.06425531953573227\n",
      "batch 244 complete. Loss:  0.05135812982916832\n",
      "batch 245 complete. Loss:  0.03904293477535248\n",
      "batch 246 complete. Loss:  0.015711385756731033\n",
      "batch 247 complete. Loss:  0.011810669675469398\n",
      "batch 248 complete. Loss:  0.01581648923456669\n",
      "batch 249 complete. Loss:  0.015236321836709976\n",
      "batch 250 complete. Loss:  0.024986974895000458\n",
      "batch 251 complete. Loss:  0.017722897231578827\n",
      "batch 252 complete. Loss:  0.022367049008607864\n",
      "batch 253 complete. Loss:  0.10401850938796997\n",
      "batch 254 complete. Loss:  0.0168826375156641\n",
      "batch 255 complete. Loss:  0.03890638053417206\n",
      "batch 256 complete. Loss:  0.02523885853588581\n",
      "batch 257 complete. Loss:  0.02090166136622429\n",
      "batch 258 complete. Loss:  0.014612758532166481\n",
      "batch 259 complete. Loss:  0.09328228235244751\n",
      "batch 260 complete. Loss:  0.028966840356588364\n",
      "batch 261 complete. Loss:  0.022280482575297356\n",
      "batch 262 complete. Loss:  0.03240586444735527\n",
      "batch 263 complete. Loss:  0.021250752732157707\n",
      "batch 264 complete. Loss:  0.016340188682079315\n",
      "batch 265 complete. Loss:  0.013867547735571861\n",
      "batch 266 complete. Loss:  0.02126264199614525\n",
      "batch 267 complete. Loss:  0.027332408353686333\n",
      "batch 268 complete. Loss:  0.042396873235702515\n",
      "batch 269 complete. Loss:  0.026678211987018585\n",
      "batch 270 complete. Loss:  0.018450148403644562\n",
      "batch 271 complete. Loss:  0.01261073350906372\n",
      "batch 272 complete. Loss:  0.026956789195537567\n",
      "batch 273 complete. Loss:  0.02004302106797695\n",
      "batch 274 complete. Loss:  0.0257120244204998\n",
      "batch 275 complete. Loss:  0.0336579903960228\n",
      "batch 276 complete. Loss:  0.012060523964464664\n",
      "batch 277 complete. Loss:  0.025187835097312927\n",
      "batch 278 complete. Loss:  0.011567195877432823\n",
      "batch 279 complete. Loss:  0.019147301092743874\n",
      "batch 280 complete. Loss:  0.02728550136089325\n",
      "batch 281 complete. Loss:  0.02422967739403248\n",
      "batch 282 complete. Loss:  0.0190193522721529\n",
      "batch 283 complete. Loss:  0.039277058094739914\n",
      "batch 284 complete. Loss:  0.010908856056630611\n",
      "batch 285 complete. Loss:  0.02660231664776802\n",
      "batch 286 complete. Loss:  0.023046113550662994\n",
      "batch 287 complete. Loss:  0.013839997351169586\n",
      "batch 288 complete. Loss:  0.02665511891245842\n",
      "batch 289 complete. Loss:  0.022932609543204308\n",
      "batch 290 complete. Loss:  0.014743638224899769\n",
      "batch 291 complete. Loss:  0.010011713951826096\n",
      "batch 292 complete. Loss:  0.022276349365711212\n",
      "batch 293 complete. Loss:  0.02522459626197815\n",
      "batch 294 complete. Loss:  0.01430923119187355\n",
      "batch 295 complete. Loss:  0.018045375123620033\n",
      "batch 296 complete. Loss:  0.02996855042874813\n",
      "batch 297 complete. Loss:  0.03694296255707741\n",
      "batch 298 complete. Loss:  0.011066976934671402\n",
      "batch 299 complete. Loss:  0.08400490880012512\n",
      "batch 300 complete. Loss:  0.01519286073744297\n",
      "batch 301 complete. Loss:  0.015930596739053726\n",
      "batch 302 complete. Loss:  0.0264937374740839\n",
      "batch 303 complete. Loss:  0.01908767782151699\n",
      "batch 304 complete. Loss:  0.02968357317149639\n",
      "batch 305 complete. Loss:  0.026550378650426865\n",
      "batch 306 complete. Loss:  0.01503981277346611\n",
      "batch 307 complete. Loss:  0.024456994608044624\n",
      "batch 308 complete. Loss:  0.009185103699564934\n",
      "batch 309 complete. Loss:  0.016767481341958046\n",
      "batch 310 complete. Loss:  0.026482300832867622\n",
      "batch 311 complete. Loss:  0.016112923622131348\n",
      "batch 312 complete. Loss:  0.06585343927145004\n",
      "batch 313 complete. Loss:  0.01784183643758297\n",
      "batch 314 complete. Loss:  0.02895568497478962\n",
      "batch 315 complete. Loss:  0.01794600859284401\n",
      "batch 316 complete. Loss:  0.01788109913468361\n",
      "batch 317 complete. Loss:  0.019004419445991516\n",
      "batch 318 complete. Loss:  0.024428829550743103\n",
      "batch 319 complete. Loss:  0.02238396927714348\n",
      "batch 320 complete. Loss:  0.009124623611569405\n",
      "batch 321 complete. Loss:  0.015180084854364395\n",
      "batch 322 complete. Loss:  0.014741106890141964\n",
      "batch 323 complete. Loss:  0.011509977281093597\n",
      "batch 324 complete. Loss:  0.013977400958538055\n",
      "batch 325 complete. Loss:  0.0141453817486763\n",
      "batch 326 complete. Loss:  0.010762479156255722\n",
      "batch 327 complete. Loss:  0.03297639265656471\n",
      "batch 328 complete. Loss:  0.021941864863038063\n",
      "batch 329 complete. Loss:  0.023547841235995293\n",
      "batch 330 complete. Loss:  0.015931425616145134\n",
      "batch 331 complete. Loss:  0.07304156571626663\n",
      "batch 332 complete. Loss:  0.02069520577788353\n",
      "batch 333 complete. Loss:  0.01458861492574215\n",
      "batch 334 complete. Loss:  0.01614612154662609\n",
      "batch 335 complete. Loss:  0.022012611851096153\n",
      "batch 336 complete. Loss:  0.020214315503835678\n",
      "batch 337 complete. Loss:  0.013141553848981857\n",
      "batch 338 complete. Loss:  0.016443168744444847\n",
      "batch 339 complete. Loss:  0.025083307176828384\n",
      "batch 340 complete. Loss:  0.02078988216817379\n",
      "batch 341 complete. Loss:  0.02412555366754532\n",
      "batch 342 complete. Loss:  0.014589780941605568\n",
      "batch 343 complete. Loss:  0.012268328107893467\n",
      "batch 344 complete. Loss:  0.02804526314139366\n",
      "batch 345 complete. Loss:  0.038817111402750015\n",
      "batch 346 complete. Loss:  0.0278088990598917\n",
      "batch 347 complete. Loss:  0.028853485360741615\n",
      "batch 348 complete. Loss:  0.013074619695544243\n",
      "batch 349 complete. Loss:  0.014313015155494213\n",
      "batch 350 complete. Loss:  0.02138490229845047\n",
      "batch 351 complete. Loss:  0.017499251291155815\n",
      "batch 352 complete. Loss:  0.008478766307234764\n",
      "batch 353 complete. Loss:  0.014210947789251804\n",
      "batch 354 complete. Loss:  0.018293164670467377\n",
      "batch 355 complete. Loss:  0.03476085141301155\n",
      "batch 356 complete. Loss:  0.023044342175126076\n",
      "batch 357 complete. Loss:  0.011981288902461529\n",
      "batch 358 complete. Loss:  0.014462620951235294\n",
      "batch 359 complete. Loss:  0.03993639349937439\n",
      "batch 360 complete. Loss:  0.027531616389751434\n",
      "batch 361 complete. Loss:  0.018612544983625412\n",
      "batch 362 complete. Loss:  0.019306883215904236\n",
      "batch 363 complete. Loss:  0.01758601889014244\n",
      "batch 364 complete. Loss:  0.014112841337919235\n",
      "batch 365 complete. Loss:  0.01608978398144245\n",
      "batch 366 complete. Loss:  0.016875920817255974\n",
      "batch 367 complete. Loss:  0.016736242920160294\n",
      "batch 368 complete. Loss:  0.009801477193832397\n",
      "batch 369 complete. Loss:  0.018509071320295334\n",
      "batch 370 complete. Loss:  0.01190297119319439\n",
      "batch 371 complete. Loss:  0.01358480378985405\n",
      "batch 372 complete. Loss:  0.021654371172189713\n",
      "batch 373 complete. Loss:  0.021362192928791046\n",
      "batch 374 complete. Loss:  0.018793512135744095\n",
      "batch 375 complete. Loss:  0.01515430212020874\n",
      "batch 376 complete. Loss:  0.040154457092285156\n",
      "batch 377 complete. Loss:  0.013782070949673653\n",
      "batch 378 complete. Loss:  0.018358368426561356\n",
      "batch 379 complete. Loss:  0.017470646649599075\n",
      "batch 380 complete. Loss:  0.018395032733678818\n",
      "batch 381 complete. Loss:  0.01390128955245018\n",
      "batch 382 complete. Loss:  0.025111960247159004\n",
      "batch 383 complete. Loss:  0.016766348853707314\n",
      "batch 384 complete. Loss:  0.009113255888223648\n",
      "batch 385 complete. Loss:  0.0114204790443182\n",
      "batch 386 complete. Loss:  0.10269378870725632\n",
      "batch 387 complete. Loss:  0.01577892154455185\n",
      "batch 388 complete. Loss:  0.01927056536078453\n",
      "batch 389 complete. Loss:  0.013697898015379906\n",
      "batch 390 complete. Loss:  0.008504770696163177\n",
      "batch 391 complete. Loss:  0.013490106910467148\n",
      "batch 392 complete. Loss:  0.059571169316768646\n",
      "batch 393 complete. Loss:  0.02472418174147606\n",
      "batch 394 complete. Loss:  0.02584226056933403\n",
      "batch 395 complete. Loss:  0.028073709458112717\n",
      "batch 396 complete. Loss:  0.03183551877737045\n",
      "batch 397 complete. Loss:  0.019795184955000877\n",
      "batch 398 complete. Loss:  0.013882559724152088\n",
      "batch 399 complete. Loss:  0.011444786563515663\n",
      "batch 400 complete. Loss:  0.011735067702829838\n",
      "batch 401 complete. Loss:  0.012832771986722946\n",
      "batch 402 complete. Loss:  0.017977382987737656\n",
      "batch 403 complete. Loss:  0.023210693150758743\n",
      "batch 404 complete. Loss:  0.012392843142151833\n",
      "batch 405 complete. Loss:  0.006365057080984116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 406 complete. Loss:  0.017470721155405045\n",
      "batch 407 complete. Loss:  0.01946035772562027\n",
      "batch 408 complete. Loss:  0.013616206124424934\n",
      "batch 409 complete. Loss:  0.013306587934494019\n",
      "batch 410 complete. Loss:  0.011310759000480175\n",
      "batch 411 complete. Loss:  0.017994048073887825\n",
      "batch 412 complete. Loss:  0.08794421702623367\n",
      "batch 413 complete. Loss:  0.022125568240880966\n",
      "batch 414 complete. Loss:  0.023128200322389603\n",
      "batch 415 complete. Loss:  0.014109062030911446\n",
      "batch 416 complete. Loss:  0.010925554670393467\n",
      "batch 417 complete. Loss:  0.012014295905828476\n",
      "batch 418 complete. Loss:  0.010999014601111412\n",
      "batch 419 complete. Loss:  0.02771419659256935\n",
      "batch 420 complete. Loss:  0.01206435076892376\n",
      "batch 421 complete. Loss:  0.028633514419198036\n",
      "batch 422 complete. Loss:  0.01708446629345417\n",
      "batch 423 complete. Loss:  0.020071709528565407\n",
      "batch 424 complete. Loss:  0.009466583840548992\n",
      "batch 425 complete. Loss:  0.009638387709856033\n",
      "batch 426 complete. Loss:  0.016518892720341682\n",
      "batch 427 complete. Loss:  0.020113911479711533\n",
      "batch 428 complete. Loss:  0.012167753651738167\n",
      "batch 429 complete. Loss:  0.07407470047473907\n",
      "batch 430 complete. Loss:  0.032450996339321136\n",
      "batch 431 complete. Loss:  0.011724725365638733\n",
      "batch 432 complete. Loss:  0.010915109887719154\n",
      "batch 433 complete. Loss:  0.014836547896265984\n",
      "batch 434 complete. Loss:  0.01035179104655981\n",
      "batch 435 complete. Loss:  0.020221803337335587\n",
      "batch 436 complete. Loss:  0.01286608912050724\n",
      "batch 437 complete. Loss:  0.07764468342065811\n",
      "batch 438 complete. Loss:  0.02376812882721424\n",
      "batch 439 complete. Loss:  0.011970060877501965\n",
      "batch 440 complete. Loss:  0.02246520295739174\n",
      "batch 441 complete. Loss:  0.017470691353082657\n",
      "batch 442 complete. Loss:  0.012565447948873043\n",
      "batch 443 complete. Loss:  0.018391147255897522\n",
      "batch 444 complete. Loss:  0.021923281252384186\n",
      "batch 445 complete. Loss:  0.009412938728928566\n",
      "batch 446 complete. Loss:  0.06030208244919777\n",
      "batch 447 complete. Loss:  0.015671804547309875\n",
      "batch 448 complete. Loss:  0.02368580549955368\n",
      "batch 449 complete. Loss:  0.011373225599527359\n",
      "batch 450 complete. Loss:  0.008101425133645535\n",
      "batch 451 complete. Loss:  0.015104830265045166\n",
      "batch 452 complete. Loss:  0.016224399209022522\n",
      "batch 453 complete. Loss:  0.01285190973430872\n",
      "batch 454 complete. Loss:  0.016279038041830063\n",
      "batch 455 complete. Loss:  0.04929078742861748\n",
      "batch 456 complete. Loss:  0.013331705704331398\n",
      "batch 457 complete. Loss:  0.016276992857456207\n",
      "batch 458 complete. Loss:  0.023203646764159203\n",
      "batch 459 complete. Loss:  0.020314587280154228\n",
      "batch 460 complete. Loss:  0.010038015432655811\n",
      "batch 461 complete. Loss:  0.04195253551006317\n",
      "batch 462 complete. Loss:  0.0171414352953434\n",
      "batch 463 complete. Loss:  0.024678515270352364\n",
      "batch 464 complete. Loss:  0.013933854177594185\n",
      "batch 465 complete. Loss:  0.026890913024544716\n",
      "batch 466 complete. Loss:  0.013249093666672707\n",
      "batch 467 complete. Loss:  0.013803379610180855\n",
      "batch 468 complete. Loss:  0.007411239203065634\n",
      "batch 469 complete. Loss:  0.059628091752529144\n",
      "batch 470 complete. Loss:  0.014159541577100754\n",
      "batch 471 complete. Loss:  0.025506291538476944\n",
      "batch 472 complete. Loss:  0.007329887710511684\n",
      "batch 473 complete. Loss:  0.01755904220044613\n",
      "batch 474 complete. Loss:  0.010739033110439777\n",
      "batch 475 complete. Loss:  0.012998932041227818\n",
      "batch 476 complete. Loss:  0.020634986460208893\n",
      "batch 477 complete. Loss:  0.02645633928477764\n",
      "batch 478 complete. Loss:  0.011848758906126022\n",
      "batch 479 complete. Loss:  0.018071472644805908\n",
      "batch 480 complete. Loss:  0.012605719268321991\n",
      "batch 481 complete. Loss:  0.04351259768009186\n",
      "batch 482 complete. Loss:  0.01619328185915947\n",
      "batch 483 complete. Loss:  0.008383935317397118\n",
      "batch 484 complete. Loss:  0.036450210958719254\n",
      "batch 485 complete. Loss:  0.024603305384516716\n",
      "batch 486 complete. Loss:  0.025957945734262466\n",
      "batch 487 complete. Loss:  0.018281986936926842\n",
      "batch 488 complete. Loss:  0.011451473459601402\n",
      "batch 489 complete. Loss:  0.014095306396484375\n",
      "batch 490 complete. Loss:  0.013758756220340729\n",
      "batch 491 complete. Loss:  0.01591377705335617\n",
      "batch 492 complete. Loss:  0.019688062369823456\n",
      "batch 493 complete. Loss:  0.0294576957821846\n",
      "batch 494 complete. Loss:  0.030982406809926033\n",
      "batch 495 complete. Loss:  0.030778933316469193\n",
      "batch 496 complete. Loss:  0.015159998089075089\n",
      "batch 497 complete. Loss:  0.022438136860728264\n",
      "batch 498 complete. Loss:  0.03476376086473465\n",
      "batch 499 complete. Loss:  0.011247677728533745\n",
      "batch 500 complete. Loss:  0.011367169208824635\n",
      "batch 501 complete. Loss:  0.012988206930458546\n",
      "batch 502 complete. Loss:  0.007562136277556419\n",
      "batch 503 complete. Loss:  0.00853378139436245\n",
      "batch 504 complete. Loss:  0.006919151172041893\n",
      "batch 505 complete. Loss:  0.01431226171553135\n",
      "batch 506 complete. Loss:  0.016041351482272148\n",
      "batch 507 complete. Loss:  0.020100504159927368\n",
      "batch 508 complete. Loss:  0.0146792558953166\n",
      "batch 509 complete. Loss:  0.01807381771504879\n",
      "batch 510 complete. Loss:  0.08158444613218307\n",
      "batch 511 complete. Loss:  0.009024199098348618\n",
      "batch 512 complete. Loss:  0.004518171306699514\n",
      "batch 513 complete. Loss:  0.015470568090677261\n",
      "batch 514 complete. Loss:  0.01493304967880249\n",
      "batch 515 complete. Loss:  0.014720983803272247\n",
      "batch 516 complete. Loss:  0.019268294796347618\n",
      "batch 517 complete. Loss:  0.017509344965219498\n",
      "batch 518 complete. Loss:  0.016301017254590988\n",
      "batch 519 complete. Loss:  0.017961308360099792\n",
      "batch 520 complete. Loss:  0.015723295509815216\n",
      "batch 521 complete. Loss:  0.007831228896975517\n",
      "batch 522 complete. Loss:  0.019362173974514008\n",
      "batch 523 complete. Loss:  0.04113592207431793\n",
      "batch 524 complete. Loss:  0.01356707513332367\n",
      "batch 525 complete. Loss:  0.009776409715414047\n",
      "batch 526 complete. Loss:  0.09789194911718369\n",
      "batch 527 complete. Loss:  0.007251800503581762\n",
      "batch 528 complete. Loss:  0.017854727804660797\n",
      "batch 529 complete. Loss:  0.010051880031824112\n",
      "batch 530 complete. Loss:  0.017731323838233948\n",
      "batch 531 complete. Loss:  0.018048737198114395\n",
      "batch 532 complete. Loss:  0.025263691321015358\n",
      "batch 533 complete. Loss:  0.02818749099969864\n",
      "batch 534 complete. Loss:  0.012199776247143745\n",
      "batch 535 complete. Loss:  0.018261604011058807\n",
      "batch 536 complete. Loss:  0.01569449156522751\n",
      "batch 537 complete. Loss:  0.022770894691348076\n",
      "batch 538 complete. Loss:  0.08607334643602371\n",
      "batch 539 complete. Loss:  0.013227902352809906\n",
      "batch 540 complete. Loss:  0.020634207874536514\n",
      "batch 541 complete. Loss:  0.015905233100056648\n",
      "batch 542 complete. Loss:  0.0063895490020513535\n",
      "batch 543 complete. Loss:  0.011766912415623665\n",
      "batch 544 complete. Loss:  0.014587162993848324\n",
      "batch 545 complete. Loss:  0.009319168515503407\n",
      "batch 546 complete. Loss:  0.016452249139547348\n",
      "batch 547 complete. Loss:  0.0124050322920084\n",
      "batch 548 complete. Loss:  0.012455817312002182\n",
      "batch 549 complete. Loss:  0.008969158865511417\n",
      "batch 550 complete. Loss:  0.008393984287977219\n",
      "batch 551 complete. Loss:  0.02990677021443844\n",
      "batch 552 complete. Loss:  0.02072087861597538\n",
      "batch 553 complete. Loss:  0.00837501510977745\n",
      "batch 554 complete. Loss:  0.015436552464962006\n",
      "batch 555 complete. Loss:  0.01268499530851841\n",
      "batch 556 complete. Loss:  0.01527220569550991\n",
      "batch 557 complete. Loss:  0.023004211485385895\n",
      "batch 558 complete. Loss:  0.035346269607543945\n",
      "batch 559 complete. Loss:  0.012019559741020203\n",
      "batch 560 complete. Loss:  0.014053625985980034\n",
      "batch 561 complete. Loss:  0.01518038660287857\n",
      "batch 562 complete. Loss:  0.012713229283690453\n",
      "batch 563 complete. Loss:  0.008456067182123661\n",
      "batch 564 complete. Loss:  0.1079673022031784\n",
      "batch 565 complete. Loss:  0.029871126636862755\n",
      "batch 566 complete. Loss:  0.015993375331163406\n",
      "batch 567 complete. Loss:  0.09335699677467346\n",
      "batch 568 complete. Loss:  0.03235616534948349\n",
      "batch 569 complete. Loss:  0.015439016744494438\n",
      "batch 570 complete. Loss:  0.014193566516041756\n",
      "batch 571 complete. Loss:  0.017074842005968094\n",
      "batch 572 complete. Loss:  0.02049599587917328\n",
      "batch 573 complete. Loss:  0.021362897008657455\n",
      "batch 574 complete. Loss:  0.017279861494898796\n",
      "batch 575 complete. Loss:  0.01202087476849556\n",
      "batch 576 complete. Loss:  0.01842958852648735\n",
      "batch 577 complete. Loss:  0.023831652477383614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 578 complete. Loss:  0.010977024212479591\n",
      "batch 579 complete. Loss:  0.013660670258104801\n",
      "batch 580 complete. Loss:  0.01737244427204132\n",
      "batch 581 complete. Loss:  0.016077782958745956\n",
      "batch 582 complete. Loss:  0.019955996423959732\n",
      "batch 583 complete. Loss:  0.012571057304739952\n",
      "batch 584 complete. Loss:  0.016852430999279022\n",
      "batch 585 complete. Loss:  0.02043125405907631\n",
      "batch 586 complete. Loss:  0.01997404173016548\n",
      "batch 587 complete. Loss:  0.03546028584241867\n",
      "batch 588 complete. Loss:  0.017196374014019966\n",
      "batch 589 complete. Loss:  0.009214727208018303\n",
      "batch 590 complete. Loss:  0.02125869132578373\n",
      "batch 591 complete. Loss:  0.016203057020902634\n",
      "batch 592 complete. Loss:  0.02294744923710823\n",
      "batch 593 complete. Loss:  0.01617533341050148\n",
      "batch 594 complete. Loss:  0.014458108693361282\n",
      "batch 595 complete. Loss:  0.02224375121295452\n",
      "batch 596 complete. Loss:  0.02578083798289299\n",
      "batch 597 complete. Loss:  0.018214765936136246\n",
      "batch 598 complete. Loss:  0.019211653620004654\n",
      "batch 599 complete. Loss:  0.01090214867144823\n",
      "batch 600 complete. Loss:  0.014947781339287758\n",
      "batch 601 complete. Loss:  0.016156639903783798\n",
      "batch 602 complete. Loss:  0.015178008005023003\n",
      "batch 603 complete. Loss:  0.006613689009100199\n",
      "batch 604 complete. Loss:  0.024642720818519592\n",
      "batch 605 complete. Loss:  0.011518482118844986\n",
      "batch 606 complete. Loss:  0.016233332455158234\n",
      "batch 607 complete. Loss:  0.0140764145180583\n",
      "batch 608 complete. Loss:  0.007962265983223915\n",
      "batch 609 complete. Loss:  0.026977330446243286\n",
      "batch 610 complete. Loss:  0.022410839796066284\n",
      "batch 611 complete. Loss:  0.014809731394052505\n",
      "batch 612 complete. Loss:  0.01606603153049946\n",
      "batch 613 complete. Loss:  0.007641289383172989\n",
      "batch 614 complete. Loss:  0.011138025671243668\n",
      "batch 615 complete. Loss:  0.010017534717917442\n",
      "batch 616 complete. Loss:  0.023299798369407654\n",
      "batch 617 complete. Loss:  0.021070750430226326\n",
      "batch 618 complete. Loss:  0.013930624350905418\n",
      "batch 619 complete. Loss:  0.017826426774263382\n",
      "batch 620 complete. Loss:  0.015495880506932735\n",
      "batch 621 complete. Loss:  0.09688541293144226\n",
      "batch 622 complete. Loss:  0.009742673486471176\n",
      "batch 623 complete. Loss:  0.017462845891714096\n",
      "batch 624 complete. Loss:  0.014274939894676208\n",
      "batch 625 complete. Loss:  0.013791972771286964\n",
      "batch 626 complete. Loss:  0.01965120993554592\n",
      "batch 627 complete. Loss:  0.028569679707288742\n",
      "batch 628 complete. Loss:  0.0306487288326025\n",
      "batch 629 complete. Loss:  0.0874645784497261\n",
      "batch 630 complete. Loss:  0.029799027368426323\n",
      "batch 631 complete. Loss:  0.07593798637390137\n",
      "batch 632 complete. Loss:  0.023647233843803406\n",
      "batch 633 complete. Loss:  0.012781806290149689\n",
      "batch 634 complete. Loss:  0.007332213688641787\n",
      "batch 635 complete. Loss:  0.021880842745304108\n",
      "batch 636 complete. Loss:  0.014411550015211105\n",
      "batch 637 complete. Loss:  0.06461364030838013\n",
      "batch 638 complete. Loss:  0.03741252422332764\n",
      "batch 639 complete. Loss:  0.020326435565948486\n",
      "batch 640 complete. Loss:  0.019294995814561844\n",
      "batch 641 complete. Loss:  0.014038238674402237\n",
      "batch 642 complete. Loss:  0.023336727172136307\n",
      "batch 643 complete. Loss:  0.028306232765316963\n",
      "batch 644 complete. Loss:  0.026618946343660355\n",
      "batch 645 complete. Loss:  0.02108251303434372\n",
      "batch 646 complete. Loss:  0.013083291240036488\n",
      "batch 647 complete. Loss:  0.01761300303041935\n",
      "batch 648 complete. Loss:  0.02087872102856636\n",
      "batch 649 complete. Loss:  0.009120574221014977\n",
      "batch 650 complete. Loss:  0.013199979439377785\n",
      "batch 651 complete. Loss:  0.012379184365272522\n",
      "batch 652 complete. Loss:  0.019382130354642868\n",
      "batch 653 complete. Loss:  0.016763411462306976\n",
      "batch 654 complete. Loss:  0.011150863021612167\n",
      "batch 655 complete. Loss:  0.015760162845253944\n",
      "batch 656 complete. Loss:  0.07085071504116058\n",
      "batch 657 complete. Loss:  0.015409793704748154\n",
      "batch 658 complete. Loss:  0.029010389000177383\n",
      "batch 659 complete. Loss:  0.018656723201274872\n",
      "batch 660 complete. Loss:  0.02967897057533264\n",
      "batch 661 complete. Loss:  0.03386995196342468\n",
      "batch 662 complete. Loss:  0.01898372732102871\n",
      "batch 663 complete. Loss:  0.009373584762215614\n",
      "batch 664 complete. Loss:  0.019124433398246765\n",
      "batch 665 complete. Loss:  0.043172962963581085\n",
      "batch 666 complete. Loss:  0.0175037682056427\n",
      "batch 667 complete. Loss:  0.013685420155525208\n",
      "batch 668 complete. Loss:  0.029648056253790855\n",
      "batch 669 complete. Loss:  0.0137931564822793\n",
      "batch 670 complete. Loss:  0.012131407856941223\n",
      "batch 671 complete. Loss:  0.016933970153331757\n",
      "batch 672 complete. Loss:  0.02846684865653515\n",
      "batch 673 complete. Loss:  0.06980326771736145\n",
      "batch 674 complete. Loss:  0.027817953377962112\n",
      "Epoch 3 Validation Loss: 0.029811674511313616\n",
      "4 / 10\n",
      "batch 0 complete. Loss:  0.05455053970217705\n",
      "batch 1 complete. Loss:  0.013259264640510082\n",
      "batch 2 complete. Loss:  0.03089536540210247\n",
      "batch 3 complete. Loss:  0.016918404027819633\n",
      "batch 4 complete. Loss:  0.058308057487010956\n",
      "batch 5 complete. Loss:  0.03185683488845825\n",
      "batch 6 complete. Loss:  0.015777699649333954\n",
      "batch 7 complete. Loss:  0.01701694168150425\n",
      "batch 8 complete. Loss:  0.027055934071540833\n",
      "batch 9 complete. Loss:  0.030985314399003983\n",
      "batch 10 complete. Loss:  0.02954527549445629\n",
      "batch 11 complete. Loss:  0.013655109331011772\n",
      "batch 12 complete. Loss:  0.016725581139326096\n",
      "batch 13 complete. Loss:  0.02312367781996727\n",
      "batch 14 complete. Loss:  0.016331542283296585\n",
      "batch 15 complete. Loss:  0.04945733770728111\n",
      "batch 16 complete. Loss:  0.015611153095960617\n",
      "batch 17 complete. Loss:  0.014080271124839783\n",
      "batch 18 complete. Loss:  0.028874551877379417\n",
      "batch 19 complete. Loss:  0.017668895423412323\n",
      "batch 20 complete. Loss:  0.01923467591404915\n",
      "batch 21 complete. Loss:  0.018564095720648766\n",
      "batch 22 complete. Loss:  0.03430943936109543\n",
      "batch 23 complete. Loss:  0.02338469959795475\n",
      "batch 24 complete. Loss:  0.020647453144192696\n",
      "batch 25 complete. Loss:  0.01160342525690794\n",
      "batch 26 complete. Loss:  0.013864223845303059\n",
      "batch 27 complete. Loss:  0.020513445138931274\n",
      "batch 28 complete. Loss:  0.020864862948656082\n",
      "batch 29 complete. Loss:  0.014101943001151085\n",
      "batch 30 complete. Loss:  0.026685791090130806\n",
      "batch 31 complete. Loss:  0.023078642785549164\n",
      "batch 32 complete. Loss:  0.021637320518493652\n",
      "batch 33 complete. Loss:  0.017348429188132286\n",
      "batch 34 complete. Loss:  0.011369059793651104\n",
      "batch 35 complete. Loss:  0.028234219178557396\n",
      "batch 36 complete. Loss:  0.01490834727883339\n",
      "batch 37 complete. Loss:  0.018577024340629578\n",
      "batch 38 complete. Loss:  0.013399999588727951\n",
      "batch 39 complete. Loss:  0.022975962609052658\n",
      "batch 40 complete. Loss:  0.012914524413645267\n",
      "batch 41 complete. Loss:  0.03346593677997589\n",
      "batch 42 complete. Loss:  0.0414070188999176\n",
      "batch 43 complete. Loss:  0.021664677187800407\n",
      "batch 44 complete. Loss:  0.0289006270468235\n",
      "batch 45 complete. Loss:  0.019055066630244255\n",
      "batch 46 complete. Loss:  0.018221478909254074\n",
      "batch 47 complete. Loss:  0.018965288996696472\n",
      "batch 48 complete. Loss:  0.010696927085518837\n",
      "batch 49 complete. Loss:  0.08468864113092422\n",
      "batch 50 complete. Loss:  0.06742221117019653\n",
      "batch 51 complete. Loss:  0.032320111989974976\n",
      "batch 52 complete. Loss:  0.00876208208501339\n",
      "batch 53 complete. Loss:  0.0637737363576889\n",
      "batch 54 complete. Loss:  0.016588805243372917\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-819523c1ca11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nn_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0mrelevant_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             \u001b[0;31m# TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/overrides.py\u001b[0m in \u001b[0;36mhas_torch_function\u001b[0;34m(relevant_args)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__torch_function__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disabled_torch_function_impl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_disabled_torch_function_impl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m     )\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.module.bert.parameters(), 'lr': 1e-5}, \n",
    "    {'params': model.module.nn.parameters(), 'lr': 2e-5} # our neural net\n",
    "])\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Modify your training loop (add validation)\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch,\"/\",num_epochs)\n",
    "    b=0\n",
    "    ## Training Phase\n",
    "    model.train()  # Set model to training mode\n",
    "    current_epoch_train_losses = []\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to('cuda') \n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        labels = batch['labels'].to('cuda')\n",
    "        data = batch['nn_data'].to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, nn_data=data)\n",
    "        \n",
    "        loss = loss_fn(outputs.squeeze(1), labels) # Ensure outputs are single-dimensional\n",
    "        current_epoch_train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        print(f\"batch {b} complete. Loss: \",loss.item())\n",
    "        b+=1\n",
    "    train_losses.append(sum(current_epoch_train_losses)/len(current_epoch_train_losses))\n",
    "    ## Validation Phase\n",
    "    model.eval()   # Set model to evaluation mode\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to('cuda') \n",
    "            attention_mask = batch['attention_mask'].to('cuda')\n",
    "            labels = batch['labels'].to('cuda')\n",
    "            #data = batch['nn_data'].to('cuda')\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)#,nn_data=data)\n",
    "            val_loss += loss_fn(outputs.squeeze(1), labels).item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch} Validation Loss: {val_loss}\")\n",
    "\n",
    "    # checkpoint model\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), f\"model_checkpoint_{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[212931.05557763]\n",
      "[212931.05557763] [237500.]\n"
     ]
    }
   ],
   "source": [
    "idx =101\n",
    "\n",
    "text=scaled_data['text'][idx]\n",
    "\n",
    "data = torch.tensor(nn_data[idx]).unsqueeze(0)\n",
    "\n",
    "# inference\n",
    "inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to('cuda')\n",
    "outputs = model(**inputs, nn_data=data)\n",
    "pred = scaler.inverse_transform(np.array([outputs.item()]))\n",
    "print(pred)\n",
    "print(pred, scaler.inverse_transform(np.array([scaled_data['label'][idx]])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.module.bert.state_dict(), 'bert.pth')\n",
    "# torch.save(model.module.nn.state_dict(),'nn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.11987135104145165,\n",
       "  0.03311058937123528,\n",
       "  0.024399708265921582,\n",
       "  0.022464662807545176],\n",
       " [0.043224400289621226,\n",
       "  0.038426278271993415,\n",
       "  0.02898683602861046,\n",
       "  0.029811674511313616])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "model.eval()\n",
    "with torch.no_grad():  # Disable gradient calculation for validation\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to('cuda') \n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        labels = batch['labels'].to('cuda')\n",
    "        data = batch['nn_data'].to('cuda')\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, nn_data=data)\n",
    "        \n",
    "        loss = loss_fn(outputs.squeeze(1), labels)\n",
    "        train_losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "with torch.no_grad():  # Disable gradient calculation for validation\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch['input_ids'].to('cuda') \n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        labels = batch['labels'].to('cuda')\n",
    "        data = batch['nn_data'].to('cuda')\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, nn_data=data)\n",
    "        \n",
    "        loss = loss_fn(outputs.squeeze(1), labels)\n",
    "        test_losses.append(loss.item())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1174.7601068998558, 1394.194963908308)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses = np.array(train_losses).reshape(-1,1)\n",
    "test_losses=np.array(test_losses).reshape(-1,1)\n",
    "\n",
    "mean = scaler.inverse_transform(np.array([np.mean(nn_data)]).reshape(-1,1)).flatten()[0]\n",
    "\n",
    "inv_train_losses = scaler.inverse_transform(train_losses) - mean\n",
    "inv_test_losses = scaler.inverse_transform(test_losses) - mean\n",
    "np.mean(inv_train_losses), np.mean(inv_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training err: $1174.7601068998558\n",
      "Median training err: $963.019283004207\n",
      "Mean test err: $1394.194963908308\n",
      "Median test err: $1024.363229226583\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean training err: $\" +  str(np.mean(inv_train_losses)))\n",
    "print(\"Median training err: $\" + str(np.median(inv_train_losses)))\n",
    "print(\"Mean test err: $\" + str(np.mean(inv_test_losses)))\n",
    "print(\"Median test err: $\" + str(np.median(inv_test_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, test_losses = ([0.11987135104145165,\n",
    "  0.03311058937123528,\n",
    "  0.024399708265921582,\n",
    "  0.022464662807545176],\n",
    " [0.043224400289621226,\n",
    "  0.038426278271993415,\n",
    "  0.02898683602861046,\n",
    "  0.029811674511313616])\n",
    "\n",
    "# inverse transform\n",
    "train_losses = [scaler.inverse_transform(np.array([loss]).reshape(-1,1)).flatten() for loss in train_losses]\n",
    "test_losses = [scaler.inverse_transform(np.array([loss]).reshape(-1,1)).flatten() for loss in test_losses]\n",
    "\n",
    "# subtract mean\n",
    "mean = scaler.inverse_transform(np.array([np.mean(nn_data)]).reshape(-1,1)).flatten()[0]\n",
    "train_losses = [loss - mean for loss in train_losses]\n",
    "test_losses = [loss - mean for loss in test_losses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHcCAYAAAA3PbXpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABw5UlEQVR4nO3dd1hT5/sG8DsBEmZYMgUFcYFbXLgHFa221dpaLVbcW6vWr7a/WrXWVqutVuuqraPDUW3Vui0qalVcuBcuFGUrEkA2Ob8/YiIR0AQDAXJ/ritXyTkvJ0+CldvnvOe8IkEQBBAREREZMbGhCyAiIiIyNAYiIiIiMnoMRERERGT0GIiIiIjI6DEQERERkdFjICIiIiKjx0BERERERo+BiIiIiIweAxEREREZPQYiIqp0Zs2aBZFIZOgySt26desgEolw7949Q5dCVOExEBEZIdUv0oIPZ2dndOrUCXv37i00/sWxBR+jRo1Sjxs0aJDGPqlUitq1a2PGjBnIysoCAHh5eb30eKrHunXriq2/Y8eOxX7fjRs39P556eKbb77B9u3bDVoDEenO1NAFEJHhzJ49G97e3hAEAQkJCVi3bh3efPNN7Ny5Ez179tQY+8Ybb2DgwIGFjlG7dm2N51KpFL/88gsAQC6X459//sFXX32FO3fuYP369fjhhx+Qnp6uHr9nzx5s3LgRixYtQpUqVdTbW7du/dLaPTw8MHfu3ELb3d3dMX36dHz66aev/gBKwTfffIP33nsPvXr1KvXX+uijj9CvXz9IpdJSfy2iyo6BiMiIde/eHc2aNVM/Hzp0KFxcXLBx48ZCgah27doYMGDAK49pamqqMW7MmDFo3bo1Nm7ciIULFxYKCvHx8di4cSN69eoFLy8vrWu3tbV9aT2mppX/rzcTExOYmJgYugyiSoGnzIhIzc7ODhYWFnoNEyKRCG3btoUgCLh7967ejvsyRc0hEolEGDduHLZv34769etDKpWiXr162LdvX6Hvj4mJwZAhQ+Di4qIet2bNmle+rkgkwtOnT/Hrr7+qT+ENGjQIgPJ0YlGB73VqLWoOkZeXF3r27Iljx46hRYsWMDc3R40aNfDbb78Veu1Lly6hQ4cOsLCwgIeHB+bMmYO1a9dyXhIZpcr/TygiKpZcLsejR48gCAISExPx448/Ij09vcjOS1ZWFh49elRou0wmg0QieenrqH652tvb66VuAMjPzy9Uj7m5OaytrYv9nmPHjmHr1q0YM2YMbGxssGTJEvTp0wfR0dFwdHQEACQkJKBVq1bqUOLk5IS9e/di6NChSE1NxcSJE4s9/u+//45hw4ahRYsWGDFiBADAx8enRO9Pm1qLc/v2bbz33nsYOnQoQkJCsGbNGgwaNAj+/v6oV68eAGXo69SpE0QiET777DNYWVnhl19+4ek3Ml4CERmdtWvXCgAKPaRSqbBu3bpC44saq3ps3LhRPS4kJESwsrISkpKShKSkJOH27dvCd999J4hEIqF+/fqCQqEodOwFCxYIAISoqCit6+/QoUORtYSEhAiCIAgzZ84UXvzrDYAgkUiE27dvq7ddvHhRACD8+OOP6m1Dhw4V3NzchEePHml8f79+/QRbW1shIyPjpbVZWVmp6ygoJCREqF69eqHtr1Or6udY8LOrXr26AEA4evSoeltiYqIglUqFTz75RL1t/PjxgkgkEs6fP6/e9vjxY8HBwUHnnwdRZcAOEZERW7ZsmXpSdEJCAv744w8MGzYMNjY2ePfddzXGvvPOOxg3blyhYzRo0EDj+dOnT+Hk5KSxrW3bturTSPri5eWFn3/+WWObu7v7S78nMDBQo2PTsGFDyGQy9ak8QRDw999/o2/fvhAEQaMDFRQUhE2bNuHcuXNo06aN3t5HSWt9GT8/P7Rr10793MnJCXXq1NH43n379iEgIACNGzdWb3NwcEBwcDB+/PFH/bwJogqEgYjIiLVo0UJjUnX//v3RpEkTjBs3Dj179tQ4Febh4YHAwMBXHtPc3Bw7d+4EADx8+BDz589HYmIiLCws9Fq7lZWVVvUUVK1atULb7O3t8eTJEwBAUlISUlJSsGrVKqxatarIYyQmJupebAm8qtbX/d779+8jICCg0LiaNWvqWClR5cBARERqYrEYnTp1wuLFi3Hr1i31fBNdmJiYaASVoKAg1K1bFyNHjsSOHTv0Wa7OirsiSxAEAIBCoQAADBgwACEhIUWObdiwYYleu7juWH5+fpHbX1Xry7zO9xIZKwYiItKQl5cHABr3Cnodbm5umDRpEr788kucPHkSrVq10stxS4OTkxNsbGyQn5+vc/dJpbjgY29vj5SUlELb79+/X6LXeV3Vq1fH7du3C20vahuRMeBl90Sklpubi3///RcSiQS+vr56O+748eNhaWmJefPm6e2YpcHExAR9+vTB33//jStXrhTan5SU9MpjWFlZFRl8fHx8IJfLcenSJfW2uLg4bNu27bVqLqmgoCCEh4fjwoUL6m3JyclYv369QeohMjR2iIiM2N69e9VLXSQmJmLDhg24desWPv30U8hkMo2xN2/exB9//FHoGC4uLnjjjTde+jqOjo4YPHgwli9fjuvXr+s1bOnbvHnzEBYWhpYtW2L48OHw8/NDcnIyzp07hwMHDiA5Ofml3+/v748DBw5g4cKFcHd3h7e3N1q2bIl+/fph2rRp6N27NyZMmICMjAysWLECtWvXxrlz58ro3T03depU/PHHH3jjjTcwfvx49WX31apVQ3JyslGsBUdUEAMRkRGbMWOG+mtzc3PUrVsXK1aswMiRIwuNDQ0NRWhoaKHtHTp0eGUgAoDJkydj5cqV+Pbbb1+6Tpmhubi44PTp05g9eza2bt2K5cuXw9HREfXq1cO33377yu9fuHAhRowYgenTpyMzMxMhISFo2bIlHB0dsW3bNkyePBlTp06Ft7c35s6di1u3bhkkEHl6eiIsLAwTJkzAN998AycnJ4wdOxZWVlaYMGECzM3Ny7wmIkMSCZxlR0REz0ycOBE//fQT0tPTuSwIGRXOISIiMlKZmZkazx8/fozff/8dbdu2ZRgio8NTZkRERiogIAAdO3aEr68vEhISsHr1aqSmpuKLL74wdGlEZY6BiIjISL355pv466+/sGrVKohEIjRt2hSrV69G+/btDV0aUZnjHCIiIiIyepxDREREREaPgYiIiIiMHgMRUQUyaNAgeHl5GbqMEunYsSM6duxY5q9b1GcmEokwa9asV37vrFmz9H6DwsOHD0MkEuHw4cN6PS4RvR4GIiI9EIlEWj34S7B4586dg0gkwvTp04sdc+vWLYhEIkyePLkMKyuZ5cuXl7sbUHbs2BH169c3dBlE5RKvMiPSg99//13j+W+//YbQ0NBC2193yYqff/5ZvSJ7ZdO0aVPUrVsXGzduxJw5c4ocs2HDBgDK1ehfR2ZmJkxNS/evv+XLl6NKlSoYNGiQxvb27dsjMzMTEomkVF+fiHTDQESkBy/+gj558iRCQ0Nf+Ys7IyMDlpaWWr+OmZlZieqrKIKDg/HFF1/g5MmTaNWqVaH9GzduRN26ddG0adPXeh1DLkshFou5LAZROcRTZkRlRHW6IiIiAu3bt4elpSX+7//+DwDwzz//oEePHnB3d4dUKoWPjw+++uor5Ofnaxzjxfkw9+7dg0gkwnfffYdVq1bBx8cHUqkUzZs3x5kzZ15ZU3JyMqZMmYIGDRrA2toaMpkM3bt3x8WLFzXGqea9bN68GV9//TU8PDxgbm6OLl264Pbt24WOq6rFwsICLVq0wH///afVZxQcHAzgeSeooIiICERGRqrHaPuZFaWoOUTHjh1D8+bNYW5uDh8fH/z0009Ffu/atWvRuXNnODs7QyqVws/PDytWrNAY4+XlhatXr+LIkSPq06Wq+VPFzSHasmUL/P39YWFhgSpVqmDAgAGIiYnRGDNo0CBYW1sjJiYGvXr1grW1NZycnDBlyhSt3re2li9fjnr16kEqlcLd3R1jx45FSkqKxphbt26hT58+cHV1hbm5OTw8PNCvXz/I5XL1mNDQULRt2xZ2dnawtrZGnTp11H/mVbKzszFz5kzUrFkTUqkUnp6emDp1KrKzszXGaXMsotfBDhFRGXr8+DG6d++Ofv36YcCAAXBxcQEArFu3DtbW1pg8eTKsra1x6NAhzJgxA6mpqViwYMErj7thwwakpaVh5MiREIlEmD9/Pt59913cvXv3pV2lu3fvYvv27Xj//ffh7e2NhIQE/PTTT+jQoQOuXbsGd3d3jfHz5s2DWCzGlClTIJfLMX/+fAQHB+PUqVPqMatXr8bIkSPRunVrTJw4EXfv3sXbb78NBwcHeHp6vvR9eHt7o3Xr1ti8eTMWLVqksXyEKiR9+OGHevnMCrp8+TK6du0KJycnzJo1C3l5eZg5c6b651PQihUrUK9ePbz99tswNTXFzp07MWbMGCgUCowdOxYA8MMPP2D8+PGwtrbG559/DgBFHktl3bp1GDx4MJo3b465c+ciISEBixcvxvHjx3H+/HnY2dmpx+bn5yMoKAgtW7bEd999hwMHDuD777+Hj48PRo8erdP7LsqsWbPw5ZdfIjAwEKNHj0ZkZCRWrFiBM2fO4Pjx4zAzM0NOTg6CgoKQnZ2N8ePHw9XVFTExMdi1axdSUlJga2uLq1evomfPnmjYsCFmz54NqVSK27dv4/jx4+rXUigUePvtt3Hs2DGMGDECvr6+uHz5MhYtWoSbN29i+/btAKDVsYhem0BEejd27Fjhxf+9OnToIAAQVq5cWWh8RkZGoW0jR44ULC0thaysLPW2kJAQoXr16urnUVFRAgDB0dFRSE5OVm//559/BADCzp07X1pnVlaWkJ+fr7EtKipKkEqlwuzZs9XbwsLCBACCr6+vkJ2drd6+ePFiAYBw+fJlQRAEIScnR3B2dhYaN26sMW7VqlUCAKFDhw4vrUcQBGHZsmUCAGH//v3qbfn5+ULVqlWFgIAA9baSfmaCIAgAhJkzZ6qf9+rVSzA3Nxfu37+v3nbt2jXBxMSk0M+xqNcNCgoSatSoobGtXr16Rb5f1WcZFhYmCMLzz6x+/fpCZmametyuXbsEAMKMGTM03gsAjZ+NIAhCkyZNBH9//0Kv9aIOHToI9erVK3Z/YmKiIJFIhK5du2r8uVi6dKkAQFizZo0gCIJw/vx5AYCwZcuWYo+1aNEiAYCQlJRU7Jjff/9dEIvFwn///aexfeXKlQIA4fjx41ofi+h18ZQZURmSSqUYPHhwoe0WFhbqr9PS0vDo0SO0a9cOGRkZuHHjxiuP+8EHH8De3l79vF27dgCUHaBX1SMWK/8ayM/Px+PHj9WnI86dO1do/ODBgzUmA7/4OmfPnkViYiJGjRqlMW7QoEGwtbV95ftQvRczMzON02ZHjhxBTEyM+nQZ8PqfmUp+fj7279+PXr16oVq1aurtvr6+CAoKKjS+4OvK5XI8evQIHTp0wN27dzVOF2lL9ZmNGTNGY25Rjx49ULduXezevbvQ94waNUrjebt27V75s9bGgQMHkJOTg4kTJ6r/XADA8OHDIZPJ1LWofpb79+9HRkZGkcdSdbX++eefYi8E2LJlC3x9fVG3bl08evRI/ejcuTMAICwsTOtjEb0uBiKiMlS1atUiry66evUqevfuDVtbW8hkMjg5OaknZGvzS7bgL3IA6nD05MmTl36fQqHAokWLUKtWLUilUlSpUgVOTk64dOlSka/7qte5f/8+AKBWrVoa48zMzFCjRo1Xvg8AcHR0RFBQELZt24asrCwAytNlpqam6Nu3r3rc635mKklJScjMzCxUMwDUqVOn0Lbjx48jMDAQVlZWsLOzg5OTk3ouS0kCkeozK+q16tatq96vYm5uDicnJ41t9vb2r/xZv04tEokENWrUUO/39vbG5MmT8csvv6BKlSoICgrCsmXLNN7/Bx98gDZt2mDYsGFwcXFBv379sHnzZo1Ac+vWLVy9ehVOTk4aj9q1awMAEhMTtT4W0eviHCKiMlSwu6CSkpKCDh06QCaTYfbs2fDx8YG5uTnOnTuHadOmafWXfsG5NgUJr1iq8JtvvsEXX3yBIUOG4KuvvoKDgwPEYjEmTpxY5OuW9HV0NWDAAOzatQu7du3C22+/jb///ls9xwfQz2dWEnfu3EGXLl1Qt25dLFy4EJ6enpBIJNizZw8WLVpUJr+gi/sZlLXvv/8egwYNwj///IN///0XEyZMwNy5c3Hy5El4eHjAwsICR48eRVhYGHbv3o19+/bhzz//ROfOnfHvv//CxMQECoUCDRo0wMKFC4t8DdWcM22ORfS6GIiIDOzw4cN4/Pgxtm7dqrHKeFRUVKm/9l9//YVOnTph9erVGttTUlJQpUoVnY9XvXp1AMp/+atOewBAbm4uoqKi0KhRI62O8/bbb8PGxgYbNmyAmZkZnjx5onG6TJ+fmZOTEywsLHDr1q1C+yIjIzWe79y5E9nZ2dixY4dGt0x1aqcgbe9wrfrMIiMjNT4z1TbV/rJQsJaCHb2cnBxERUUhMDBQY3yDBg3QoEEDTJ8+HSdOnECbNm2wcuVK9X2kxGIxunTpgi5dumDhwoX45ptv8PnnnyMsLAyBgYHw8fHBxYsX0aVLl1d+Xq86FtHr4ikzIgNT/eu2YJclJycHy5cvL5PXfrG7s2XLlkKXe2urWbNmcHJywsqVK5GTk6Pevm7dukKXbb+MhYUFevfujT179mDFihWwsrLCO++8o1E3oJ/PzMTEBEFBQdi+fTuio6PV269fv479+/cXGvvi68rlcqxdu7bQca2srLR6z82aNYOzszNWrlypcan53r17cf36dfTo0UPXt1RigYGBkEgkWLJkicZ7XL16NeRyubqW1NRU5OXlaXxvgwYNIBaL1e8hOTm50PEbN24MAOoxffv2RUxMDH7++edCYzMzM/H06VOtj0X0utghIjKw1q1bw97eHiEhIZgwYQJEIhF+//13vZ+GKkrPnj0xe/ZsDB48GK1bt8bly5exfv16ref7vMjMzAxz5szByJEj0blzZ3zwwQeIiorC2rVrdT7mgAED8Ntvv2H//v0IDg6GlZWVep++P7Mvv/wS+/btQ7t27TBmzBjk5eXhxx9/RL169XDp0iX1uK5du0IikeCtt97CyJEjkZ6ejp9//hnOzs6Ii4vTOKa/vz9WrFiBOXPmoGbNmnB2di7UAQKUn9m3336LwYMHo0OHDujfv7/6snsvLy9MmjSpRO+pOElJSUXeCdzb2xvBwcH47LPP8OWXX6Jbt254++23ERkZieXLl6N58+bqOVqHDh3CuHHj8P7776N27drIy8vD77//DhMTE/Tp0wcAMHv2bBw9ehQ9evRA9erVkZiYiOXLl8PDwwNt27YFAHz00UfYvHkzRo0ahbCwMLRp0wb5+fm4ceMGNm/ejP3796NZs2ZaHYvotRns+jaiSqy4y+6Lu+T5+PHjQqtWrQQLCwvB3d1dmDp1qrB//36Ny7MFofjL7hcsWFDomHjh0vKiZGVlCZ988ong5uYmWFhYCG3atBHCw8OFDh06aFwyrrpU/MXLrFWvv3btWo3ty5cvF7y9vQWpVCo0a9ZMOHr0aKFjvkpeXp7g5uYmABD27NlTaH9JPzNBKPqzOXLkiODv7y9IJBKhRo0awsqVK4WZM2cW+jnu2LFDaNiwoWBubi54eXkJ3377rbBmzRoBgBAVFaUeFx8fL/To0UOwsbHRuOXAi5fdq/z5559CkyZNBKlUKjg4OAjBwcHCw4cPNcaEhIQIVlZWhT6LouosiurWD0U9unTpoh63dOlSoW7duoKZmZng4uIijB49Wnjy5Il6/927d4UhQ4YIPj4+grm5ueDg4CB06tRJOHDggHrMwYMHhXfeeUdwd3cXJBKJ4O7uLvTv31+4efOmRk05OTnCt99+K9SrV0+QSqWCvb294O/vL3z55ZeCXC7X6VhEr0MkCGXwz1AiIiKicoxziIiIiMjoMRARERGR0WMgIiIiIqPHQERERERGj4GIiIiIjB4DERERERk93phRCwqFArGxsbCxsdH6dvxERERkWIIgIC0tDe7u7hCLX94DYiDSQmxsrHqRQSIiIqpYHjx4AA8Pj5eOYSDSgo2NDQDlByqTyQxcDREREWkjNTUVnp6e6t/jL8NApAXVaTKZTMZAREREVMFoM92Fk6qJiIjI6DEQERERkdFjICIiIiKjxzlERERkVBQKBXJycgxdBumJRCJ55SX12mAgIiIio5GTk4OoqCgoFApDl0J6IhaL4e3tDYlE8lrHYSAiIiKjIAgC4uLiYGJiAk9PT710FciwVDdOjouLQ7Vq1V7r5skMREREZBTy8vKQkZEBd3d3WFpaGroc0hMnJyfExsYiLy8PZmZmJT4O4zERERmF/Px8AHjtUytUvqh+nqqfb0kxEBERkVHhmpSVi75+ngxEREREZPQYiIiIiIyIl5cXfvjhB0OXUe5wUjUREVE517FjRzRu3FgvQebMmTOwsrJ6/aIqGXaIDOzJ0xxceJBi6DKIiKgCEwQBeXl5Wo11cnLiVXZFYCAyoIj7T9D220MYu/4ccvJ4kzAiIips0KBBOHLkCBYvXgyRSASRSIR169ZBJBJh79698Pf3h1QqxbFjx3Dnzh288847cHFxgbW1NZo3b44DBw5oHO/FU2YikQi//PILevfuDUtLS9SqVQs7duwo43dpeAxEBlTPXQYrqSliUjKxJeKBocshIjIqgiAgIyfPIA9BELSuc/HixQgICMDw4cMRFxeHuLg4eHp6AgA+/fRTzJs3D9evX0fDhg2Rnp6ON998EwcPHsT58+fRrVs3vPXWW4iOjn7pa3z55Zfo27cvLl26hDfffBPBwcFITk5+rc+3ouEcIgMyNzPBmI4+mLXzGpYduo33/T0hMWVGJSIqC5m5+fCbsd8gr31tdhAsJdr9Cra1tYVEIoGlpSVcXV0BADdu3AAAzJ49G2+88YZ6rIODAxo1aqR+/tVXX2Hbtm3YsWMHxo0bV+xrDBo0CP379wcAfPPNN1iyZAlOnz6Nbt266fzeKir+9jWwfi2qwUUmRaw8C5vPsktERETaa9asmcbz9PR0TJkyBb6+vrCzs4O1tTWuX7/+yg5Rw4YN1V9bWVlBJpMhMTGxVGour9ghMjBll6gmZu64imVht/F+Mw9ITU0MXRYRUaVnYWaCa7ODDPba+vDi1WJTpkxBaGgovvvuO9SsWRMWFhZ47733kJOT89LjvLjkhUgkMroFcBmIyoEPmntixeE7iJNnYfPZh/ioVXVDl0REVOmJRCKtT1sZmkQi0WppiuPHj2PQoEHo3bs3AGXH6N69e6VcXeXAU2blgLmZCcZ08gEALA+7jey811uPhYiIKhcvLy+cOnUK9+7dw6NHj4rt3tSqVQtbt27FhQsXcPHiRXz44YdG1+kpKQaicuKD5p5wlZkru0RnOJeIiIiemzJlCkxMTODn5wcnJ6di5wQtXLgQ9vb2aN26Nd566y0EBQWhadOmZVxtxWTwQBQTE4MBAwbA0dERFhYWaNCgAc6ePaveLwgCZsyYATc3N1hYWCAwMBC3bt3SOEZycjKCg4Mhk8lgZ2eHoUOHIj09XWPMpUuX0K5dO5ibm8PT0xPz588vk/enLampCcY+6xItC7uDrFx2iYiISKl27doIDw9HRkYGBEHAoEGDIAgC7OzsNMZ5eXnh0KFDyMjIQHR0NMaOHYvDhw9r3Hfo3r17mDhxovq5IAjo1auXxnFSUlIwaNCgUns/5ZFBA9GTJ0/Qpk0bmJmZYe/evbh27Rq+//572Nvbq8fMnz8fS5YswcqVK3Hq1ClYWVkhKCgIWVlZ6jHBwcG4evUqQkNDsWvXLhw9ehQjRoxQ709NTUXXrl1RvXp1REREYMGCBZg1axZWrVpVpu/3Vfo294SbrTniU3nFGRERUZkSDGjatGlC27Zti92vUCgEV1dXYcGCBeptKSkpglQqFTZu3CgIgiBcu3ZNACCcOXNGPWbv3r2CSCQSYmJiBEEQhOXLlwv29vZCdna2xmvXqVNHqzrlcrkAQJDL5Tq9v5L4LfyeUH3aLqHF16FCZk5eqb8eEZGxyMzMFK5duyZkZmYauhTSo5f9XHX5/W3QDtGOHTvQrFkzvP/++3B2dkaTJk3w888/q/dHRUUhPj4egYGB6m22trZo2bIlwsPDAQDh4eGws7PTuBdDYGAgxGIxTp06pR7Tvn17SCQS9ZigoCBERkbiyZMnherKzs5GamqqxqOs9G3mAXdbcySkZmPT6ZffN4KIiIj0w6CB6O7du1ixYgVq1aqF/fv3Y/To0ZgwYQJ+/fVXAEB8fDwAwMXFReP7XFxc1Pvi4+Ph7Oyssd/U1BQODg4aY4o6RsHXKGju3LmwtbVVP1S3SC8LUlMTjO1cEwCw/DDnEhEREZUFgwYihUKBpk2b4ptvvkGTJk0wYsQIDB8+HCtXrjRkWfjss88gl8vVjwcPynY+z/v+nqhqZ4HEtGxsZJeIiIio1Bk0ELm5ucHPz09jm6+vr/pyQtWaLQkJCRpjEhIS1PtcXV0L3V48Ly8PycnJGmOKOkbB1yhIKpVCJpNpPMqSxFSMsZ2UXaIV7BIRERGVOoMGojZt2iAyMlJj282bN1G9uvJOzd7e3nB1dcXBgwfV+1NTU3Hq1CkEBAQAAAICApCSkoKIiAj1mEOHDkGhUKBly5bqMUePHkVubq56TGhoKOrUqaNxRVt58p6/h7pLtOEUu0RERESlyaCBaNKkSTh58iS++eYb3L59Gxs2bMCqVaswduxYAMrbqk+cOBFz5szBjh07cPnyZQwcOBDu7u7qeyb4+vqiW7duGD58OE6fPo3jx49j3Lhx6NevH9zd3QEAH374ISQSCYYOHYqrV6/izz//xOLFizF58mRDvfVXkpiKMe7ZXKIVR9glIiIiKk0GDUTNmzfHtm3bsHHjRtSvXx9fffUVfvjhBwQHB6vHTJ06FePHj8eIESPQvHlzpKenY9++fTA3N1ePWb9+PerWrYsuXbrgzTffRNu2bTXuMWRra4t///0XUVFR8Pf3xyeffIIZM2Zo3KuoPOrTVNklSkrLxnp2iYiIiEqNSBAEwdBFlHepqamwtbWFXC4v8/lEm05H49Otl1HFWor/pnaChUQ/KyQTERmbrKwsREVFwdvbW+Mf1cbAy8sLEydOVN+hWiQSYdu2bYXuUK1y7949eHt74/z582jcuHGJX1dfx3mZl/1cdfn9bfClO+jl+vh7wMPeAo/Ss7H+1H1Dl0NERJVAXFwcunfvrtdjDho0qFDA8vT0RFxcHOrXr6/X1yoNDETlnJmJGOOfzSVaeeQuMnM4l4iIiF6Pq6srpFJpqb+OiYkJXF1dYWpqWuqv9boYiCqAd5t6wNOBXSIiImO0atUquLu7Q6FQaGx/5513MGTIENy5cwfvvPMOXFxcYG1tjebNm+PAgQMvPaZIJML27dvVz0+fPo0mTZrA3NwczZo1w/nz5zXG5+fnY+jQofD29oaFhQXq1KmDxYsXq/fPmjULv/76K/755x+IRCKIRCIcPnwY9+7dg0gkwoULF9Rjjxw5ghYtWkAqlcLNzQ2ffvop8vLy1Ps7duyICRMmYOrUqXBwcICrqytmzZql+wenIwaiCsDMRIzxnWoBAFYeuYOMnLxXfAcREb2SIAA5Tw3z0GH67vvvv4/Hjx8jLCxMvS05ORn79u1DcHAw0tPT8eabb+LgwYM4f/48unXrhrfeekt9T79XSU9PR8+ePeHn54eIiAjMmjULU6ZM0RijUCjg4eGBLVu24Nq1a5gxYwb+7//+D5s3bwYATJkyBX379kW3bt0QFxeHuLg4tG7dutBrxcTE4M0330Tz5s1x8eJFrFixAqtXr8acOXM0xv3666+wsrLCqVOnMH/+fMyePRuhoaFaf2YlUf57WAQA6N20KpaG3UZ0cgbWn4zG8PY1DF0SEVHFlpsBfONumNf+v1hAYqXVUHt7e3Tv3h0bNmxAly5dAAB//fUXqlSpgk6dOkEsFqNRo0bq8V999RW2bduGHTt2YNy4ca88/oYNG6BQKLB69WqYm5ujXr16ePjwIUaPHq0eY2Zmhi+//FL93NvbG+Hh4di8eTP69u0La2trWFhYIDs7u8gbHqssX74cnp6eWLp0KUQiEerWrYvY2FhMmzYNM2bMgFis7NM0bNgQM2fOBADUqlULS5cuxcGDB/HGG29o9ZmVBDtEFYSZyfP7ErFLRERkXIKDg/H3338jOzsbgPJ2M/369YNYLEZ6ejqmTJkCX19f2NnZwdraGtevX9e6Q3T9+nU0bNhQ4wot1c2PC1q2bBn8/f3h5OQEa2trrFq1SuvXKPhaAQEBEIlE6m1t2rRBeno6Hj58qN7WsGFDje9zc3MrtCqFvrFDVIG826QqloXdxv3HGfg9/D5GdvAxdElERBWXmaWyU2Oo19bBW2+9BUEQsHv3bjRv3hz//fcfFi1aBEB5uio0NBTfffcdatasCQsLC7z33nvIycnRW7mbNm3ClClT8P333yMgIAA2NjZYsGABTp06pbfXKMjMzEzjuUgkKjSHSt8YiCoQUxMxxnWqif/9dQk/Hb2LjwKqw1LCHyERUYmIRFqftjI0c3NzvPvuu1i/fj1u376NOnXqoGnTpgCA48ePY9CgQejduzcA5Zyge/fuaX1sX19f/P7778jKylJ3iU6ePKkx5vjx42jdujXGjBmj3nbnzh2NMRKJBPn5L78S2tfXF3///TcEQVB3iY4fPw4bGxt4eHhoXXNp4CmzCqZ3k6rwcrRE8tMc/BbOK86IiIxFcHAwdu/ejTVr1mis6FCrVi1s3boVFy5cwMWLF/Hhhx/q1E358MMPIRKJMHz4cFy7dg179uzBd999pzGmVq1aOHv2LPbv34+bN2/iiy++wJkzZzTGeHl54dKlS4iMjMSjR4801g9VGTNmDB48eIDx48fjxo0b+OeffzBz5kxMnjxZPX/IUBiIKhhTEzHGd1Zecbbq6F08zeZcIiIiY9C5c2c4ODggMjISH374oXr7woULYW9vj9atW+Ott95CUFCQunukDWtra+zcuROXL19GkyZN8Pnnn+Pbb7/VGDNy5Ei8++67+OCDD9CyZUs8fvxYo1sEAMOHD0edOnXQrFkzODk54fjx44Veq2rVqtizZw9Onz6NRo0aYdSoURg6dCimT5+u46ehf1y6QwuGXLqjKHn5Cryx6CiiHj3FtG51Mboj5xIREb2KMS/dUZlx6Q4jZlrg7tWrjt5hl4iIiOg1MRBVUG83cod3FSs8ycjFr+H3DF0OERFRhcZAVEGZmogxoYuqS3QX6ewSERERlRgDUQX2VkN31KhihZSMXPx64p6hyyEiIqqwGIgqMGWXSHnF2c//3UVaVuFLHImISBOvJapc9PXzZCCq4N5q5I4aTsouEe9LRERUPBMTEwDQ6x2cyfBUP0/Vz7ekeJvjCs5ELMLHXWrh400XsOroXQwMqA4bc7NXfyMRkZExNTWFpaUlkpKSYGZmZvAbAdLrUygUSEpKgqWlJUxNXy/SMBBVAj0bumPJwVu4k/QUv564h3HPbtxIRETPiUQiuLm5ISoqCvfvs6NeWYjFYlSrVk1jwdiSYCCqBEzEIkx41iX6+b8oDGztBRm7REREhUgkEtSqVYunzSoRiUSil24fA1El0bOhO348dBu3E9Ox7vg99WRrIiLSJBaLeadqKoQnUCsJVZcIAH757y5SecUZERGR1hiIKpEeDdxQy9kaqVl5WHvsnqHLISIiqjAYiCoRE7EIHwcqu0Srj92FPJNdIiIiIm0wEFUyb9Z3Q22XZ12i41GGLoeIiKhCYCCqZMRiET7uUhsAsPpYFLtEREREWmAgqoS613dFHRcbpGXlYc0xdomIiIhehYGoEhIXmEu05lgU5BnsEhEREb0MA1El1a2eK+q62iAtOw+rOZeIiIjopRiIKinxszXOAGAtu0REREQvxUBUiQUV7BIdu2vocoiIiMotBqJKTCwWYaJqLtHxe0jJ4No9RERERWEgquS6+rnC102G9Ow8rOYVZ0REREViIKrkNOYSsUtERERUJAYiIxBUzwV+z7pEv/zHLhEREdGLGIiMgEj0/L5Ea49H4clTdomIiIgKYiAyEl39lF2ipzn5+Pk/XnFGRERUEAORkRCJnl9x9uuJe0hml4iIiEiNgciIvOHngvpV2SUiIiJ6EQORERGJRJjYpTYAdomIiIgKYiAyMl18ndGgqi0ycvKx6ii7RERERAADkdEpOJfot/B7eJyebeCKiIiIDI+ByAh1ruuMhh7PukScS0RERMRAZIw0ukQn7uMRu0RERGTkGIiMVKc6zmjkYYvM3Hz8zLlERERk5BiIjJSyS6S84uy3cHaJiIjIuDEQGbGOdZzQyNMOmbm84oyIiIwbA5ERe/GKs6Q0domIiMg4MRAZuY61ndDY0w5ZuQqsOnrH0OUQEREZBAORkSvYJfr95H0kpmUZuCIiIqKyx0BE6FDbCU2qKbtEPx3hXCIiIjI+DESkccXZH+wSERGREWIgIgBA+1pV0LSaHbLzFFh5mF0iIiIyLgxEBEDZJZr0hrJLtP7UfSSmsktERETGg4GI1NrWrAL/6vbIzlNgxRFecUZERMaDgYjURCIRJj2bS7ThVDS7REREZDQMGohmzZoFkUik8ahbt656f1ZWFsaOHQtHR0dYW1ujT58+SEhI0DhGdHQ0evToAUtLSzg7O+N///sf8vLyNMYcPnwYTZs2hVQqRc2aNbFu3bqyeHsVUpuajmj2rEu0/DC7REREZBwM3iGqV68e4uLi1I9jx46p902aNAk7d+7Eli1bcOTIEcTGxuLdd99V78/Pz0ePHj2Qk5ODEydO4Ndff8W6deswY8YM9ZioqCj06NEDnTp1woULFzBx4kQMGzYM+/fvL9P3WVEUnEu04XQ04uXsEhERUeUnEgRBMNSLz5o1C9u3b8eFCxcK7ZPL5XBycsKGDRvw3nvvAQBu3LgBX19fhIeHo1WrVti7dy969uyJ2NhYuLi4AABWrlyJadOmISkpCRKJBNOmTcPu3btx5coV9bH79euHlJQU7Nu3T6s6U1NTYWtrC7lcDplM9vpvvJwTBAF9fwrHmXtPMKi1F2a9Xc/QJREREelMl9/fBu8Q3bp1C+7u7qhRowaCg4MRHR0NAIiIiEBubi4CAwPVY+vWrYtq1aohPDwcABAeHo4GDRqowxAABAUFITU1FVevXlWPKXgM1RjVMYqSnZ2N1NRUjYcx0ZhLxC4REREZAYMGopYtW2LdunXYt28fVqxYgaioKLRr1w5paWmIj4+HRCKBnZ2dxve4uLggPj4eABAfH68RhlT7VfteNiY1NRWZmZlF1jV37lzY2tqqH56envp4uxVKgI8jWng5ICdPgRWHbxu6HCIiolJl0EDUvXt3vP/++2jYsCGCgoKwZ88epKSkYPPmzYYsC5999hnkcrn68eDBA4PWYwgikQgT31Cucbbx9APEyYsOj0RERJWBwU+ZFWRnZ4fatWvj9u3bcHV1RU5ODlJSUjTGJCQkwNXVFQDg6upa6Koz1fNXjZHJZLCwsCiyDqlUCplMpvEwRq19qqCltwNy8hVYwSvOiIioEitXgSg9PR137tyBm5sb/P39YWZmhoMHD6r3R0ZGIjo6GgEBAQCAgIAAXL58GYmJieoxoaGhkMlk8PPzU48peAzVGNUx6OVUa5xtOv0AsSnsEhERUeVk0EA0ZcoUHDlyBPfu3cOJEyfQu3dvmJiYoH///rC1tcXQoUMxefJkhIWFISIiAoMHD0ZAQABatWoFAOjatSv8/Pzw0Ucf4eLFi9i/fz+mT5+OsWPHQiqVAgBGjRqFu3fvYurUqbhx4waWL1+OzZs3Y9KkSYZ86xVGgI8jWtVQdomWcy4RERFVUgYNRA8fPkT//v1Rp04d9O3bF46Ojjh58iScnJwAAIsWLULPnj3Rp08ftG/fHq6urti6dav6+01MTLBr1y6YmJggICAAAwYMwMCBAzF79mz1GG9vb+zevRuhoaFo1KgRvv/+e/zyyy8ICgoq8/dbUam6RH+eYZeIiIgqJ4Peh6iiMLb7EBWl/6qTCL/7GMEtq+Hr3g0MXQ4REdErVaj7EFHFMDFQecXZ5rMPEMMuERERVTIMRKSVljUc0drHEbn5ApaFcS4RERFVLiUKRLm5uXjw4AEiIyORnJys75qonFLNJdpy9gEePskwcDVERET6o3UgSktLw4oVK9ChQwfIZDJ4eXnB19cXTk5OqF69OoYPH44zZ86UZq1kYC28HdCmpqpLxPsSERFR5aFVIFq4cCG8vLywdu1aBAYGqhdkvXnzJsLDwzFz5kzk5eWha9eu6NatG27dulXadZOBFOwSPUhml4iIiCoHra4y69+/P6ZPn4569V6+6nl2djbWrl0LiUSCIUOG6K1IQ+NVZpoG/HIKx24/Qv8Wnpj7bkNDl0NERFQkXX5/87J7LTAQaTp7LxnvrQyHqViEsCkd4elgaeiSiIiICuFl91Sqmnk5oF2tKshT8IozIiKqHHQKRImJiXj48KH6eV5eHqZPn44OHTrgk08+QUYG55QYC9Vcor8iHnIuERERVXg6BaLhw4fj119/VT9fsGABfv75ZzRv3hw7duzg+mBGxL+6PdrXdkKeQsDSQ+wSERFRxaZTILp06RI6deqkfv77779jyZIl+O6777Bp0ybs3LlT7wVS+aW6e/Vf5x4i+jG7REREVHGZajNo8ODBAIDY2FgsXLgQP//8M3JychAZGYlt27Zh//79UCgUSExMVF9dtmbNmtKrmsqFptXs0aG2E47cTMKPh25hwfuNDF0SERFRieh0lZmPjw+WLl2K7t27488//8TXX3+NS5cuAQAeP36MWrVqVco7V/Mqs+Kdj36C3stPwEQswqFPOqC6o5WhSyIiIgJQileZ9ejRA0OGDMHIkSMxYcIEhISEqPedPn0afn5+JauYKqwm1ezRsY4T8hUCfuRcIiIiqqB0CkTz58/HsGHDEBsbi0mTJmlMoj516hRGjRql9wKp/FNdcbbtfAzuPXpq4GqIiIh0xxszaoGnzF5t8NrTCItMQp+mHvi+L+cSERGR4fHGjFTmVF2i7RfYJSIioopHq0DUrVs3nDx58pXj0tLS8O2332LZsmWvXRhVLI087dC5rjPyFQKWHOLivkREVLFoddn9+++/jz59+sDW1hZvvfUWmjVrBnd3d5ibm+PJkye4du0ajh07hj179qBHjx5YsGBBaddN5dDEwFo4dCMR28/HYHznWvCuwivOiIioYtB6DlF2dja2bNmCP//8E8eOHYNcLlceQCSCn58fgoKCMHToUPj6+pZqwYbAOUTaG7ruDA7eSMS7Tapi4QeNDV0OEREZsTJZ7V4ulyMzMxOOjo4wMzMrUaEVBQOR9i4/lOOtpccgFgGhkzvAx8na0CUREZGRKpNJ1ba2tnB1da30YYh008DDFoG+zlAI4BpnRERUYfAqM9I71RVn/1yIwZ2kdANXQ0RE9GoMRKR39avaItDXBQoB+PEgrzgjIqLyj4GISsXEwFoAgB0XY3E7kV0iIiIq33QKRPn5+Th69ChSUlJKqRyqLOpXtUVXv2ddIt6XiIiIyjmdApGJiQm6du2KJ0+elFY9VIl8rNElSjNwNURERMXT+ZRZ/fr1cffu3dKohSqZeu62CKrnAkEAFh/kFWdERFR+6RyI5syZgylTpmDXrl2Ii4tDamqqxoOooI+7KK8423UpFrcS2CUiIqLySecbM4rFzzOUSCRSfy0IAkQiEfLz8/VXXTnBGzO+nlG/R2Df1Xj0bOiGpR82NXQ5RERkJHT5/a3VWmYFhYWFlbgwMk4fB9bCvqvx2H05DhMS0lDbxcbQJREREWnQORB16NChNOqgSszXTYbu9V2x90o8Fh+8hWXsEhERUTmjcyACgJSUFKxevRrXr18HANSrVw9DhgyBra2tXoujyuPjwFrYeyUeey7HITI+DXVc2SUiIqLyQ+dJ1WfPnoWPjw8WLVqE5ORkJCcnY+HChfDx8cG5c+dKo0aqBOq6yvBmA1cIArCEd68mIqJyRudJ1e3atUPNmjXx888/w9RU2WDKy8vDsGHDcPfuXRw9erRUCjUkTqrWj8j4NAT9oPzzsW9iO9R15WdJRESlp1RXuz979iymTZumDkMAYGpqiqlTp+Ls2bO6V0tGo46rDXo0cAPALhEREZUvOgcimUyG6OjoQtsfPHgAGxvOC6GXm9ClFkQiYM/leFyP432riIiofNA5EH3wwQcYOnQo/vzzTzx48AAPHjzApk2bMGzYMPTv3780aqRKpI6rDd5kl4iIiMoZna8y++677yASiTBw4EDk5eUBAMzMzDB69GjMmzdP7wVS5TOxSy3suRyHvVficS02FX7unEtERESGpdOk6vz8fBw/fhwNGjSAVCrFnTt3AAA+Pj6wtLQstSINjZOq9W/8xvPYeTEW3eq5YuVH/oYuh4iIKqFSm1StWu0+JSUFlpaWaNCgARo0aFCpwxCVjgmda0IkAvZdjcfVWLmhyyEiIiPH1e7JIGq52OCthu4AgMUHOJeIiIgMi6vdk8FM6KLsEv17LQFXYtglIiIiw+Fq91rgHKLS8/Gm8/jnQize8HPBzwObGbocIiKqRLjaPVUY4zvXws6LsQh91iWqX5Xr4RERUdnTKRDl5uZi9uzZWLlyJWrVqlVaNZERqelsjbcbuWP7hVj8cOAWfglhl4iIiMqeTnOIzMzMcOnSpdKqhYzU+C61IBYBB65zLhERERmGzpOqBwwYgNWrV5dGLWSkfJys8U7jqgCAHw7cNHA1RERkjHSeQ5SXl4c1a9bgwIED8Pf3h5WVlcb+hQsX6q04Mh7jO9fEPxdicOB6Ii49TEFDDztDl0REREZE50B05coVNG3aFABw86bmv+YLXnVGpIsaTtbo1bgqtp6PweIDt7B6UHNDl0REREaEV5lRuTG+Sy1svxCDgzcScfFBChp52hm6JCIiMhI6zyF6mcTERH0ejoyMdxUr9GqinEu0+CDvXk1ERGVH60BkaWmJpKQk9fMePXogLi5O/TwhIQFubm76rY6MzoTOtWAiFuHQjURceJBi6HKIiMhIaB2IsrKyUPCm1kePHkVmZqbGGB1vek1UiFcVK/R6dsXZYl5xRkREZUSvp8w4qZr0YUKXmjARixAWmYTz0U8MXQ4RERkBvQYiIn2o7miFd5uo7kvEuURERFT6tA5EIpFIowP04vPXNW/ePIhEIkycOFG9LSsrC2PHjoWjoyOsra3Rp08fJCQkaHxfdHQ0evToAUtLSzg7O+N///sf8vLyNMYcPnwYTZs2hVQqRc2aNbFu3Tq91U2lY1xnZZfoyM0knGOXiIiISpnWgUgQBNSuXRsODg5wcHBAeno6mjRpon5et27dEhdx5swZ/PTTT2jYsKHG9kmTJmHnzp3YsmULjhw5gtjYWLz77rvq/fn5+ejRowdycnJw4sQJ/Prrr1i3bh1mzJihHhMVFYUePXqgU6dOuHDhAiZOnIhhw4Zh//79Ja6XSl91Ryv0acouERERlQ2RoOVM6F9//VWrA4aEhOhUQHp6Opo2bYrly5djzpw5aNy4MX744QfI5XI4OTlhw4YNeO+99wAAN27cgK+vL8LDw9GqVSvs3bsXPXv2RGxsLFxcXAAAK1euxLRp05CUlASJRIJp06Zh9+7duHLlivo1+/Xrh5SUFOzbt0+rGlNTU2Frawu5XA6ZTKbT+6OSi36cgc7fH0aeQsDfo1vDv7q9oUsiIqIKRJff31rfmFHXoKOtsWPHokePHggMDMScOXPU2yMiIpCbm4vAwED1trp166JatWrqQBQeHo4GDRqowxAABAUFYfTo0bh69SqaNGmC8PBwjWOoxhQ8Nfei7OxsZGdnq5+npqbq4Z2Srqo5WqJPUw/8efYBfjhwE78PbWnokoiIqJIy6KTqTZs24dy5c5g7d26hffHx8ZBIJLCzs9PY7uLigvj4ePWYgmFItV+172VjUlNTC902QGXu3LmwtbVVPzw9PUv0/uj1jetcE6ZiEf679QgR95MNXQ4REVVSBgtEDx48wMcff4z169fD3NzcUGUU6bPPPoNcLlc/Hjx4YOiSjJangyXe8/cAwLlERERUegwWiCIiIpCYmIimTZvC1NQUpqamOHLkCJYsWQJTU1O4uLggJycHKSkpGt+XkJAAV1dXAICrq2uhq85Uz181RiaTwcLCosjapFIpZDKZxoMMZ2yn512is/fYJSIiIv0zWCDq0qULLl++jAsXLqgfzZo1Q3BwsPprMzMzHDx4UP09kZGRiI6ORkBAAAAgICAAly9f1lhDLTQ0FDKZDH5+fuoxBY+hGqM6BpV/ng6WeL8Zu0RERFR6dF7tXiUnJwdRUVHw8fGBqanuh7GxsUH9+vU1tllZWcHR0VG9fejQoZg8eTIcHBwgk8kwfvx4BAQEoFWrVgCArl27ws/PDx999BHmz5+P+Ph4TJ8+HWPHjoVUKgUAjBo1CkuXLsXUqVMxZMgQHDp0CJs3b8bu3btL+tbJAMZ2qoktZx/i2O1HOHMvGc29HAxdEhERVSI6d4gyMjIwdOhQWFpaol69eoiOjgYAjB8/HvPmzdNrcYsWLULPnj3Rp08ftG/fHq6urti6dat6v4mJCXbt2gUTExMEBARgwIABGDhwIGbPnq0e4+3tjd27dyM0NBSNGjXC999/j19++QVBQUF6rZVKl4e9Jd5vppzc/gPXOCMiIj3T+j5EKh9//DGOHz+OH374Ad26dcOlS5dQo0YN/PPPP5g1axbOnz9fWrUaDO9DVD48fJKBTt8dRm6+gM0jA9DCm10iIiIqni6/v3XuEG3fvh1Lly5F27ZtNZbuqFevHu7cuaN7tURaYpeIiIhKi86BKCkpCc7OzoW2P336lKvdU6kb26kmzExEOHHnMU7dfWzocoiIqJLQORA1a9ZMY0KyKgT98ssvvHKLSl1VOwt80FzZJVrELhEREemJzpeHffPNN+jevTuuXbuGvLw8LF68GNeuXcOJEydw5MiR0qiRSMOYjjWx+cxDnLybjPA7jxHg42jokoiIqILTuUPUtm1bXLhwAXl5eWjQoAH+/fdfODs7Izw8HP7+/qVRI5EG9wJdIs4lIiIifdD5KjNjxKvMyp84eSY6zD+MnHwFNg5vxS4REREVUqpXmZmYmGjcGVrl8ePHMDEx0fVwRCXiZmuBfi2ezyViriciotehcyAq7hdPdnY2JBLJaxdEpK3RHX0gMRHjdFQywnnFGRERvQatJ1UvWbIEgPKqsl9++QXW1tbqffn5+Th69Cjq1q2r/wqJiuFma4H+LTzxa/h9/BB6CwE1HHnrByIiKhGtA9GiRYsAKDtEK1eu1Dg9JpFI4OXlhZUrV+q/QqKXGN2xJjaeeYDT95RXnLWuWcXQJRERUQWkdSCKiooCAHTq1Albt26Fvb19qRVFpC1XW3N82KIa1p24h0UHbiLAh10iIiLSnc5ziMLCwhiGqFwZ3dEHElMxztx7guO3OZeIiIh0p/ONGYcMGfLS/WvWrClxMUQl4SJ73iX64cBNtKnJLhEREelG50D05MkTjee5ubm4cuUKUlJS0LlzZ70VRqSLMR19sPF0NM7ef4Jjtx+hXS0nQ5dEREQViM6BaNu2bYW2KRQKjB49Gj4+PnopikhXzjJzfNiyGtYev4cfDtxC25pV2CUiIiKt6TyHqMiDiMWYPHmy+ko0IkMY3cEHUlMxIu4/wX+3Hhm6HCIiqkD0EogA4M6dO8jLy9PX4Yh05iwzR3DL6gCUa5zx7tVERKQtnU+ZTZ48WeO5IAiIi4vD7t27ERISorfCiEpiVMca2HD6Ps5Fp+DorUfoUJtziYiI6NV0DkTnz5/XeC4Wi+Hk5ITvv//+lVegEZU2ZxtzDGhZHb8ci8Ki0JtoX4tziYiI6NV0DkRhYWGlUQeR3ozs4IM/Tt3HhQcpOHIzCR3rOBu6JCIiKuf0NoeIqLxwspHio1bKuUSLDtziXCIiInolrTpETZo00fq0w7lz516rICJ9GNHeB7+fvI+LD1Jw+GYSOrFLREREL6FVIOrVq1cpl0GkX042UgwM8MKqo3fxQ+hNdKztxLlERERULJHA8wmvlJqaCltbW8jlcshkMkOXQ1p6lJ6Ndt+GITM3H2sHNUenuuwSEREZE11+f5d4DlFERAT++OMP/PHHH4WuPCMqD6pYSzEwQDWXiPclIiKi4ul8lVliYiL69euHw4cPw87ODgCQkpKCTp06YdOmTXBy4n1fqPwY0b4Gfgu/j0sP5Th0IxFdfF0MXRIREZVDOneIxo8fj7S0NFy9ehXJyclITk7GlStXkJqaigkTJpRGjUQl5mgtxcDWqrtX84ozIiIqms6BaN++fVi+fDl8fX3V2/z8/LBs2TLs3btXr8UR6cOIdjVgKTHB5Rg5Dl5PNHQ5RERUDukciBQKBczMzAptNzMzg0Kh0EtRRPrkaK284gwAfjjIuURERFSYzoGoc+fO+PjjjxEbG6veFhMTg0mTJqFLly56LY5IX0a0V3aJrsSk4gC7RERE9AKdA9HSpUuRmpoKLy8v+Pj4wMfHB97e3khNTcWPP/5YGjUSvTYHKwlCWnsBAH7gFWdERPQCna8y8/T0xLlz53DgwAHcuHEDAODr64vAwEC9F0ekTyPa1cBvJ+7hamwqQq8loGs9V0OXRERE5YRebsyYkpKivgS/MuKNGSuPBftvYFnYHfi5ybB7QlvevZqIqBIr1Rszfvvtt/jzzz/Vz/v27QtHR0dUrVoVFy9e1L1aojI0rG0NWEtNcS0uFf9eSzB0OUREVE7oHIhWrlwJT09PAEBoaChCQ0Oxd+9edO/eHf/73//0XiCRPtlbSTBIPZfoFhQKziUiIqISBKL4+Hh1INq1axf69u2Lrl27YurUqThz5ozeCyTSt2HtvGEtNcV1domIiOgZnQORvb09Hjx4AEB5k0bVZGpBEJCfn6/f6ohKgZ2lBIPbeAFQXnHGLhEREekciN599118+OGHeOONN/D48WN0794dAHD+/HnUrFlT7wUSlYahbb1hIzXFjfg0/Hst3tDlEBGRgekciBYtWoRx48bBz88PoaGhsLa2BgDExcVhzJgxei+QqDRodok4l4iIyNjp5bL7yo6X3VdO8oxctP32ENKy87A8uCnebOBm6JKIiEiPSvWyewCIjIzEuHHj0KVLF3Tp0gXjxo1DZGRkiYolMhRbSzMMbusNAFjMLhERkVHTORD9/fffqF+/PiIiItCoUSM0atQI586dQ/369fH333+XRo1EpWZoW2/YmJsiMiENe69wLhERkbHS+ZSZj48PgoODMXv2bI3tM2fOxB9//IE7d+7otcDygKfMKrdFoTex+OAt1Haxxr6P20Ms5t2riYgqg1I9ZRYXF4eBAwcW2j5gwADExcXpejgigxvyrEt0MyEde67wzzARkTHSORB17NgR//33X6Htx44dQ7t27fRSFFFZsrUww1DOJSIiMmparXa/Y8cO9ddvv/02pk2bhoiICLRq1QoAcPLkSWzZsgVffvll6VRJVMoGt/HGmmNRuJWYjt2X4/BWI3dDl0RERGVIqzlEYrF2jSSRSFQp71bNOUTGYfGBW1h04CZqOltj/8T2MOFcIiKiCk3vc4gUCoVWj8oYhsh4DG7rBZm5KW4/6xIREZHxKNF9iIqSkpKCpUuX6utwRGVOZm6G4e1qAAAWH7iJfM4lIiIyGq8diA4ePIgPP/wQbm5umDlzpj5qIjKYQW28YGthhjtJT7HrUqyhyyEiojJSokD04MEDzJ49G97e3ujatStEIhG2bduG+Hje2I4qNhtzMwxvp7zibMnBW+wSEREZCa0DUW5uLrZs2YKgoCDUqVMHFy5cwIIFCyAWi/H555+jW7duMDMzK81aicpESGsv2FmyS0REZEy0DkRVq1bFjz/+iD59+iAmJgZbt27Fe++9V5q1ERmETcG5ROwSEREZBa0DUV5eHkQiEUQiEUxMTEqzJiKDGxhQHXaWZrib9BQ7LsYYuhwiIiplWgei2NhYjBgxAhs3boSrqyv69OmDbdu2QSTivVqo8inYJfrx4G3k5SsMXBEREZUmrQORubk5goODcejQIVy+fBm+vr6YMGEC8vLy8PXXXyM0NJT3IaJKJaS1F+wtzXD30VPsuMi5RERElVmJrjLz8fHBnDlzcP/+fezevRvZ2dno2bMnXFxcdDrOihUr0LBhQ8hkMshkMgQEBGDv3r3q/VlZWRg7diwcHR1hbW2NPn36ICEhQeMY0dHR6NGjBywtLeHs7Iz//e9/yMvL0xhz+PBhNG3aFFKpFDVr1sS6detK8rbJyFhLTTG8/bMu0SF2iYiIKrPXug+RWCxG9+7d8ddff+Hhw4f4v//7P52+38PDA/PmzUNERATOnj2Lzp0745133sHVq1cBAJMmTcLOnTuxZcsWHDlyBLGxsXj33XfV35+fn48ePXogJycHJ06cwK+//op169ZhxowZ6jFRUVHo0aMHOnXqhAsXLmDixIkYNmwY9u/f/zpvnYxESICySxT16Cn+ucAuERFRZaXVWmZlycHBAQsWLMB7770HJycnbNiwQX01240bN+Dr64vw8HC0atUKe/fuRc+ePREbG6vuTq1cuRLTpk1DUlISJBIJpk2bht27d+PKlSvq1+jXrx9SUlKwb98+rWriWmbGbcXhO/h23w14OVriwOQOMDXR2w3eiYioFOl9LbOykJ+fj02bNuHp06cICAhAREQEcnNzERgYqB5Tt25dVKtWDeHh4QCA8PBwNGjQQONUXVBQEFJTU9VdpvDwcI1jqMaojlGU7OxspKamajzIeA0MqA4HKwnuPc7AdnaJiIgqJYMHosuXL8Pa2hpSqRSjRo3Ctm3b4Ofnh/j4eEgkEtjZ2WmMd3FxUd8ROz4+vtC8JdXzV41JTU1FZmZmkTXNnTsXtra26oenp6c+3ipVUFZSU4xQzyW6xblERESVkMEDkequ16dOncLo0aMREhKCa9euGbSmzz77DHK5XP148OCBQeshw1N1ie4/zsC287wvERFRZWPwQCSRSFCzZk34+/tj7ty5aNSoERYvXgxXV1fk5OQgJSVFY3xCQgJcXV0BAK6uroWuOlM9f9UYmUwGCwuLImuSSqXqK99UDzJulhJTjCxwxVkuu0RERJWKzoEoPz8fq1evxocffojAwEB07txZ4/G6FAoFsrOz4e/vDzMzMxw8eFC9LzIyEtHR0QgICAAABAQE4PLly0hMTFSPCQ0NhUwmg5+fn3pMwWOoxqiOQaStjwKqo4q1BNHJ7BIREVU2prp+w8cff4x169ahR48eqF+//mvdqfqzzz5D9+7dUa1aNaSlpWHDhg04fPgw9u/fD1tbWwwdOhSTJ0+Gg4MDZDIZxo8fj4CAALRq1QoA0LVrV/j5+eGjjz7C/PnzER8fj+nTp2Ps2LGQSqUAgFGjRmHp0qWYOnUqhgwZgkOHDmHz5s3YvXt3iesm46TsEvng6z3X8eOhW+jdpCrMeMUZEVHlIOjI0dFR2L17t67fVqQhQ4YI1atXFyQSieDk5CR06dJF+Pfff9X7MzMzhTFjxgj29vaCpaWl0Lt3byEuLk7jGPfu3RO6d+8uWFhYCFWqVBE++eQTITc3V2NMWFiY0LhxY0EikQg1atQQ1q5dq1OdcrlcACDI5fISv1eqHJ5m5wr+X/0rVJ+2S/jzdLShyyEiopfQ5fe3zvchcnd3x+HDh1G7du3SSWjlEO9DRAX98t9dzNl9HZ4OFjj0SUd2iYiIyqlSvQ/RJ598gsWLF0PHHEVUaQS3rI4q1lI8SM7E1nMPDV0OERHpgc5ziI4dO4awsDDs3bsX9erVg5mZmcb+rVu36q04ovLIQmKCUR1qYM7u6/jx0G30buIBiSm7REREFZnOgcjOzg69e/cujVqIKozgltWx8shdPHySib/PPUT/FtUMXRIREb2GcreWWXnEOURUlNXHovDVrmuoameBsCkd2SUiIipnKuRaZkQVTXDLanCykSImJRN/RXAuERFRRabzKTMA+Ouvv7B582ZER0cjJydHY9+5c+f0UhhReWduZoLRHXwwe9c1LAu7jff8OZeIiKii0vlv7yVLlmDw4MFwcXHB+fPn0aJFCzg6OuLu3bvo3r17adRIVG592LIanJ91ibZEcM07IqKKSudAtHz5cqxatQo//vgjJBIJpk6ditDQUEyYMAFyubw0aiQqt8zNTDC6ow8AYNmh28jJ4xpnREQVkc6BKDo6Gq1btwYAWFhYIC0tDQDw0UcfYePGjfqtjqgC6N9C2SWKlWdh81l2iYiIKiKdA5GrqyuSk5MBANWqVcPJkycBAFFRUbxZIxklczMTjFF1icJuIzsv38AVERGRrnQORJ07d8aOHTsAAIMHD8akSZPwxhtv4IMPPuD9icho9WtRDa4yc8TJs7D5LK84IyKqaHS+D5FCoYBCoYCpqfICtU2bNuHEiROoVasWRo4cCYlEUiqFGhLvQ0Ta+C38Hmb8cxVutuY4/L+OkJqaGLokIiKjpsvvb96YUQsMRKSNrNx8dFxwGPGpWfjqnXr4KMDL0CURERm1Ur8x43///YcBAwYgICAAMTExAIDff/8dx44dK8nhiCoFczMTjO2kmkt0B1m5nEtERFRR6ByI/v77bwQFBcHCwgLnz59HdnY2AEAul+Obb77Re4FEFUnf5p5wszVHfCqvOCMiqkh0DkRz5szBypUr8fPPP2usdN+mTRvepZqMntTUBGM61QSgvOKMXSIioopB50AUGRmJ9u3bF9pua2uLlJQUfdREVKH1beYBd1tzJKRmY9PpaEOXQ0REWijRfYhu375daPuxY8dQo0YNvRRFVJEV7BItP8y5REREFYHOgWj48OH4+OOPcerUKYhEIsTGxmL9+vWYMmUKRo8eXRo1ElU4fZt5wt3WHIlp2djILhERUbmn82r3n376KRQKBbp06YKMjAy0b98eUqkUU6ZMwfjx40ujRqIKR2IqxtjONfH5titYcfgO+reoBnMz3peIiKi8KvF9iHJycnD79m2kp6fDz88P1tbW+q6t3OB9iKgkcvIU6PTdYcSkZGJGTz8Maett6JKIiIxKqd+HCAAkEgn8/PzQokWLSh2GiEpKYirG2GdziVYc4VwiIqLyTOtTZkOGDNFq3Jo1a0pcDFFl856/B5aF3UZMSibWn4rGUHaJiIjKJa07ROvWrUNYWBhSUlLw5MmTYh9E9JzEVIxxnZ91iQ7fQWYOu0REROWR1h2i0aNHY+PGjYiKisLgwYMxYMAAODg4lGZtRJVCn6bKLtHDJ5lYf+o+hrXj7SmIiMobrTtEy5YtQ1xcHKZOnYqdO3fC09MTffv2xf79+8H1YYmKJzEVY9yzuUQrj9xll4iIqBzSaVK1VCpF//79ERoaimvXrqFevXoYM2YMvLy8kJ6eXlo1ElV4ffw94OlggUfp2Vh/6r6hyyEioheU+CozsVgMkUgEQRCQn89/8RK9jJmJGOM71QIArDxyBxk5eQauiIiICtIpEGVnZ2Pjxo144403ULt2bVy+fBlLly5FdHQ0L70neoXeTauimoMlHqXnYP1J3r2aiKg80ToQjRkzBm5ubpg3bx569uyJBw8eYMuWLXjzzTchFpe40URkNMxMnl9xxi4REVH5ovWdqsViMapVq4YmTZpAJBIVO27r1q16K6684J2qSV9y8xXo8v0RRCdn4LPudTGyg4+hSyIiqrR0+f2t9WX3AwcOfGkQIqJXMzMRY3znmvjfX5fw09G7+CigOiwlOi8pSEREeqb138Tr1q0rxTKIjEfvJlWxNOw27j/OwG/h9zGKXSIiIoPj5B+iMmZqIsb4zsorzlYdvYun2ZxLRERkaAxERAbQq7E7vBwtkfw0B7+F875ERESGxkBEZACaXaI77BIRERkYAxGRgbzT2B3eVazwJCMXv4bfM3Q5RERGjYGIyEBMn11xBijnEqWzS0REZDAMREQG9HYjd9SoYoWUjFz8euKeocshIjJaDEREBmRqIsb4Lsou0c//3UVaVq6BKyIiMk4MREQG9najqqjhpOwS8YozIiLDYCAiMjATsQgfd3l+XyJ2iYiIyh4DEVE50LOhO3ycrCDP5FwiIiJDYCAiKgdMxCJMeNYl+vm/KKSyS0REVKa4qqQh5eUAa7sBMndAVrXAf599beMGmEoMXSWVkZ4N3fHjodu4nZiOdcfvqQMSERGVPgYiQ0qLA2IilI/iWDkrw5Gtx7PA9GJ4cgdMpWVXM5UaVZdowsbz+OW/uxjUxgsyczNDl0VEZBQYiAzJ0hHotwGQxwCpMUBq7LPHs6/zs4GnicpH3IWXHKfK84BkW7Xo0GRmUWZvi0quRwM3LDl4C7cT07H22D18HMguERFRWRAJgiAYuojyLjU1Fba2tpDL5ZDJZGXzooIAZDwuEJRingWnWM1teVnaHc/CoUBIKiY8SaxK9z2RVnZejMX4jechMzfFf9M6w9aCXSIiopLQ5fc3O0TllUgEWFVRPtwaFT1GEIDMJ5oBSdVlkj98vi03A8hMVj4SLhf/mua2gKyoU3MFTtlJbUrn/ZKaqkt0KzEda49HYWJgbUOXRERU6bFDpAWDdIj0RRCALHmBwPRCeFKdrstJ1+54UtkLgemFLpNtVeUYkah031clt+tSLMZtOA8bc1McY5eIiKhE2CGi50QiwMJO+XDxK35cVmrxoUkVnLLlQHYqkJQKJN0o/lgS62ImgBfoOFnYMzS9xJv13VDb5RZuJqRjzbEoTHqDXSIiotLEDpEWKnSHSJ+y04DUuBcmgD+ExmTwzCfaHcvMsuhTc+pTdlUBSwejDk27L8Vh7IZz7BIREZUQO0RUOqQ2gJMN4PSSbkVORuGJ3y92njIeK+c1Pb6tfBTHRPqSq+eehSdLR0BcOe8v2r2+K+q42CAyIQ2rj0VhMrtERESlhh0iLbBDpGe5mcp7MBW6aq5Ax+lpknbHMpEob2BZcA7Ti1fTWTlX2NC053Icxqw/Bxvpsy6RJbtERETaYoeIyjczC8ChhvJRnLxsZWgq6qo51bb0BCA/B0i5r3wUR2wK2LgXCEkFb3T5LDxZuwBiE/2/19fUrZ4r6rra4EZ8GlYfu4vJXesYuiQiokqJHSItsENUTuXlAOnxRV81pw5N8YCgePWxRCaAjWvhq+YKhidrV8Ck7P8NsfdyHEavPwdrqSmOTesEO0su50JEpI0K0yGaO3cutm7dihs3bsDCwgKtW7fGt99+izp1nv8rOCsrC5988gk2bdqE7OxsBAUFYfny5XBxcVGPiY6OxujRoxEWFgZra2uEhIRg7ty5MDV9/vYOHz6MyZMn4+rVq/D09MT06dMxaNCgsny7pG+mEsCumvJRnPy8wqHpxRtdpsUBQv7zeU7FEYmVnaSirporxfXngjS6RFH4hF0iIiK9M2ggOnLkCMaOHYvmzZsjLy8P//d//4euXbvi2rVrsLJS3jV50qRJ2L17N7Zs2QJbW1uMGzcO7777Lo4fPw4AyM/PR48ePeDq6ooTJ04gLi4OAwcOhJmZGb755hsAQFRUFHr06IFRo0Zh/fr1OHjwIIYNGwY3NzcEBQUZ7P1TGTAxVXZ4bD2KH6PIB9ITX7hq7oVbDqTFAoo8ZXhSrUFXJBFg7fzyWw7ouP6cWCzCxMDaGPVHBNYev4ehbb3ZJSIi0rNydcosKSkJzs7OOHLkCNq3bw+5XA4nJyds2LAB7733HgDgxo0b8PX1RXh4OFq1aoW9e/eiZ8+eiI2NVXeNVq5ciWnTpiEpKQkSiQTTpk3D7t27ceXKFfVr9evXDykpKdi3b98r6+IpM4JCoZzoXdRVcxrrz+VodzwrpyKumnshOBVYf06hENDjx2O4HpeKcZ1qYkoQu0RERK9SYU6ZvUgulwMAHBwcAAARERHIzc1FYGCgekzdunVRrVo1dSAKDw9HgwYNNE6hBQUFYfTo0bh69SqaNGmC8PBwjWOoxkycOLHIOrKzs5Gdna1+npqaqq+3SBWVWAzYuCgfVZsWPUahKLz+XFH/zctShqunSUDcxeJfU7X+nG1ViGXuWFJVhpUJWbh2/Brkfj1h61KN688REelJuQlECoUCEydORJs2bVC/fn0AQHx8PCQSCezs7DTGuri4ID4+Xj2mYBhS7Vfte9mY1NRUZGZmwsJCcyX4uXPn4ssvv9TbeyMjIRYD1k7Kh3vjoseo1p8r6qo51ek6eQyQl1lo/blaAL5XnSlbPUf5X3O7oieAy9wBu+qAvXeFveUAEVFZKjeBaOzYsbhy5QqOHTtm6FLw2WefYfLkyernqamp8PT0NGBFVGmIRMo7cFs6AG4Nix4jCEBWSpFXzT2Ki0JKXBTcRY9hKcpWjstKARKvFn0sc1ugajPAoxng0Ryo6q98bSIi0lAuAtG4ceOwa9cuHD16FB4ezye/urq6IicnBykpKRpdooSEBLi6uqrHnD59WuN4CQkJ6n2q/6q2FRwjk8kKdYcAQCqVQirVftIrkV6JRMq13izsAZd6GrscBQEhPx7D1Vg5Pm7rikktrIq/5UDyXeXCvncOKh8qDj7KgKQKSi719X5lHBFRRWPQQCQIAsaPH49t27bh8OHD8Pb21tjv7+8PMzMzHDx4EH369AEAREZGIjo6GgEBAQCAgIAAfP3110hMTISzszMAIDQ0FDKZDH5+fuoxe/bs0Th2aGio+hhEFYVIpLzibPhvZ/Hz6UcI6dQQDs6+RQ/OywESriiviHt4Fnh4Bki+8/xx6U/lOBOp8hSfupPUDLD1NOp15IjI+Bj0KrMxY8Zgw4YN+OeffzTuPWRra6vu3IwePRp79uzBunXrIJPJMH78eADAiRMnACgvu2/cuDHc3d0xf/58xMfH46OPPsKwYcM0LruvX78+xo4diyFDhuDQoUOYMGECdu/erdVl97zKjMoTQRDw1tJjuBKTilEdfPBp97raf3NGsmZAiolQnnJ7kbWLZkByb6Jcy46IqALR5fe3QQORqJh/ga5du1Z900TVjRk3btyocWNG1ekwALh//z5Gjx6Nw4cPw8rKCiEhIZg3b16hGzNOmjQJ165dg4eHB7744gutb8zIQETlzYFrCRj221lYSkzw39ROcLQu4SlehULZLXp4Foh5FpISrirvuVSQSAw4+QIe/s/mIjUDnOqUy+VOiIhUKkwgqigYiKi8EQQBby89jssxcozsUAOfdS/mtFlJ5GQobwegCkgPI5RXwL1IYgNUbfI8IHk0U96UkoionGAg0jMGIiqPDl5PwNBfz8LCzAT/TeuEKiXtEmkjNU4zIMWeA3IzCo+zq1YgIDUHXBsAZualVxcR0UswEOkZAxGVR4Ig4J1lx3HpoRwj29fAZ2/qsUv0Kvl5QNL1Z3ORnp1uS7pReJzYTBmKPJo/n49k780J20RUJhiI9IyBiMqrQzcSMGRdGXWJXiVLDsSc05yPlPG48DhLxxcmbDcFLOzKvFwiqvwYiPSMgYjKK0EQ0Gv5CVx8kIIR7Wvg/8qyS/QqggA8uffsqrYzyqAUf6no9d6q1HkekKo2A5z9lAvzEhG9BgYiPWMgovIsLDIRg9eegbmZGP9N7Qwnm3J8U9G8bCD+8vOA9PAMkHK/8DgzS+Wl/lX9n59uk7mXfb1EVKFV2MVdiUh3HWs7obGnHS48SMGqo3fweQ8/Q5dUPFPp806QSnrSs1NsqnsjnQNy0oD7x5UPFVlVzYDk1hiQWJb5WyCiyokdIi2wQ0Tl3eHIRAx61iU6OrUTnG0q8JVdCgXw6OazcPQsKCVeAwSF5jiRiXJpE/U6bc0Ax5pczJaI1HjKTM8YiKi8EwQBvZefwIUHKRja1htf9CzHXaKSyE4HYs8X6CSdBdLjC48zt33eRVJN3OZitkRGi4FIzxiIqCI4cjMJIWtOQ2oqxn/TKniX6FUEQbmQrXou0lkg7gKQl1V4rEONAgHJH3BpwMVsiYwEA5GeMRBRRSAIAt5dcQLno1MwpI03ZrxVybpEr5Kfq1zMtuC9kR7fLjzORAq4NXo2F+lZN4mL2RJVSgxEesZARBXF0ZtJGKjqEk3tBGdZJe4SaSMj+dm9kQrMRypqMVsrZ82AxMVsiSoFBiI9YyCiikIQBPRZcQLnolMwuI0XZr5Vz9AllS+CADy+U2AZkrPKrtLLFrNVLUPCxWyJKhwGIj1jIKKK5L9bSfho9WlITMV4p5E7GnjYokFVW/i6yWBuxl/oheRmKhezVQWkmAhA/qDwONVitqqAxMVsico9BiI9YyCiikQQBAxccxr/3Xqksd1ELEItZ2s0qGrLkPQqafEF7osUoTztlvu08Di7apoBybUhF7MlKkcYiPSMgYgqmpw8BcIiE3ElRo5LD+W4EiPH46eFl8x4MSTVr2oLP4akwvLzlIvXFpyLlBQJ4IW/PtWL2arujeSvvMqNE7aJDIKBSM8YiKiiEwQBcfIsXI5RhqPLMXJcfsiQ9FpUi9nGnAUePluvLeNR4XEWDpoBqao/F7MlKiMMRHrGQESVEUOSngmCcl22hwWWISl2MdvazwOSR3MuZktUShiI9IyBiIxFUSHpSowcj9IZkkpEvZjt2edXtj25V3icmaVybTbVOm8ezbmYLZEeMBDpGQMRGTOGJD17+kgzIMWcA7JTC4+zcdcMSFzMlkhnDER6xkBEpEkQBMSnZqknbDMkvQbVYrbqeyNFAIlXi1nM1q/AMiTNuZgt0SswEOkZAxHRq6lC0uWHz+YjaRGS6le1RUOGpMKy05Vrsz0scAPJohazldpq3jySi9kSaWAg0jMGIqKSYUjSE/VitgXujRR7vvjFbNUBiYvZknFjINIzBiIi/SkYkq7EyHFJy5CkOuXGkPRMfi6QcPX5fZEengUe3yo8Tr2Y7bP5SFWbKW8oyXsjkRFgINIzBiKi0sWQpCeZT5TdI1VAijmr3PYiK2fNgFS1KRezpZcTBGUIV+Q++29+ga9zlTcvVeQq1wVUfV3kvoL/Ve17tt/MEmgxXK9lMxDpGQMRUdl7MSSpTrlpE5JUp9ssJEYekgQBSL77fB7SwzNFL2YLEeDs+zwgeTQDnOpyMduSEoQifunnPg8DRe0rMiy8OP7Fffma41T7CoWSAt9fVD0v26faL+SX/udm7QpMidTrIRmI9IyBiKh8KDokpeJRenahsQxJxcjNBOIuaS5DUuRittaAe5Pnk7WrNgNsXPRfj0Z4eDEs6PALWx/7XtXBeLHzocgv+vhlER7KA5FYuVyNiRkgNn3+X7GZ8kajBfcV3G9iVvQ+C3ug21y9lshApGcMRETlF0OSHqgWs1UFpOIWs7WtBrg3BkzNtehgFAwSL3YwCgSJQt2qykpUIAgUDAtmyk5ckftMXwgQBfeZaB5DtU9sWvj4JqYFgsqLoeQl+zTqenGfaYW45QMDkZ4xEBFVLIIgICE1G5cepjAklYQi//litqr5SEk3UGgx21Ij0vzF+2JIKDIsvKwTUTAs6BIIinvtV9VVVOgp/+GhMmIg0jMGIqKKT9eQVNPJGg08GJLUslKB2HPKK9sEQYtOhA5dikKdCCP+nEmvGIj0jIGIqHJShSTlwrYpDElElQwDkZ4xEBEZD42QpA5KLw9JL95MkiGJqHxgINIzBiIi46ZLSBKLgFrONgxJROUAA5GeMRAR0YteDElXYuS49FD+ypDUoKoMDTzsGJKIygADkZ4xEBGRNhiSiMoXBiI9YyAiopIqKiRdjpEjKU2bkGQLPzdbhiSiEmIg0jMGIiLSt4TULFx6yJBEVJoYiPSMgYiIygJDEpF+MRDpGQMRERlKwrNlSS4xJBHpjIFIzxiIiKg80SUk1XS2RoOqdgxJZJQYiPSMgYiIyjtVSFLfK4khiYiBSN8YiIioIioYkq7EKDtKLwtJ9avaomFVW4YkqjQYiPSMgYiIKguGJDImDER6xkBERJXZiyHpcowcicWEpBpO1nCwksBSYvLsYQpLiQksJCaweuHrgv+1fOFrCzMTiMUiA7xbMia6/P42LaOaiIionHKRmcPFzxyBfi7qbcWFpNuJ6Xp7XQszE1hJlQHK0swUllJVWDKFlbTowPViuCpqm8RUrLcayXgwEBERUSHFhaTI+DSkZeUhIycPGTn5zx6Fv87MycfTnDxkFjFGJTM3H5m5+UW9/GsxFYs0wpSltOjApQpiqq8Lhiv19xYMZexqVWoMREREpBUXmTlcZOavdQyFQkBW3rOQlJ2PjNy8518XE7Iyc/Lw9FnIyijw9YuBKzdfOQMkTyEgNSsPqVl5+njbGizMTDRCloXkWZfLTBmaVF8/D1wmsJQWDlcvBi52tQyPgYiIiMqMWCx6FgZMAWv9Hjs3X1G4S5Wdh4zc54ErMzcfT7OVISsjJ/9ZuCqu2/X8uYqqq/X4qX5rNxWLNOZhFRe41N0u1dfFdbNUHS92tbTGQERERJWCmYkYthZi2FqY6fW4giAgK1eh0ZFSff00WxmyMlRf5+Q/C2DPglXBr58FrEx1EMtHTr4CgLKrlZaVh7RS7GoVd1qwcOB6tu8VgUtiIoZIVHnCFgMRERHRS4hEyu5NadxyQNXVynyhO1Xc/KsMdcfrWRcs91kXTP31s45Xbj5U15Cr52qVUleruEnuRQWpoibOP//aFE42Uv0Wqcv7MdgrExERGbnS7mq9avL7i3O0VKcYX+x+qYNYTj5y8orqahW+TYOuHKwkOPfFG699nJJiICIiIqpkCna1HPV87Lx8hca8rKJPB2rO0XpaRBfsxfle1lLDRhIGIiIiItKaqYkYMhMxZOb672oZEq/zIyIiIoMz9ARtBiIiIiIyegxEREREZPQYiIiIiMjoGTQQHT16FG+99Rbc3d0hEomwfft2jf2CIGDGjBlwc3ODhYUFAgMDcevWLY0xycnJCA4Ohkwmg52dHYYOHYr0dM3FBy9duoR27drB3Nwcnp6emD9/fmm/NSIiIqpADBqInj59ikaNGmHZsmVF7p8/fz6WLFmClStX4tSpU7CyskJQUBCysrLUY4KDg3H16lWEhoZi165dOHr0KEaMGKHen5qaiq5du6J69eqIiIjAggULMGvWLKxatarU3x8RERFVEEI5AUDYtm2b+rlCoRBcXV2FBQsWqLelpKQIUqlU2LhxoyAIgnDt2jUBgHDmzBn1mL179woikUiIiYkRBEEQli9fLtjb2wvZ2dnqMdOmTRPq1KmjdW1yuVwAIMjl8pK+PSIiIipjuvz+LrdziKKiohAfH4/AwED1NltbW7Rs2RLh4eEAgPDwcNjZ2aFZs2bqMYGBgRCLxTh16pR6TPv27SGRSNRjgoKCEBkZiSdPnhT52tnZ2UhNTdV4EBERUeVVbgNRfHw8AMDFxUVju4uLi3pffHw8nJ2dNfabmprCwcFBY0xRxyj4Gi+aO3cubG1t1Q9PT8/Xf0NERERUbpXbQGRIn332GeRyufrx4MEDQ5dEREREpajcBiJXV1cAQEJCgsb2hIQE9T5XV1ckJiZq7M/Ly0NycrLGmKKOUfA1XiSVSiGTyTQeREREVHmV20Dk7e0NV1dXHDx4UL0tNTUVp06dQkBAAAAgICAAKSkpiIiIUI85dOgQFAoFWrZsqR5z9OhR5ObmqseEhoaiTp06sLe3L6N3Q0REROWZQQNReno6Lly4gAsXLgBQTqS+cOECoqOjIRKJMHHiRMyZMwc7duzA5cuXMXDgQLi7u6NXr14AAF9fX3Tr1g3Dhw/H6dOncfz4cYwbNw79+vWDu7s7AODDDz+ERCLB0KFDcfXqVfz5559YvHgxJk+ebKB3TUREROVOGVz1VqywsDABQKFHSEiIIAjKS++/+OILwcXFRZBKpUKXLl2EyMhIjWM8fvxY6N+/v2BtbS3IZDJh8ODBQlpamsaYixcvCm3bthWkUqlQtWpVYd68eTrVycvuiYiIKh5dfn+LBEEQDJjHKgS5XA47Ozs8ePCA84mIiIgqiNTUVHh6eiIlJQW2trYvHWtaRjVVaGlpaQDAy++JiIgqoLS0tFcGInaItKBQKBAbGwsbGxuIRCK9HluVXtl9In3inysqLfyzRaWhtP5cCYKAtLQ0uLu7Qyx++bRpdoi0IBaL4eHhUaqvwcv7qTTwzxWVFv7ZotJQGn+uXtUZUim3l90TERERlRUGIiIiIjJ6DEQGJpVKMXPmTEilUkOXQpUI/1xRaeGfLSoN5eHPFSdVExERkdFjh4iIiIiMHgMRERERGT0GIiIiIjJ6DEQGtGzZMnh5ecHc3BwtW7bE6dOnDV0SVXBHjx7FW2+9BXd3d4hEImzfvt3QJVElMHfuXDRv3hw2NjZwdnZGr169EBkZaeiyqBJYsWIFGjZsqL7/UEBAAPbu3WuQWhiIDOTPP//E5MmTMXPmTJw7dw6NGjVCUFAQEhMTDV0aVWBPnz5Fo0aNsGzZMkOXQpXIkSNHMHbsWJw8eRKhoaHIzc1F165d8fTpU0OXRhWch4cH5s2bh4iICJw9exadO3fGO++8g6tXr5Z5LbzKzEBatmyJ5s2bY+nSpQCUy4N4enpi/Pjx+PTTTw1cHVUGIpEI27ZtQ69evQxdClUySUlJcHZ2xpEjR9C+fXtDl0OVjIODAxYsWIChQ4eW6euyQ2QAOTk5iIiIQGBgoHqbWCxGYGAgwsPDDVgZEdGryeVyAMpfXET6kp+fj02bNuHp06cICAgo89fnWmYG8OjRI+Tn58PFxUVju4uLC27cuGGgqoiIXk2hUGDixIlo06YN6tevb+hyqBK4fPkyAgICkJWVBWtra2zbtg1+fn5lXgcDERERaW3s2LG4cuUKjh07ZuhSqJKoU6cOLly4ALlcjr/++gshISE4cuRImYciBiIDqFKlCkxMTJCQkKCxPSEhAa6urgaqiojo5caNG4ddu3bh6NGj8PDwMHQ5VElIJBLUrFkTAODv748zZ85g8eLF+Omnn8q0Ds4hMgCJRAJ/f38cPHhQvU2hUODgwYMGOW9KRPQygiBg3Lhx2LZtGw4dOgRvb29Dl0SVmEKhQHZ2dpm/LjtEBjJ58mSEhISgWbNmaNGiBX744Qc8ffoUgwcPNnRpVIGlp6fj9u3b6udRUVG4cOECHBwcUK1aNQNWRhXZ2LFjsWHDBvzzzz+wsbFBfHw8AMDW1hYWFhYGro4qss8++wzdu3dHtWrVkJaWhg0bNuDw4cPYv39/mdfCy+4NaOnSpViwYAHi4+PRuHFjLFmyBC1btjR0WVSBHT58GJ06dSq0PSQkBOvWrSv7gqhSEIlERW5fu3YtBg0aVLbFUKUydOhQHDx4EHFxcbC1tUXDhg0xbdo0vPHGG2VeCwMRERERGT3OISIiIiKjx0BERERERo+BiIiIiIweAxEREREZPQYiIiIiMnoMRERERGT0GIiIiIjI6DEQERERkdFjICIiKiGRSITt27cbugwi0gMGIiKqkAYNGgSRSFTo0a1bN0OXRkQVEBd3JaIKq1u3bli7dq3GNqlUaqBqiKgiY4eIiCosqVQKV1dXjYe9vT0A5emsFStWoHv37rCwsECNGjXw119/aXz/5cuX0blzZ1hYWMDR0REjRoxAenq6xpg1a9agXr16kEqlcHNzw7hx4zT2P3r0CL1794alpSVq1aqFHTt2lO6bJqJSwUBERJXWF198gT59+uDixYsIDg5Gv379cP36dQDA06dPERQUBHt7e5w5cwZbtmzBgQMHNALPihUrMHbsWIwYMQKXL1/Gjh07ULNmTY3X+PLLL9G3b19cunQJb775JoKDg5GcnFym75OI9EAgIqqAQkJCBBMTE8HKykrj8fXXXwuCIAgAhFGjRml8T8uWLYXRo0cLgiAIq1atEuzt7YX09HT1/t27dwtisViIj48XBEEQ3N3dhc8//7zYGgAI06dPVz9PT08XAAh79+7V2/skorLBOUREVGF16tQJK1as0Njm4OCg/jogIEBjX0BAAC5cuAAAuH79Oho1agQrKyv1/jZt2kChUCAyMhIikQixsbHo0qXLS2to2LCh+msrKyvIZDIkJiaW9C0RkYEwEBFRhWVlZVXoFJa+WFhYaDXOzMxM47lIJIJCoSiNkoioFHEOERFVWidPniz03NfXFwDg6+uLixcv4unTp+r9x48fh1gsRp06dWBjYwMvLy8cPHiwTGsmIsNgh4iIKqzs7GzEx8drbDM1NUWVKlUAAFu2bEGzZs3Qtm1brF+/HqdPn8bq1asBAMHBwZg5cyZCQkIwa9YsJCUlYfz48fjoo4/g4uICAJg1axZGjRoFZ2dndO/eHWlpaTh+/DjGjx9ftm+UiEodAxERVVj79u2Dm5ubxrY6dergxo0bAJRXgG3atAljxoyBm5sbNm7cCD8/PwCApaUl9u/fj48//hjNmzeHpaUl+vTpg4ULF6qPFRISgqysLCxatAhTpkxBlSpV8N5775XdGySiMiMSBEEwdBFERPomEomwbds29OrVy9ClEFEFwDlEREREZPQYiIiIiMjocQ4REVVKnA1ARLpgh4iIiIiMHgMRERERGT0GIiIiIjJ6DERERERk9BiIiIiIyOgxEBEREZHRYyAiIiIio8dAREREREaPgYiIiIiM3v8DTl3jkWrq21wAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train and val losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(test_losses, label='validation')\n",
    "plt.title('BERT Fine tuning\\nTrain and Validation Losses')\n",
    "# integer x ticks\n",
    "plt.xticks(range(len(train_losses)))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error ($)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
